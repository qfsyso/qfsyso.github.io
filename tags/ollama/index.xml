<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Ollama on MLOG</title><link>https://qfsyso.github.io/tags/ollama/</link><description>Recent content in Ollama on MLOG</description><generator>Hugo -- 0.126.3</generator><language>zh-CN</language><lastBuildDate>Wed, 16 Oct 2024 00:11:42 +0000</lastBuildDate><atom:link href="https://qfsyso.github.io/tags/ollama/index.xml" rel="self" type="application/rss+xml"/><item><title>.net local LLM</title><link>https://qfsyso.github.io/posts/.net-local-llm/</link><pubDate>Wed, 16 Oct 2024 00:11:42 +0000</pubDate><guid>https://qfsyso.github.io/posts/.net-local-llm/</guid><description>ollama ollama --version 比较常用的指令不多，也很简单 列出本地下载的模型
ollama list 查看正在运行的模型
ollama ps 模型标识下载模型到本地
ollama pull 比如我要下载llama2-chinese:7b则使用
ollama pull llama2-chinese:7b 模型标识运行模型，如果已下载则直接运行，如果没下载则先下载再运行。
ollama run 比如我要运行llama2-chinese:7b可以直接运行
ollama run llama2-chinese:7b .net package dotnet add package Ollama --version 1.9.0 //模型名称是必须要传递的，默认 流式输出,如果想一次返回同样的是设置stream为false。 string modelName = &amp;#34;llama2-chinese:7b&amp;#34;; using var ollama = new OllamaApiClient(baseUri: new Uri(&amp;#34;http://127.0.0.1:11434/api&amp;#34;)); Console.WriteLine(&amp;#34;begin~~~&amp;#34;); string userInput = &amp;#34;&amp;#34;; do { Console.WriteLine(&amp;#34;User:&amp;#34;); userInput = Console.ReadLine()!; var enumerable = ollama.Completions.GenerateCompletionAsync(modelName, userInput); Console.WriteLine(&amp;#34;Agent:&amp;#34;); await foreach (var response in enumerable) { Console.</description></item><item><title>ollama gpt qwen gemma2 deepseek loacl AI</title><link>https://qfsyso.github.io/posts/ollama-gpt-qwen-gemma2-deepseek-loacl-ai/</link><pubDate>Fri, 26 Jul 2024 23:33:08 +0000</pubDate><guid>https://qfsyso.github.io/posts/ollama-gpt-qwen-gemma2-deepseek-loacl-ai/</guid><description>ollama https://ollama.com/ ollama
https://ollama.com/library models
ollama run qwen2 send a message ollama run gemma2 https://github.com/open-webui/open-webui webui
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main N卡 gpu
docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda http://localhost:3000/
deepseek https://ollama.com/library/deepseek-coder-v2 deepseek-coder-v2 ~~ codeqwen sqlcoder
curl -X POST http://localhost:11434/api/generate -d &amp;#39;{ &amp;#34;model&amp;#34;: &amp;#34;llama2-chinese:7b-chat-q4_0&amp;#34;, &amp;#34;prompt&amp;#34;:&amp;#34;为什么天空是蓝色的&amp;#34; }&amp;#39; https://docs.openwebui.com/
单独部署Open WebUI Open WebUI https://docs.openwebui.com/ 安装默认配置</description></item></channel></rss>