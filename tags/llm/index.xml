<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>LLM on MLOG</title><link>https://qfsyso.github.io/tags/llm/</link><description>Recent content in LLM on MLOG</description><generator>Hugo -- 0.126.3</generator><language>zh-CN</language><lastBuildDate>Wed, 16 Oct 2024 00:11:42 +0000</lastBuildDate><atom:link href="https://qfsyso.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>.net local LLM</title><link>https://qfsyso.github.io/posts/.net-local-llm/</link><pubDate>Wed, 16 Oct 2024 00:11:42 +0000</pubDate><guid>https://qfsyso.github.io/posts/.net-local-llm/</guid><description>ollama ollama --version 比较常用的指令不多，也很简单 列出本地下载的模型
ollama list 查看正在运行的模型
ollama ps 模型标识下载模型到本地
ollama pull 比如我要下载llama2-chinese:7b则使用
ollama pull llama2-chinese:7b 模型标识运行模型，如果已下载则直接运行，如果没下载则先下载再运行。
ollama run 比如我要运行llama2-chinese:7b可以直接运行
ollama run llama2-chinese:7b .net package dotnet add package Ollama --version 1.9.0 //模型名称是必须要传递的，默认 流式输出,如果想一次返回同样的是设置stream为false。 string modelName = &amp;#34;llama2-chinese:7b&amp;#34;; using var ollama = new OllamaApiClient(baseUri: new Uri(&amp;#34;http://127.0.0.1:11434/api&amp;#34;)); Console.WriteLine(&amp;#34;begin~~~&amp;#34;); string userInput = &amp;#34;&amp;#34;; do { Console.WriteLine(&amp;#34;User:&amp;#34;); userInput = Console.ReadLine()!; var enumerable = ollama.Completions.GenerateCompletionAsync(modelName, userInput); Console.WriteLine(&amp;#34;Agent:&amp;#34;); await foreach (var response in enumerable) { Console.</description></item></channel></rss>