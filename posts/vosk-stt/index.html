<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Vosk STT | MLOG</title>
<meta name=keywords content="API,WebAPI,AI,Vosk,Python"><meta name=description content='Vosk STT 一、准备环境（Debian） sudo apt update sudo apt install -y python3 python3-pip ffmpeg pip3 install --user vosk 二、下载微型中文模型（≈ 40 MB） wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip unzip vosk-model-small-cn-0.22.zip 三、保存以下脚本为 listen.py #!/usr/bin/env python3 import sys, json, os from vosk import Model, KaldiRecognizer import wave MAP = { "空调": 1, "电视": 2, "风扇": 3, "卧室灯": 4, "客厅灯": 5 } def wav2text(path): wf = wave.open(path, "rb") if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError("音频必须是 16 kHz 16-bit 单声道 WAV") model = Model("vosk-model-small-cn-0.'><meta name=author content="dwd"><link rel=canonical href=https://qfsyso.github.io/posts/vosk-stt/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://qfsyso.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://qfsyso.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://qfsyso.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://qfsyso.github.io/apple-touch-icon.png><link rel=mask-icon href=https://qfsyso.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://qfsyso.github.io/posts/vosk-stt/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://qfsyso.github.io/posts/vosk-stt/"><meta property="og:site_name" content="MLOG"><meta property="og:title" content="Vosk STT"><meta property="og:description" content='Vosk STT 一、准备环境（Debian） sudo apt update sudo apt install -y python3 python3-pip ffmpeg pip3 install --user vosk 二、下载微型中文模型（≈ 40 MB） wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip unzip vosk-model-small-cn-0.22.zip 三、保存以下脚本为 listen.py #!/usr/bin/env python3 import sys, json, os from vosk import Model, KaldiRecognizer import wave MAP = { "空调": 1, "电视": 2, "风扇": 3, "卧室灯": 4, "客厅灯": 5 } def wav2text(path): wf = wave.open(path, "rb") if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError("音频必须是 16 kHz 16-bit 单声道 WAV") model = Model("vosk-model-small-cn-0.'><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-17T21:36:01+08:00"><meta property="article:modified_time" content="2025-07-17T21:36:01+08:00"><meta property="article:tag" content="API"><meta property="article:tag" content="WebAPI"><meta property="article:tag" content="AI"><meta property="article:tag" content="Vosk"><meta property="article:tag" content="Python"><meta name=twitter:card content="summary"><meta name=twitter:title content="Vosk STT"><meta name=twitter:description content='Vosk STT 一、准备环境（Debian） sudo apt update sudo apt install -y python3 python3-pip ffmpeg pip3 install --user vosk 二、下载微型中文模型（≈ 40 MB） wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip unzip vosk-model-small-cn-0.22.zip 三、保存以下脚本为 listen.py #!/usr/bin/env python3 import sys, json, os from vosk import Model, KaldiRecognizer import wave MAP = { "空调": 1, "电视": 2, "风扇": 3, "卧室灯": 4, "客厅灯": 5 } def wav2text(path): wf = wave.open(path, "rb") if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError("音频必须是 16 kHz 16-bit 单声道 WAV") model = Model("vosk-model-small-cn-0.'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://qfsyso.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Vosk STT","item":"https://qfsyso.github.io/posts/vosk-stt/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Vosk STT","name":"Vosk STT","description":"Vosk STT 一、准备环境（Debian） sudo apt update sudo apt install -y python3 python3-pip ffmpeg pip3 install --user vosk 二、下载微型中文模型（≈ 40 MB） wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip unzip vosk-model-small-cn-0.22.zip 三、保存以下脚本为 listen.py #!/usr/bin/env python3 import sys, json, os from vosk import Model, KaldiRecognizer import wave MAP = { \u0026#34;空调\u0026#34;: 1, \u0026#34;电视\u0026#34;: 2, \u0026#34;风扇\u0026#34;: 3, \u0026#34;卧室灯\u0026#34;: 4, \u0026#34;客厅灯\u0026#34;: 5 } def wav2text(path): wf = wave.open(path, \u0026#34;rb\u0026#34;) if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError(\u0026#34;音频必须是 16 kHz 16-bit 单声道 WAV\u0026#34;) model = Model(\u0026#34;vosk-model-small-cn-0.","keywords":["API","WebAPI","AI","Vosk","Python"],"articleBody":"Vosk STT 一、准备环境（Debian） sudo apt update sudo apt install -y python3 python3-pip ffmpeg pip3 install --user vosk 二、下载微型中文模型（≈ 40 MB） wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip unzip vosk-model-small-cn-0.22.zip 三、保存以下脚本为 listen.py #!/usr/bin/env python3 import sys, json, os from vosk import Model, KaldiRecognizer import wave MAP = { \"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5 } def wav2text(path): wf = wave.open(path, \"rb\") if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError(\"音频必须是 16 kHz 16-bit 单声道 WAV\") model = Model(\"vosk-model-small-cn-0.22\") rec = KaldiRecognizer(model, wf.getframerate()) rec.SetWords(True) text = \"\" while True: data = wf.readframes(4000) if len(data) == 0: break if rec.AcceptWaveform(data): text += json.loads(rec.Result())[\"text\"] text += json.loads(rec.FinalResult())[\"text\"] return text.replace(\" \", \"\") def find_index(text): for key, idx in MAP.items(): if key in text: return idx return 0 if __name__ == \"__main__\": if len(sys.argv) \u003c 2: print(\"用法: python3 listen.py \") sys.exit(1) t = wav2text(sys.argv[1]) print(\"识别文本:\", t) print(\"对应序号:\", find_index(t)) 四、运行示例 python3 listen.py wsd.wav 如果音频内容是「我是登」，识别结果里包含「卧室灯」，脚本会输出\n识别文本: 我是登卧室灯 对应序号: 4 若未命中任何关键词则输出 对应序号: 0。 打包docker运行 listen.py #!/usr/bin/env python3 import sys, json, os from vosk import Model, KaldiRecognizer import wave MAP = { \"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5 } def wav2text(path): wf = wave.open(path, \"rb\") if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError(\"音频必须是 16 kHz 16-bit 单声道 WAV\") model = Model(\"/opt\") rec = KaldiRecognizer(model, wf.getframerate()) rec.SetWords(True) text = \"\" while True: data = wf.readframes(4000) if len(data) == 0: break if rec.AcceptWaveform(data): text += json.loads(rec.Result())[\"text\"] text += json.loads(rec.FinalResult())[\"text\"] return text.replace(\" \", \"\") def find_index(text): for key, idx in MAP.items(): if key in text: return idx return 0 if __name__ == \"__main__\": if len(sys.argv) \u003c 2: print(\"用法: python3 listen.py \") sys.exit(1) t = wav2text(sys.argv[1]) print(\"识别文本:\", t) print(\"对应序号:\", find_index(t)) dockerfile # ---- 1. 基础镜像 ---- FROM python:3.11-slim-bookworm # ---- 2. 系统依赖 \u0026 ffmpeg ---- RUN apt-get update \u0026\u0026 \\ apt-get install -y --no-install-recommends \\ ffmpeg \\ wget \\ unzip \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* # ---- 3. 安装 Vosk ---- RUN pip install --no-cache-dir vosk==0.3.45 # ---- 4. 下载并解压中文小模型 ---- WORKDIR /opt RUN wget -q https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip \u0026\u0026 \\ unzip -q vosk-model-small-cn-0.22.zip \u0026\u0026 \\ mv vosk-model-small-cn-0.22/* ./ \u0026\u0026 \\ rmdir vosk-model-small-cn-0.22 \u0026\u0026 \\ rm vosk-model-small-cn-0.22.zip # ---- 5. 复制脚本 ---- COPY listen.py /opt/listen.py RUN chmod +x /opt/listen.py # ---- 6. 入口脚本：自动转码 + 识别 ---- # 创建一个小脚本作为 ENTRYPOINT RUN printf '#!/bin/bash\\n\\ set -e\\n\\ IN=\"$1\"\\n\\ BASE=$(basename \"$IN\" .wav)\\n\\ OUT=\"${BASE}_16k.wav\"\\n\\ echo \"Converting $IN -\u003e $OUT ...\"\\n\\ ffmpeg -y -i \"$IN\" -ar 16000 -ac 1 -sample_fmt s16 \"$OUT\" \u003e/dev/null 2\u003e\u00261\\n\\ echo \"Running Vosk on $OUT ...\"\\n\\ exec python3 /opt/listen.py \"$OUT\"\\n' \u003e /opt/entry.sh \u0026\u0026 chmod +x /opt/entry.sh WORKDIR /workspace ENTRYPOINT [\"/opt/entry.sh\"] 构建使用 docker build -t stt-cn-mini . docker run --rm -v $(pwd):/workspace stt-cn-mini wsd.wav 使用pypinyin进行纠错 下面给出「整句拼音容错」的完整实现： 用 pypinyin 把整句话转拼音（带声调）， 再与标准关键词的拼音做模糊匹配， 支持「登」→「灯」，「客厅」→「客厅灯」等任意同音/近音组合。 只改 listen.py，其余不变。\n1. 安装依赖（Dockerfile 里加一行即可） RUN pip install --no-cache-dir vosk==0.3.45 pypinyin fuzzywuzzy python-Levenshtein 2. 新的 listen.py（直接替换） #!/usr/bin/env python3 import sys, json, os, re from vosk import Model, KaldiRecognizer import wave from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz # 关键词→序号 KEYWORD2ID = { \"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5, } # 预先把关键词转成拼音列表，避免每次重复计算 KW_PINYIN = {kw: \"\".join(lazy_pinyin(kw)) for kw in KEYWORD2ID} def wav2text(path): wf = wave.open(path, \"rb\") if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError(\"音频必须是 16 kHz 16-bit 单声道 WAV\") model = Model(\"/opt\") rec = KaldiRecognizer(model, wf.getframerate()) rec.SetWords(True) text = \"\" while True: data = wf.readframes(4000) if len(data) == 0: break if rec.AcceptWaveform(data): text += json.loads(rec.Result())[\"text\"] text += json.loads(rec.FinalResult())[\"text\"] return text.replace(\" \", \"\") def fuzzy_match(text: str) -\u003e int: \"\"\"基于拼音的模糊匹配，返回最相似关键词的序号\"\"\" text_py = \"\".join(lazy_pinyin(text)) # 整句话转拼音 best_kw, best_score = \"\", 0 for kw, kw_py in KW_PINYIN.items(): score = fuzz.partial_ratio(text_py, kw_py) if score \u003e best_score: best_kw, best_score = kw, score # 阈值可自己调，80 以上基本可接受 if best_score \u003e= 80: return KEYWORD2ID[best_kw] return 0 if __name__ == \"__main__\": if len(sys.argv) \u003c 2: print(\"用法: python3 listen.py \") sys.exit(1) t = wav2text(sys.argv[1]) idx = fuzzy_match(t) print(\"识别文本:\", t) print(\"对应序号:\", idx) 3. 构建 \u0026 运行 docker build -t stt-cn-mini . docker run --rm -v $(pwd):/workspace stt-cn-mini wsd.wav 示例结果\n识别文本: 我是登 对应序号: 4 # 因为 \"woshideng\" 与 \"woshideng\" 完全匹配 \"卧室灯\" 4. 效果说明 pypinyin.lazy_pinyin() → 把整句话转拼音（带数字声调）。 fuzz.partial_ratio() → 计算句子拼音与关键词拼音的相似度。\n阈值 80 以上即可覆盖常见同音/近音错误，调低或调高视实际效果。\n结果 docker run --rm -v $(pwd):/workspace stt-cn-mini test_wsd.wav Converting test_wsd.wav -\u003e test_wsd_16k.wav ... Running Vosk on test_wsd_16k.wav ... 识别文本: 小爱同学帮我打开我是灯 对应序号: 4 web API 轻量级 FastAPI 服务 容器化部署（Debian 基础镜像 + Vosk + 中文拼音容错）。 通过 HTTP POST /recognize 提交 JSON：\n{\"url\": \"http:///.wav\"} 返回同音容错后的结果：\n{ \"text\": \"我是登\", \"matched\": \"卧室灯\", \"id\": 4 } 1. 目录结构 stt-api/ ├── Dockerfile ├── requirements.txt └── app.py\n2. app.py from fastapi import FastAPI, HTTPException from pydantic import BaseModel, HttpUrl import tempfile, os, requests, wave, json from vosk import Model, KaldiRecognizer from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz app = FastAPI(title=\"STT-CN-API\", version=\"1.0\") # 关键词配置 KEYWORD2ID = {\"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5} KW_PINYIN = {kw: \"\".join(lazy_pinyin(kw)) for kw in KEYWORD2ID} # 全局加载一次模型 MODEL = Model(\"/opt/models\") def wav2text(path: str) -\u003e str: with wave.open(path, \"rb\") as wf: if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError(\"音频必须是 16 kHz 16-bit 单声道 WAV\") rec = KaldiRecognizer(MODEL, wf.getframerate()) rec.SetWords(True) result = [] while True: data = wf.readframes(4000) if len(data) == 0: break if rec.AcceptWaveform(data): result.append(json.loads(rec.Result())[\"text\"]) result.append(json.loads(rec.FinalResult())[\"text\"]) return \"\".join(result).strip() def fuzzy_match(text: str): text_py = \"\".join(lazy_pinyin(text)) best_kw, best_score = \"\", 0 for kw, kw_py in KW_PINYIN.items(): score = fuzz.partial_ratio(text_py, kw_py) if score \u003e best_score: best_kw, best_score = kw, score return best_kw if best_score \u003e= 70 else None class RecognizeRequest(BaseModel): url: HttpUrl @app.post(\"/recognize\") async def recognize(req: RecognizeRequest): wav_url = str(req.url) try: # 下载到临时文件 with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp: tmp.write(requests.get(wav_url, timeout=10).content) tmp_path = tmp.name # 转码（Vosk 只认 16k/16bit/单声道） out_path = tmp_path + \"_16k.wav\" os.system( f\"ffmpeg -y -i {tmp_path} -ar 16000 -ac 1 -sample_fmt s16 {out_path} \u003e/dev/null 2\u003e\u00261\" ) # 识别 text = wav2text(out_path) matched = fuzzy_match(text) resp = {\"text\": text, \"matched\": matched, \"id\": KEYWORD2ID.get(matched, 0)} except Exception as e: raise HTTPException(status_code=500, detail=str(e)) finally: for p in (tmp_path, out_path): if os.path.exists(p): os.remove(p) return resp 3. requirements.txt fastapi==0.110.2 uvicorn[standard]==0.29.0 vosk==0.3.45 pypinyin==0.51.0 fuzzywuzzy==0.18.0 python-Levenshtein==0.25.1 requests==2.32.3 4. Dockerfile FROM python:3.11-slim-bookworm RUN apt-get update \u0026\u0026 \\ apt-get install -y --no-install-recommends \\ ffmpeg wget unzip \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* WORKDIR /opt # 下载并解压模型到 /opt/models RUN wget -q https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip \u0026\u0026 \\ unzip -q vosk-model-small-cn-0.22.zip \u0026\u0026 \\ mv vosk-model-small-cn-0.22 models \u0026\u0026 \\ rm vosk-model-small-cn-0.22.zip WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY app.py . EXPOSE 8000 CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] 5. 构建 \u0026 运行 docker build -t stt-api . docker run -d --name stt -p 8000:8000 stt-api 6. 调用示例 curl -X POST http://localhost:8000/recognize \\ -H \"Content-Type: application/json\" \\ -d '{\"url\":\"http://192.168.80.47:16552/wsd.wav\"}' 返回：\n{ \"text\": \"我是登\", \"matched\": \"卧室灯\", \"id\": 4 } 可使用 tts 进行生成语音测试 ttsfm https://github.com/dbccccccc/ttsfm ChatTTS-ui https://github.com/tianhm/ChatTTS-ui\ndebian nodocker webapi 直接裸机部署、无需 Docker” 的 Vosk + 中文拼音容错 WebAPI 完整代码。 已在 Ubuntu / Debian ≥20.04 + Python 3.8+ 验证通过，仅需 5 步即可上线。\n1. 系统依赖（一次性） sudo apt update sudo apt install -y python3 python3-pip ffmpeg wget unzip 2. 下载并放置中文小模型 # 在任意目录，例如 /opt/vosk-model sudo mkdir -p /opt/vosk-model cd /opt/vosk-model sudo wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip sudo unzip vosk-model-small-cn-0.22.zip sudo mv vosk-model-small-cn-0.22/* ./ \u0026\u0026 sudo rmdir vosk-model-small-cn-0.22 3. 安装 Python 依赖 pip3 install --user \\ fastapi uvicorn vosk pypinyin fuzzywuzzy python-Levenshtein requests 4. 完整代码 app.py 保存为 ~/stt_api/app.py（路径随意）。\n#!/usr/bin/env python3 import os, tempfile, wave, json, requests from fastapi import FastAPI, HTTPException from pydantic import BaseModel, HttpUrl from vosk import Model, KaldiRecognizer from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz # ------------- 全局配置 ------------- KEYWORD2ID = {\"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5} KW_PINYIN = {kw: \"\".join(lazy_pinyin(kw)) for kw in KEYWORD2ID} MODEL_DIR = \"/opt/vosk-model\" # 与步骤2一致 MODEL = Model(MODEL_DIR) app = FastAPI(title=\"Vosk-CN-API\", version=\"1.0\") # ------------- 工具函数 ------------- def wav2text(path: str) -\u003e str: with wave.open(path, \"rb\") as wf: if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError(\"音频需 16kHz/16-bit/单声道\") rec = KaldiRecognizer(MODEL, wf.getframerate()) rec.SetWords(True) result = [] while True: data = wf.readframes(4000) if len(data) == 0: break if rec.AcceptWaveform(data): result.append(json.loads(rec.Result())[\"text\"]) result.append(json.loads(rec.FinalResult())[\"text\"]) return \"\".join(result).strip() def fuzzy_match(text: str): txt_py = \"\".join(lazy_pinyin(text)) best_kw, best_score = \"\", 0 for kw, kw_py in KW_PINYIN.items(): score = fuzz.partial_ratio(txt_py, kw_py) if score \u003e best_score: best_kw, best_score = kw, score return best_kw if best_score \u003e= 70 else None # ------------- 接口 ------------- class RecognizeRequest(BaseModel): url: HttpUrl @app.post(\"/recognize\") def recognize(req: RecognizeRequest): wav_url = str(req.url) try: with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp: tmp.write(requests.get(wav_url, timeout=10).content) tmp_path = tmp.name # 转码 out_path = tmp_path + \"_16k.wav\" os.system( f\"ffmpeg -y -i {tmp_path} -ar 16000 -ac 1 -sample_fmt s16 {out_path} \u003e/dev/null 2\u003e\u00261\" ) text = wav2text(out_path) matched = fuzzy_match(text) return {\"text\": text, \"matched\": matched, \"id\": KEYWORD2ID.get(matched, 0)} except Exception as e: raise HTTPException(status_code=500, detail=str(e)) finally: for p in (tmp_path, out_path): if os.path.exists(p): os.remove(p) # ------------- 本地调试 ------------- if __name__ == \"__main__\": import uvicorn uvicorn.run(app, host=\"0.0.0.0\", port=8000) 5. 启动服务 mkdir -p ~/stt_api \u0026\u0026 cd ~/stt_api nano app.py # 粘贴上面代码 python3 app.py # 前台运行，或后台用 nohup / systemd 6. 调用示例 curl -X POST http://\u003c服务器IP\u003e:8000/recognize \\ -H \"Content-Type: application/json\" \\ -d '{\"url\":\"http://192.168.80.47:16552/wsd.wav\"}' 返回：\n{ \"text\": \"我是登\", \"matched\": \"卧室灯\", \"id\": 4 } 7. 开机自启（可选） 创建 systemd 文件 /etc/systemd/system/stt.service：\n[Unit] Description=Vosk-CN-API After=network.target [Service] Type=exec ExecStart=/usr/bin/python3 /home/ubuntu/stt_api/app.py Restart=always User=ubuntu WorkingDirectory=/home/ubuntu/stt_api [Install] WantedBy=multi-user.target sudo systemctl daemon-reload sudo systemctl enable --now stt 至此，无需 Docker 的 Vosk WebAPI 已部署完成。\nPEP668 Debian 12 / Ubuntu 23.04+ 启用了 PEP668，禁止用 pip3 –user 或 sudo pip 往系统 Python 里装包。 虚拟环境\n安装系统包（只需一次）\nsudo apt update sudo apt install -y python3-venv python3-pip ffmpeg wget unzip 创建并激活虚拟环境\npython3 -m venv ~/stt_venv source ~/stt_venv/bin/activate # 以后每次运行前都先激活 在虚拟环境里安装依赖\npip install --upgrade pip pip install fastapi uvicorn vosk pypinyin fuzzywuzzy python-Levenshtein requests cd ~/stt_api ~/stt_venv/bin/python app.py # 或 uvicorn app:app --host 0.0.0.0 --port 8000 测试\ncurl -X POST http://localhost:8000/recognize \\ -H \"Content-Type: application/json\" \\ -d '{\"url\":\"http://192.168.80.47:16552/wsd.wav\"}' Whisper版本 Whisper 是 OpenAI 出的开源语音识别模型（支持多语言，含中文）\nwhisper.cpp 是 Whisper 的轻量 C/C++ 版本，专门针对 CPU 做了优化\n有 Python binding，也可命令行直接跑\n体积小，tiny/base 模型几十 MB\n优点\n开源，完全离线，专门针对 CPU 做了优化\n缺点\n对比 Vosk 稍微吃一点 CPU（但 tiny 模型负载很低）\n不支持流式识别（一次识别一段音频）\nwhisper-api\n使用 轻量版 Whisper（openai-whisper CPU 推理即可，无需 GPU）。 其余逻辑不变：接收 URL → 下载 → 转码 → 中文同音纠错 → 返回 JSON。\n1. 目录结构 whisper-api/ ├── Dockerfile ├── requirements.txt └── app.py\n2. requirements.txt fastapi==0.110.2 uvicorn[standard]==0.29.0 openai-whisper==20231117 pypinyin==0.51.0 fuzzywuzzy==0.18.0 python-Levenshtein==0.25.1 requests==2.32.3 3. app.py（完全替换旧版本） from fastapi import FastAPI, HTTPException from pydantic import BaseModel, HttpUrl import tempfile, os, requests import whisper from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz app = FastAPI(title=\"Whisper-CN-API\", version=\"1.1\") # 关键词配置 KEYWORD2ID = {\"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5} KW_PINYIN = {kw: \"\".join(lazy_pinyin(kw)) for kw in KEYWORD2ID} # 全局加载一次模型（base 体积小，中文够用） MODEL = whisper.load_model(\"base\") class RecognizeRequest(BaseModel): url: HttpUrl def fuzzy_match(text: str): text_py = \"\".join(lazy_pinyin(text)) best_kw, best_score = \"\", 0 for kw, kw_py in KW_PINYIN.items(): score = fuzz.partial_ratio(text_py, kw_py) if score \u003e best_score: best_kw, best_score = kw, score return best_kw if best_score \u003e= 70 else None @app.post(\"/recognize\") async def recognize(req: RecognizeRequest): wav_url = str(req.url) try: # 下载到临时文件 with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp: tmp.write(requests.get(wav_url, timeout=10).content) tmp_path = tmp.name # 转码 16k/16bit/单声道（Whisper 会自动重采样，但统一格式更稳） out_path = tmp_path + \"_16k.wav\" os.system( f\"ffmpeg -y -i {tmp_path} -ar 16000 -ac 1 -sample_fmt s16 {out_path} \u003e/dev/null 2\u003e\u00261\" ) # Whisper 识别 result = MODEL.transcribe(out_path, language=\"zh\", fp16=False) text = result[\"text\"].strip() matched = fuzzy_match(text) resp = {\"text\": text, \"matched\": matched, \"id\": KEYWORD2ID.get(matched, 0)} except Exception as e: raise HTTPException(status_code=500, detail=str(e)) finally: for p in (tmp_path, out_path): if os.path.exists(p): os.remove(p) return resp 4. Dockerfile FROM python:3.11-slim-bookworm RUN apt-get update \u0026\u0026 \\ apt-get install -y --no-install-recommends \\ ffmpeg wget \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY app.py . EXPOSE 8000 CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] 5. 构建 \u0026 运行 docker build -t whisper-api . docker run -d --name whisper -p 8000:8000 whisper-api 6. 调用示例（与之前完全一致） curl -X POST http://localhost:8000/recognize \\ -H \"Content-Type: application/json\" \\ -d '{\"url\":\"http://192.168.80.47:16552/wsd.wav\"}' 返回示例：\n{ \"text\": \"我是登\", \"matched\": \"卧室灯\", \"id\": 4 } 7. 模型大小说明 模型\t体积\tCPU 推理速度 tiny\t39 MB\t非常快 base\t74 MB\t推荐（已用） small\t244 MB\t更准但更慢\n如需更准，把 load_model(“base”) 换成 “small” 即可，无需改其他代码。\nvosk-qwen 1. 系统依赖（一次性） sudo apt update sudo apt install -y python3-venv ffmpeg wget unzip git git-lfs 2. 虚拟环境 \u0026 依赖 python3 -m venv ~/qwen_stt source ~/qwen_stt/bin/activate pip install --upgrade pip pip install vosk transformers torch accelerate \\ pypinyin fuzzywuzzy python-Levenshtein \\ fastapi uvicorn aiohttp aiofiles requests 3. 下载模型（≈ 1.5 GB） mkdir -p ~/qwen_stt/models \u0026\u0026 cd ~/qwen_stt/models # 1. Vosk 中文小模型 wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip unzip vosk-model-small-cn-0.22.zip \u0026\u0026 mv vosk-model-small-cn-0.22 vosk # 2. Qwen2.5-0.5B-it git lfs install git clone https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct qwen2.5-0.5b 4. 单文件 app.py 保存为 ~/qwen_stt/app.py 即可运行。\n#!/usr/bin/env python3 \"\"\" Vosk 关键词（拼音匹配） + Qwen2.5-0.5B（未命中） - 命中关键词 → 返回 id - 未命中 → 用 Vosk 原文对话 \"\"\" import torch import os, tempfile, wave, json, asyncio, aiofiles, aiohttp from fastapi import FastAPI, HTTPException, UploadFile, File from pydantic import BaseModel, HttpUrl from vosk import Model, KaldiRecognizer from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz from transformers import AutoTokenizer, AutoModelForCausalLM ############################################################################### KEYWORD2ID = {\"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5} QWEN_PATH = os.path.join(os.path.dirname(__file__), \"models\", \"qwen2.5-0.5b\") VOSK_PATH = os.path.join(os.path.dirname(__file__), \"models\", \"vosk\") vosk_model = Model(VOSK_PATH) tok = AutoTokenizer.from_pretrained(QWEN_PATH, trust_remote_code=True) chat_model = AutoModelForCausalLM.from_pretrained( QWEN_PATH, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True ).eval() app = FastAPI(title=\"Vosk-Key-Qwen\", version=\"1.0\") ############################################################################### def wav2text(path: str) -\u003e str: with wave.open(path, \"rb\") as wf: if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000: raise ValueError(\"音频需 16 kHz 16-bit 单声道\") rec = KaldiRecognizer(vosk_model, wf.getframerate()) res = [] while True: data = wf.readframes(4000) if not data: break if rec.AcceptWaveform(data): res.append(json.loads(rec.Result())[\"text\"]) res.append(json.loads(rec.FinalResult())[\"text\"]) return \"\".join(res).strip() def fuzzy_match(text: str): \"\"\"拼音匹配\"\"\" txt_py = \"\".join(lazy_pinyin(text)) for kw, kid in KEYWORD2ID.items(): if fuzz.partial_ratio(\"\".join(lazy_pinyin(kw)), txt_py) \u003e= 70: return kw, kid return None, 0 def qwen_chat(prompt: str) -\u003e str: msgs = [{\"role\": \"user\", \"content\": prompt}] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) inputs = tok(text, return_tensors=\"pt\") out = chat_model.generate(**inputs, max_new_tokens=64, do_sample=True, temperature=0.7) return tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip() ############################################################################### class RecURL(BaseModel): url: HttpUrl async def _do_recognize(audio_path: str) -\u003e dict: text = await asyncio.to_thread(wav2text, audio_path) kw, kid = fuzzy_match(text) if kw: return {\"text\": text, \"keyword\": kw, \"id\": kid} reply = await asyncio.to_thread(qwen_chat, text) return {\"text\": text, \"keyword\": None, \"id\": 0, \"chat\": reply} ############################################################################### @app.post(\"/recognize\") async def recognize_url(req: RecURL): try: async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session: async with session.get(str(req.url)) as resp: if resp.status != 200: raise HTTPException(status_code=400, detail=f\"远程文件 {resp.status}\") data = await resp.read() with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp: tmp.write(data) tmp_path = tmp.name out_path = tmp_path + \"_16k.wav\" await asyncio.to_thread(lambda: os.system(f\"ffmpeg -y -i {tmp_path} -ar 16000 -ac 1 -sample_fmt s16 {out_path} \u003e/dev/null 2\u003e\u00261\")) return await asyncio.wait_for(_do_recognize(out_path), timeout=15) except asyncio.TimeoutError: raise HTTPException(status_code=504, detail=\"处理超时\") except Exception as e: raise HTTPException(status_code=500, detail=str(e)) finally: for p in (tmp_path, out_path) if 'tmp_path' in locals() else (): if os.path.exists(p): os.remove(p) @app.post(\"/upload\") async def upload_file(file: UploadFile = File(...)): try: with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp: async with aiofiles.open(tmp.name, \"wb\") as f: await f.write(await file.read()) out = tmp.name + \"_16k.wav\" await asyncio.to_thread(lambda: os.system(f\"ffmpeg -y -i {tmp.name} -ar 16000 -ac 1 -sample_fmt s16 {out} \u003e/dev/null 2\u003e\u00261\")) return await asyncio.wait_for(_do_recognize(out), timeout=15) except asyncio.TimeoutError: raise HTTPException(status_code=504, detail=\"处理超时\") except Exception as e: raise HTTPException(status_code=500, detail=str(e)) finally: for p in (tmp.name, out) if 'tmp.name' in locals() else (): if os.path.exists(p): os.remove(p) ############################################################################### if __name__ == \"__main__\": import uvicorn uvicorn.run(app, host=\"0.0.0.0\", port=8000) 启动 source ~/qwen_stt/bin/activate python ~/qwen_stt/app.py 调用示例 # 远程 curl -X POST http://localhost:8000/recognize \\ -H \"Content-Type: application/json\" \\ -d '{\"url\":\"http://192.168.80.47:16552/test.wav\"}' # 本地 curl -X POST http://localhost:8000/upload \\ -F \"file=@/home/ubuntu/test.wav\" 返回（命中卧室灯）：\n{ \"text\": \"帮我打开卧室灯\", \"keyword\": \"卧室灯\", \"id\": 4, \"chat\": \"已为您打开卧室灯！\" } 未命中：\n{ \"text\": \"今天天气怎么样\", \"keyword\": null, \"id\": 0, \"chat\": \"今天北京晴，25℃。\" } multipart err pip install python-multipart Qwen2.5-1.5B 0.5b-\u003e1.5b\ngit lfs install git clone https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct qwen2.5-1.5b 采样率 16kHz，保证vad ASR #!/usr/bin/env python3 import argparse import sys import os import json from pathlib import Path import numpy as np import sherpa_onnx import sounddevice as sd import threading import queue import time from collections import defaultdict import soundfile as sf from typing import Dict, List, Optional, Tuple from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz import torch from transformers import AutoTokenizer, AutoModelForCausalLM import resampy # 用于重采样 # ------------------ 路径工具 ------------------ current_dir = os.path.dirname(os.path.abspath(__file__)) parent_dir = os.path.dirname(current_dir) def get_relative_path(relative_path: str) -\u003e str: return os.path.join(parent_dir, relative_path) # ------------------ 关键词配置 ------------------ KEYWORD2ID = {\"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5} def fuzzy_match(text: str) -\u003e Tuple[Optional[str], int]: txt_py = \"\".join(lazy_pinyin(text)) for kw, kid in KEYWORD2ID.items(): if fuzz.partial_ratio(\"\".join(lazy_pinyin(kw)), txt_py) \u003e= 70: return kw, kid return None, 0 # ------------------ 大模型初始化 ------------------ QWEN_PATH = get_relative_path(\"/root/room/models/qwen2.5-0.5b\") tok = AutoTokenizer.from_pretrained(QWEN_PATH, trust_remote_code=True) chat_model = AutoModelForCausalLM.from_pretrained( \"Qwen/Qwen2.5-0.5B\", cache_dir=\"/root/room/models\", torch_dtype=\"auto\", device_map=\"auto\" ) def qwen_chat(prompt: str) -\u003e str: msgs = [{\"role\": \"user\", \"content\": prompt}] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) inputs = tok(text, return_tensors=\"pt\") with torch.no_grad(): out = chat_model.generate( **inputs, max_new_tokens=64, do_sample=True, temperature=0.7 ) return tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip() # ------------------ 声纹相关 ------------------ def load_speaker_embedding_model(model_path: str): config = sherpa_onnx.SpeakerEmbeddingExtractorConfig( model=model_path, num_threads=1, debug=False, provider=\"cpu\", ) if not config.validate(): raise ValueError(f\"Invalid config. {config}\") return sherpa_onnx.SpeakerEmbeddingExtractor(config) def load_speaker_file(speaker_file_path: str) -\u003e Dict[str, List[str]]: ans = defaultdict(list) with open(speaker_file_path) as f: for line in f: line = line.strip() if not line: continue fields = line.split() if len(fields) != 2: raise ValueError(f\"Invalid line: {line}\") speaker_name, filename = fields ans[speaker_name].append(filename) return ans def compute_speaker_embedding(filenames: List[str], extractor) -\u003e np.ndarray: ans = None for filename in filenames: samples, sr = sf.read(filename, always_2d=True, dtype=\"float32\") samples = samples[:, 0] samples = np.ascontiguousarray(samples) stream = extractor.create_stream() stream.accept_waveform(sr, samples) stream.input_finished() embedding = np.array(extractor.compute(stream)) if ans is None: ans = embedding else: ans += embedding return ans / len(filenames) def init_speaker_identification(): speaker_model = get_relative_path(\"/root/room/wespeaker_zh_cnceleb_resnet34.onnx\") speaker_file = get_relative_path(\"/root/room/speaker.txt\") extractor = load_speaker_embedding_model(speaker_model) speaker_data = load_speaker_file(speaker_file) manager = sherpa_onnx.SpeakerEmbeddingManager(extractor.dim) for name, filelist in speaker_data.items(): emb = compute_speaker_embedding(filelist, extractor) status = manager.add(name, emb) if not status: raise RuntimeError(f\"Failed to register speaker {name}\") return extractor, manager # ------------------ 唤醒词 ------------------ def wake_word_detection(input_device: int, input_sr: int) -\u003e bool: kws_dir = get_relative_path(\"/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01\") keyword_spotter = sherpa_onnx.KeywordSpotter( tokens=os.path.join(kws_dir, \"tokens.txt\"), encoder=os.path.join(kws_dir, \"encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), decoder=os.path.join(kws_dir, \"decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), joiner=os.path.join(kws_dir, \"joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx\"), num_threads=1, max_active_paths=4, keywords_file=os.path.join(kws_dir, \"keywords.txt\"), keywords_score=1.0, keywords_threshold=0.25, num_trailing_blanks=1, provider=\"cpu\", ) print(\"等待唤醒词...\") stream = keyword_spotter.create_stream() chunk = int(0.1 * input_sr) with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) stream.accept_waveform(input_sr, samples) while keyword_spotter.is_ready(stream): keyword_spotter.decode_stream(stream) result = keyword_spotter.get_result(stream) if result: print(f\"检测到唤醒词: {result}\") keyword_spotter.reset_stream(stream) return True # ------------------ 语音识别 ------------------ def speech_recognition(extractor, speaker_manager, input_device: int, input_sr: int) -\u003e Tuple[str, str]: target_sr = 16000 # VAD/ASR 采样率 recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer( paraformer=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx\"), tokens=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt\"), num_threads=1, sample_rate=target_sr, feature_dim=80, decoding_method=\"greedy_search\", debug=False, ) vad_config = sherpa_onnx.VadModelConfig() vad_config.silero_vad.model = get_relative_path(\"/root/room/silero_vad.onnx\") vad_config.silero_vad.min_silence_duration = 0.5 vad_config.silero_vad.min_speech_duration = 0.5 vad_config.sample_rate = target_sr vad = sherpa_onnx.VoiceActivityDetector(vad_config, buffer_size_in_seconds=100) window_size = vad_config.silero_vad.window_size print(\"请说出您的指令...\") chunk = int(0.1 * input_sr) audio_buffer = np.array([], dtype=\"float32\") with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) resampled = resampy.resample(samples, input_sr, target_sr) audio_buffer = np.concatenate([audio_buffer, resampled]) while len(audio_buffer) \u003e window_size: vad.accept_waveform(audio_buffer[:window_size]) audio_buffer = audio_buffer[window_size:] while not vad.empty(): if len(vad.front.samples) \u003c 0.5 * target_sr: vad.pop() continue stream = extractor.create_stream() stream.accept_waveform(target_sr, vad.front.samples) stream.input_finished() emb = np.array(extractor.compute(stream)) speaker = speaker_manager.search(emb, threshold=0.6) or \"unknown\" asr_stream = recognizer.create_stream() asr_stream.accept_waveform(target_sr, vad.front.samples) recognizer.decode_stream(asr_stream) text = asr_stream.result.text.strip() vad.pop() return text, speaker # ------------------ TTS ------------------ buffer = queue.Queue() started = False stopped = False killed = False event = threading.Event() tts_sr = 24000 output_sr = 48000 # 假设声卡输出 48kHz def generated_audio_callback(samples: np.ndarray, progress: float) -\u003e int: global started # 这里重采样 TTS 输出到声卡的采样率 resampled = resampy.resample(samples, tts_sr, output_sr) buffer.put(resampled) if not started: started = True return 0 if killed else 1 def play_audio_callback(outdata: np.ndarray, frames: int, time, status): if killed or (started and buffer.empty() and stopped): event.set() if buffer.empty(): outdata.fill(0) return n = 0 while n \u003c frames and not buffer.empty(): remaining = frames - n k = buffer.queue[0].shape[0] if remaining \u003c= k: outdata[n:, 0] = buffer.queue[0][:remaining] buffer.queue[0] = buffer.queue[0][remaining:] n = frames if buffer.queue[0].shape[0] == 0: buffer.get() break outdata[n:n + k, 0] = buffer.get() n += k if n \u003c frames: outdata[n:, 0] = 0 def tts_synthesis(text: str, output_device: int): global tts_sr, started, stopped tts_config = sherpa_onnx.OfflineTtsConfig( model=sherpa_onnx.OfflineTtsModelConfig( vits=sherpa_onnx.OfflineTtsVitsModelConfig( model=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/zh_CN-huayan-medium.onnx\"), tokens=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/tokens.txt\"), data_dir=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/espeak-ng-data\"), ), provider=\"cpu\", debug=False, num_threads=1, ), max_num_sentences=1, ) tts = sherpa_onnx.OfflineTts(tts_config) tts_sr = tts.sample_rate threading.Thread(target=lambda: sd.OutputStream( device=output_device, channels=1, callback=play_audio_callback, dtype=\"float32\", samplerate=output_sr, blocksize=1024 ).start()).start() event.clear() started = False stopped = False tts.generate(text, sid=0, speed=1.0, callback=generated_audio_callback) stopped = True event.wait() # ------------------ 主流程 ------------------ def select_devices(): print(\"=== 可用输入输出设备列表 ===\") print(sd.query_devices()) input_device = int(input(\"请输入输入设备ID: \")) output_device = int(input(\"请输入输出设备ID: \")) input_info = sd.query_devices(input_device) output_info = sd.query_devices(output_device) input_sr = int(input_info['default_samplerate']) print(f\"选择输入采样率: {input_sr} Hz\") return input_device, output_device, input_sr def main(): global killed try: input_device, output_device, input_sr = select_devices() print(\"初始化声纹识别...\") extractor, speaker_manager = init_speaker_identification() if wake_word_detection(input_device, input_sr): text, speaker = speech_recognition(extractor, speaker_manager, input_device, input_sr) print(f\"识别结果: {text} (说话人: {speaker})\") keyword, kid = fuzzy_match(text) if keyword: reply = f\"已为您操作【{keyword}】，编号{kid}\" else: reply = qwen_chat(text) print(\"回复:\", reply) tts_synthesis(reply, output_device) except KeyboardInterrupt: killed = True print(\"\\n程序已终止\") sys.exit(0) if __name__ == \"__main__\": main() qwen去除无关的符号 Qwen等大模型输出太自由，修改prompt\nmsgs = [ {\"role\": \"system\", \"content\": \"你是一个简洁的中文助手，只输出简短的答案，不要解释。\"}, {\"role\": \"user\", \"content\": prompt} ] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) 完整代码\ndef qwen_chat(prompt: str) -\u003e str: msgs = [ {\"role\": \"system\", \"content\": \"你是一个简洁的助手，只回答核心内容，用中文回答。\"}, {\"role\": \"user\", \"content\": prompt} ] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) inputs = tok(text, return_tensors=\"pt\") with torch.no_grad(): out = chat_model.generate( **inputs, max_new_tokens=64, do_sample=True, temperature=0.7, pad_token_id=tok.eos_token_id ) raw_output = tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip() # 额外清理 for token in [\"\", \"\", \".user\", \"🤨\", \"## 128000\"]: raw_output = raw_output.replace(token, \"\") reply = raw_output.split(\"\\n\")[0].strip() return reply 语速调整 tts.generate(text, sid=0, speed=1, callback=generated_audio_callback) 分离配置文件json prompt KEYWORD2ID 改成从 keywords.json 文件读取，把 system prompt 变成从外部 system_prompt.txt 文件读取，并且在程序里做错误处理和相应的加载。 下面是修改后的完整示例代码，我保持了结构清晰，你可以直接替换：\n#!/usr/bin/env python3 import argparse import sys import os import json from pathlib import Path import numpy as np import sherpa_onnx import sounddevice as sd import threading import queue import time from collections import defaultdict import soundfile as sf from typing import Dict, List, Optional, Tuple from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz import torch from transformers import AutoTokenizer, AutoModelForCausalLM import resampy # 用于重采样 # ------------------ 路径工具 ------------------ current_dir = os.path.dirname(os.path.abspath(__file__)) parent_dir = os.path.dirname(current_dir) def get_relative_path(relative_path: str) -\u003e str: return os.path.join(parent_dir, relative_path) # ------------------ 加载关键词配置 ------------------ def load_keywords() -\u003e Dict[str, int]: keywords_file = get_relative_path(\"keywords.json\") if not os.path.exists(keywords_file): raise FileNotFoundError(f\"未找到关键词文件: {keywords_file}\") with open(keywords_file, \"r\", encoding=\"utf-8\") as f: return json.load(f) KEYWORD2ID = load_keywords() def fuzzy_match(text: str) -\u003e Tuple[Optional[str], int]: txt_py = \"\".join(lazy_pinyin(text)) for kw, kid in KEYWORD2ID.items(): if fuzz.partial_ratio(\"\".join(lazy_pinyin(kw)), txt_py) \u003e= 70: return kw, kid return None, 0 # ------------------ 加载 system prompt ------------------ def load_system_prompt() -\u003e str: prompt_file = get_relative_path(\"system_prompt.txt\") if not os.path.exists(prompt_file): raise FileNotFoundError(f\"未找到 system prompt 文件: {prompt_file}\") with open(prompt_file, \"r\", encoding=\"utf-8\") as f: return f.read().strip() SYSTEM_PROMPT = load_system_prompt() # ------------------ 大模型初始化 ------------------ QWEN_PATH = get_relative_path(\"/root/room/models/qwen2.5-0.5b\") tok = AutoTokenizer.from_pretrained(QWEN_PATH, trust_remote_code=True) chat_model = AutoModelForCausalLM.from_pretrained( \"Qwen/Qwen2.5-0.5B\", cache_dir=\"/root/room/models\", torch_dtype=\"auto\", device_map=\"auto\" ) def qwen_chat(prompt: str) -\u003e str: msgs = [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": prompt} ] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) inputs = tok(text, return_tensors=\"pt\") with torch.no_grad(): out = chat_model.generate( **inputs, max_new_tokens=64, do_sample=True, temperature=0.7, pad_token_id=tok.eos_token_id ) raw_output = tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip() for token in [\"\", \"\", \".user\", \"🤨\", \"## 128000\", \"\\x0c\"]: raw_output = raw_output.replace(token, \"\") reply = raw_output.split(\"\\n\")[0].strip() return reply # ------------------ 声纹相关 ------------------ def load_speaker_embedding_model(model_path: str): config = sherpa_onnx.SpeakerEmbeddingExtractorConfig( model=model_path, num_threads=1, debug=False, provider=\"cpu\", ) if not config.validate(): raise ValueError(f\"Invalid config. {config}\") return sherpa_onnx.SpeakerEmbeddingExtractor(config) def load_speaker_file(speaker_file_path: str) -\u003e Dict[str, List[str]]: ans = defaultdict(list) with open(speaker_file_path) as f: for line in f: line = line.strip() if not line: continue fields = line.split() if len(fields) != 2: raise ValueError(f\"Invalid line: {line}\") speaker_name, filename = fields ans[speaker_name].append(filename) return ans def compute_speaker_embedding(filenames: List[str], extractor) -\u003e np.ndarray: ans = None for filename in filenames: samples, sr = sf.read(filename, always_2d=True, dtype=\"float32\") samples = samples[:, 0] samples = np.ascontiguousarray(samples) stream = extractor.create_stream() stream.accept_waveform(sr, samples) stream.input_finished() embedding = np.array(extractor.compute(stream)) if ans is None: ans = embedding else: ans += embedding return ans / len(filenames) def init_speaker_identification(): speaker_model = get_relative_path(\"/root/room/wespeaker_zh_cnceleb_resnet34.onnx\") speaker_file = get_relative_path(\"/root/room/speaker.txt\") extractor = load_speaker_embedding_model(speaker_model) speaker_data = load_speaker_file(speaker_file) manager = sherpa_onnx.SpeakerEmbeddingManager(extractor.dim) for name, filelist in speaker_data.items(): emb = compute_speaker_embedding(filelist, extractor) status = manager.add(name, emb) if not status: raise RuntimeError(f\"Failed to register speaker {name}\") return extractor, manager # ------------------ 唤醒词 ------------------ def wake_word_detection(input_device: int, input_sr: int) -\u003e bool: kws_dir = get_relative_path(\"/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01\") keyword_spotter = sherpa_onnx.KeywordSpotter( tokens=os.path.join(kws_dir, \"tokens.txt\"), encoder=os.path.join(kws_dir, \"encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), decoder=os.path.join(kws_dir, \"decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), joiner=os.path.join(kws_dir, \"joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx\"), num_threads=1, max_active_paths=4, keywords_file=os.path.join(kws_dir, \"keywords.txt\"), keywords_score=1.0, keywords_threshold=0.25, num_trailing_blanks=1, provider=\"cpu\", ) print(\"等待唤醒词...\") stream = keyword_spotter.create_stream() chunk = int(0.1 * input_sr) with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) stream.accept_waveform(input_sr, samples) while keyword_spotter.is_ready(stream): keyword_spotter.decode_stream(stream) result = keyword_spotter.get_result(stream) if result: print(f\"检测到唤醒词: {result}\") keyword_spotter.reset_stream(stream) return True # ------------------ 语音识别 ------------------ def speech_recognition(extractor, speaker_manager, input_device: int, input_sr: int) -\u003e Tuple[str, str]: target_sr = 16000 # VAD/ASR 采样率 recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer( paraformer=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx\"), tokens=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt\"), num_threads=1, sample_rate=target_sr, feature_dim=80, decoding_method=\"greedy_search\", debug=False, ) vad_config = sherpa_onnx.VadModelConfig() vad_config.silero_vad.model = get_relative_path(\"/root/room/silero_vad.onnx\") vad_config.silero_vad.min_silence_duration = 0.5 vad_config.silero_vad.min_speech_duration = 0.5 vad_config.sample_rate = target_sr vad = sherpa_onnx.VoiceActivityDetector(vad_config, buffer_size_in_seconds=100) window_size = vad_config.silero_vad.window_size print(\"请说出您的指令...\") chunk = int(0.1 * input_sr) audio_buffer = np.array([], dtype=\"float32\") with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) resampled = resampy.resample(samples, input_sr, target_sr) audio_buffer = np.concatenate([audio_buffer, resampled]) while len(audio_buffer) \u003e window_size: vad.accept_waveform(audio_buffer[:window_size]) audio_buffer = audio_buffer[window_size:] while not vad.empty(): if len(vad.front.samples) \u003c 0.5 * target_sr: vad.pop() continue stream = extractor.create_stream() stream.accept_waveform(target_sr, vad.front.samples) stream.input_finished() emb = np.array(extractor.compute(stream)) speaker = speaker_manager.search(emb, threshold=0.6) or \"unknown\" asr_stream = recognizer.create_stream() asr_stream.accept_waveform(target_sr, vad.front.samples) recognizer.decode_stream(asr_stream) text = asr_stream.result.text.strip() vad.pop() return text, speaker # ------------------ TTS ------------------ buffer = queue.Queue() started = False stopped = False killed = False event = threading.Event() tts_sr = 24000 output_sr = 48000 def generated_audio_callback(samples: np.ndarray, progress: float) -\u003e int: global started resampled = resampy.resample(samples, tts_sr, output_sr) buffer.put(resampled) if not started: started = True return 0 if killed else 1 def play_audio_callback(outdata: np.ndarray, frames: int, time, status): if killed or (started and buffer.empty() and stopped): event.set() if buffer.empty(): outdata.fill(0) return n = 0 while n \u003c frames and not buffer.empty(): remaining = frames - n k = buffer.queue[0].shape[0] if remaining \u003c= k: outdata[n:, 0] = buffer.queue[0][:remaining] buffer.queue[0] = buffer.queue[0][remaining:] n = frames if buffer.queue[0].shape[0] == 0: buffer.get() break outdata[n:n + k, 0] = buffer.get() n += k if n \u003c frames: outdata[n:, 0] = 0 def tts_synthesis(text: str, output_device: int): global tts_sr, started, stopped tts_config = sherpa_onnx.OfflineTtsConfig( model=sherpa_onnx.OfflineTtsModelConfig( vits=sherpa_onnx.OfflineTtsVitsModelConfig( model=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/zh_CN-huayan-medium.onnx\"), tokens=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/tokens.txt\"), data_dir=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/espeak-ng-data\"), ), provider=\"cpu\", debug=False, num_threads=1, ), max_num_sentences=1, ) tts = sherpa_onnx.OfflineTts(tts_config) tts_sr = tts.sample_rate threading.Thread(target=lambda: sd.OutputStream( device=output_device, channels=1, callback=play_audio_callback, dtype=\"float32\", samplerate=output_sr, blocksize=1024 ).start()).start() event.clear() started = False stopped = False tts.generate(text, sid=0, speed=1, callback=generated_audio_callback) stopped = True event.wait() # ------------------ 主流程 ------------------ def select_devices(): print(\"=== 可用输入输出设备列表 ===\") print(sd.query_devices()) input_device = int(input(\"请输入输入设备ID: \")) output_device = int(input(\"请输入输出设备ID: \")) input_info = sd.query_devices(input_device) output_info = sd.query_devices(output_device) input_sr = int(input_info['default_samplerate']) print(f\"选择输入采样率: {input_sr} Hz\") return input_device, output_device, input_sr def main(): global killed try: input_device, output_device, input_sr = select_devices() print(\"初始化声纹识别...\") extractor, speaker_manager = init_speaker_identification() if wake_word_detection(input_device, input_sr): text, speaker = speech_recognition(extractor, speaker_manager, input_device, input_sr) print(f\"识别结果: {text} (说话人: {speaker})\") keyword, kid = fuzzy_match(text) if keyword: reply = f\"已为您操作【{keyword}】，编号{kid}\" else: reply = qwen_chat(text) print(\"回复:\", reply) tts_synthesis(reply, output_device) except KeyboardInterrupt: killed = True print(\"\\n程序已终止\") sys.exit(0) if __name__ == \"__main__\": main() keywords.json\n{ \"空调\": 1, \"电视\": 2, \"风扇\": 3, \"卧室灯\": 4, \"客厅灯\": 5 } system_prompt.txt\n你是一个简洁的助手，只回答核心内容，用中文回答。 外部编辑 keywords.json 和 system_prompt.txt 既可\n先加载声纹再选设备 extractor, speaker_manager = init_speaker_identification() input_device, output_device, input_sr = select_devices() 语音识别 分离 from speechbrain.inference.separation import SepformerSeparation as separator import torchaudio import torchaudio.transforms as T import os # 输入混合音频文件 input_path = \"your_mix.wav\" # 加载原始音频 waveform, sample_rate = torchaudio.load(input_path) # 加载模型 model = separator.from_hparams( source=\"speechbrain/sepformer-wsj02mix\", # 这个模型要求 8k 采样率 savedir=\"pretrained_models/sepformer-wsj02mix\" ) # 从模型配置中获取目标采样率 target_sample_rate = model.hparams.sample_rate if hasattr(model.hparams, \"sample_rate\") else sample_rate # 如果采样率不同，自动转换 if sample_rate != target_sample_rate: print(f\"采样率不匹配：{sample_rate} → {target_sample_rate}，正在转换...\") resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate) waveform = resampler(waveform) sample_rate = target_sample_rate # 临时保存转换后的音频（因为 SpeechBrain 的接口要求文件路径） tmp_path = \"temp_resampled.wav\" torchaudio.save(tmp_path, waveform, sample_rate) # 分离 est_sources = model.separate_file(path=tmp_path) # 自动检测人数 num_speakers = est_sources.shape[2] print(f\"检测到 {num_speakers} 个声源\") # 保存每个分离结果（用原采样率保存） os.makedirs(\"separated\", exist_ok=True) for i in range(num_speakers): out_path = f\"separated/speaker_{i+1}.wav\" torchaudio.save(out_path, est_sources[:, :, i].detach().cpu(), sample_rate) print(f\"已保存: {out_path}\") # 删除临时文件 os.remove(tmp_path) 依赖\nsudo apt update sudo apt install ffmpeg pip install soundfile sudo apt install libsndfile1 python3 app.py stt 分离 英文 web pip install speechbrain torchaudio fastapi uvicorn python-multipart 项目结构 speechbrain_stt_project/ ├── app.py # FastAPI 后端主程序 ├── static/ │ └── index.html # 前端静态页面 ├── pretrained_models/ # 预训练模型目录（自动下载或手动放置） ├── uploads/ # 上传文件目录（运行时自动创建） ├── separated/ # 分离音频输出目录（运行时自动创建） ├── requirements.txt # Python依赖 └── Dockerfile # 容器构建文件 文件内容示例 app.py\nfrom fastapi import FastAPI, UploadFile, File from fastapi.responses import JSONResponse, HTMLResponse from fastapi.staticfiles import StaticFiles import torchaudio import torchaudio.transforms as T import os import shutil from speechbrain.inference import SepformerSeparation as separator from speechbrain.pretrained import EncoderDecoderASR app = FastAPI() # 挂载静态文件夹，访问 http://host:port/ app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\") # 加载模型 sep_model = separator.from_hparams( source=\"speechbrain/sepformer-wsj02mix\", savedir=\"pretrained_models/sepformer-wsj02mix\" ) asr_model = EncoderDecoderASR.from_hparams( source=\"speechbrain/asr-transformer-transformerlm-librispeech\", savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\" ) @app.get(\"/\") async def homepage(): with open(\"static/index.html\", \"r\", encoding=\"utf-8\") as f: html_content = f.read() return HTMLResponse(content=html_content) @app.post(\"/upload/\") async def upload_audio(file: UploadFile = File(...)): os.makedirs(\"uploads\", exist_ok=True) os.makedirs(\"separated\", exist_ok=True) file_path = f\"uploads/{file.filename}\" with open(file_path, \"wb\") as f: shutil.copyfileobj(file.file, f) waveform, sample_rate = torchaudio.load(file_path) target_sr = sep_model.hparams.sample_rate if hasattr(sep_model.hparams, \"sample_rate\") else sample_rate if sample_rate != target_sr: resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sr) waveform = resampler(waveform) sample_rate = target_sr tmp_path = \"uploads/temp_resampled.wav\" torchaudio.save(tmp_path, waveform, sample_rate) else: tmp_path = file_path est_sources = sep_model.separate_file(path=tmp_path) num_speakers = est_sources.shape[2] results = [] for i in range(num_speakers): speaker_wav = f\"separated/{file.filename}_speaker_{i+1}.wav\" torchaudio.save(speaker_wav, est_sources[:, :, i].detach().cpu(), sample_rate) transcription = asr_model.transcribe_file(speaker_wav) results.append({ \"speaker\": i + 1, \"transcript\": transcription }) if tmp_path != file_path: os.remove(tmp_path) return JSONResponse(content={\"num_speakers\": num_speakers, \"results\": results}) static/index.html\n\u003c!DOCTYPE html\u003e \u003chtml lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\" /\u003e \u003ctitle\u003e多说话人分离及语音识别\u003c/title\u003e \u003cstyle\u003e body { font-family: Arial, sans-serif; margin: 2rem; } button { padding: 0.5rem 1rem; margin-top: 0.5rem; } #results { margin-top: 1rem; white-space: pre-wrap; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch2\u003e上传混合音频，自动分离并转写\u003c/h2\u003e \u003cinput type=\"file\" id=\"audioFile\" accept=\".wav,.mp3\" /\u003e \u003cbr /\u003e \u003cbutton onclick=\"upload()\"\u003e上传识别\u003c/button\u003e \u003cdiv id=\"results\"\u003e\u003c/div\u003e \u003cscript\u003e async function upload() { const fileInput = document.getElementById(\"audioFile\"); if (!fileInput.files.length) { alert(\"请选择音频文件！\"); return; } const file = fileInput.files[0]; const formData = new FormData(); formData.append(\"file\", file); const resultsDiv = document.getElementById(\"results\"); resultsDiv.textContent = \"识别中，请稍候...\"; try { const response = await fetch(\"/upload/\", { method: \"POST\", body: formData }); if (!response.ok) throw new Error(\"上传失败\"); const data = await response.json(); let text = `检测到说话人数：${data.num_speakers}\\n\\n`; data.results.forEach(r =\u003e { text += `说话人 ${r.speaker}:\\n${r.transcript}\\n\\n`; }); resultsDiv.textContent = text; } catch (err) { resultsDiv.textContent = \"错误：\" + err.message; } } \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e requirements.txt\nfastapi uvicorn speechbrain torchaudio python-multipart Dockerfile\n# 基础镜像：官方 PyTorch 镜像（带 CUDA 也可以） FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime # 设置工作目录 WORKDIR /app # 复制依赖文件和程序 COPY requirements.txt . COPY app.py . COPY static ./static # 安装依赖 RUN pip install --upgrade pip RUN pip install -r requirements.txt # 运行端口 EXPOSE 8000 # 启动命令 CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] 启动与调试 本地调试：\npip install -r requirements.txt uvicorn app:app --reload --host 0.0.0.0 --port 8000 浏览器打开 http://localhost:8000 上传音频测试。\nDocker 构建启动：\ndocker build -t speechbrain-stt . docker run -p 8000:8000 speechbrain-stt 访问 http://localhost:8000\n扩展 前端做音频播放分离音频文件功能（接口返回的 wav 文件路径）\n后端改用异步队列（如 Celery + Redis）处理分离和识别，提升吞吐\n支持多种音频格式上传，后台自动转换成 wav\n部署时配置 HTTPS，防止数据明文传输\n加入身份验证保护接口，防止滥用\n中文识别模型 asr-wav2vec2-commonvoice-14-zh-CN from speechbrain.inference.ASR import EncoderASR m = EncoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-commonvoice-14-zh-CN\", savedir=\"tmp\") print(m.transcribe_file(\"separated/speaker_1.wav\")) Whisper from faster_whisper import WhisperModel m = WhisperModel(\"small\", device=\"cpu\") segments, info = m.transcribe(\"separated/xxx_speaker_1.wav\", beam_size=5, language=\"zh\") print(\"\".join([s.text for s in segments])) 结果：\n[ctranslate2] [thread 460025] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead. 哈喽哈喽李华李华 SpeechBrain \u0026 TTS SpeechBrain 分离模型\n#!/usr/bin/env python3 # -*- coding: utf-8 -*- import argparse import sys import os import json from pathlib import Path import numpy as np import sherpa_onnx import sounddevice as sd import threading import queue import time from collections import defaultdict import soundfile as sf from typing import Dict, List, Optional, Tuple from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz import torch from transformers import AutoTokenizer, AutoModelForCausalLM import resampy # 用于重采样 # ========== 新增：SpeechBrain 分离相关 ========== from speechbrain.inference import SepformerSeparation as separator import torchaudio.transforms as T # ------------------ 路径工具 ------------------ current_dir = os.path.dirname(os.path.abspath(__file__)) parent_dir = os.path.dirname(current_dir) def get_relative_path(relative_path: str) -\u003e str: return os.path.join(parent_dir, relative_path) # ------------------ 加载关键词配置 ------------------ def load_keywords() -\u003e Dict[str, int]: keywords_file = get_relative_path(\"/root/room/keywords.json\") if not os.path.exists(keywords_file): raise FileNotFoundError(f\"未找到关键词文件: {keywords_file}\") with open(keywords_file, \"r\", encoding=\"utf-8\") as f: return json.load(f) KEYWORD2ID = load_keywords() def fuzzy_match(text: str) -\u003e Tuple[Optional[str], int]: txt_py = \"\".join(lazy_pinyin(text)) for kw, kid in KEYWORD2ID.items(): if fuzz.partial_ratio(\"\".join(lazy_pinyin(kw)), txt_py) \u003e= 70: return kw, kid return None, 0 # ------------------ 加载 system prompt ------------------ def load_system_prompt() -\u003e str: prompt_file = get_relative_path(\"/root/room/system_prompt.txt\") if not os.path.exists(prompt_file): raise FileNotFoundError(f\"未找到 system prompt 文件: {prompt_file}\") with open(prompt_file, \"r\", encoding=\"utf-8\") as f: return f.read().strip() SYSTEM_PROMPT = load_system_prompt() # ------------------ 大模型初始化 ------------------ QWEN_PATH = get_relative_path(\"/root/room/models/qwen2.5-0.5b\") tok = AutoTokenizer.from_pretrained(QWEN_PATH, trust_remote_code=True) chat_model = AutoModelForCausalLM.from_pretrained( \"Qwen/Qwen2.5-0.5B\", cache_dir=\"/root/room/models\", torch_dtype=\"auto\", device_map=\"auto\" ) def qwen_chat(prompt: str) -\u003e str: msgs = [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": prompt} ] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) inputs = tok(text, return_tensors=\"pt\") with torch.no_grad(): out = chat_model.generate( **inputs, max_new_tokens=64, do_sample=True, temperature=0.7, pad_token_id=tok.eos_token_id ) raw_output = tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip() for token in [\"\", \"\", \".user\", \"🤨\", \"## 128000\", \"\\x0c\"]: raw_output = raw_output.replace(token, \"\") reply = raw_output.split(\"\\n\")[0].strip() return reply # ------------------ 声纹相关 ------------------ def load_speaker_embedding_model(model_path: str): config = sherpa_onnx.SpeakerEmbeddingExtractorConfig( model=model_path, num_threads=1, debug=False, provider=\"cpu\", ) if not config.validate(): raise ValueError(f\"Invalid config. {config}\") return sherpa_onnx.SpeakerEmbeddingExtractor(config) def load_speaker_file(speaker_file_path: str) -\u003e Dict[str, List[str]]: ans = defaultdict(list) with open(speaker_file_path) as f: for line in f: line = line.strip() if not line: continue fields = line.split() if len(fields) != 2: raise ValueError(f\"Invalid line: {line}\") speaker_name, filename = fields ans[speaker_name].append(filename) return ans def compute_speaker_embedding(filenames: List[str], extractor) -\u003e np.ndarray: ans = None for filename in filenames: samples, sr = sf.read(filename, always_2d=True, dtype=\"float32\") samples = samples[:, 0] samples = np.ascontiguousarray(samples) stream = extractor.create_stream() stream.accept_waveform(sr, samples) stream.input_finished() embedding = np.array(extractor.compute(stream)) if ans is None: ans = embedding else: ans += embedding return ans / len(filenames) def init_speaker_identification(): speaker_model = get_relative_path(\"/root/room/wespeaker_zh_cnceleb_resnet34.onnx\") speaker_file = get_relative_path(\"/root/room/speaker.txt\") extractor = load_speaker_embedding_model(speaker_model) speaker_data = load_speaker_file(speaker_file) manager = sherpa_onnx.SpeakerEmbeddingManager(extractor.dim) for name, filelist in speaker_data.items(): emb = compute_speaker_embedding(filelist, extractor) status = manager.add(name, emb) if not status: raise RuntimeError(f\"Failed to register speaker {name}\") return extractor, manager def wake_word_detection_with_speaker(input_device: int, input_sr: int, extractor, speaker_manager) -\u003e Optional[str]: kws_dir = get_relative_path(\"/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01\") keyword_spotter = sherpa_onnx.KeywordSpotter( tokens=os.path.join(kws_dir, \"tokens.txt\"), encoder=os.path.join(kws_dir, \"encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), decoder=os.path.join(kws_dir, \"decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), joiner=os.path.join(kws_dir, \"joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx\"), num_threads=1, max_active_paths=4, keywords_file=os.path.join(kws_dir, \"keywords.txt\"), keywords_score=1.0, keywords_threshold=0.25, num_trailing_blanks=1, provider=\"cpu\", ) print(\"等待唤醒词（需声纹匹配）...\") stream = keyword_spotter.create_stream() chunk = int(0.1 * input_sr) buffer_seconds = 2.0 # 缓存唤醒词语音用于声纹识别 max_buffer_len = int(buffer_seconds * input_sr) audio_buffer = np.zeros((0,), dtype=np.float32) with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) stream.accept_waveform(input_sr, samples) # 语音缓存 audio_buffer = np.concatenate((audio_buffer, samples)) if len(audio_buffer) \u003e max_buffer_len: audio_buffer = audio_buffer[-max_buffer_len:] while keyword_spotter.is_ready(stream): keyword_spotter.decode_stream(stream) result = keyword_spotter.get_result(stream) if result: print(f\"检测到唤醒词: {result}\") keyword_spotter.reset_stream(stream) # 进行声纹识别 print(\"正在进行声纹识别验证...\") audio = np.ascontiguousarray(audio_buffer) emb_stream = extractor.create_stream() emb_stream.accept_waveform(input_sr, audio) emb_stream.input_finished() emb = np.array(extractor.compute(emb_stream)) speaker = speaker_manager.search(emb, threshold=0.6) if speaker: print(f\"声纹识别成功，说话人: {speaker}\") return speaker else: print(\"声纹识别失败，忽略本次唤醒\") return None # ------------------ 唤醒词（不带声纹，仅示例备用） ------------------ def wake_word_detection(input_device: int, input_sr: int) -\u003e bool: kws_dir = get_relative_path(\"/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01\") keyword_spotter = sherpa_onnx.KeywordSpotter( tokens=os.path.join(kws_dir, \"tokens.txt\"), encoder=os.path.join(kws_dir, \"encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), decoder=os.path.join(kws_dir, \"decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), joiner=os.path.join(kws_dir, \"joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx\"), num_threads=1, max_active_paths=4, keywords_file=os.path.join(kws_dir, \"keywords.txt\"), keywords_score=1.0, keywords_threshold=0.25, num_trailing_blanks=1, provider=\"cpu\", ) print(\"等待唤醒词...\") stream = keyword_spotter.create_stream() chunk = int(0.1 * input_sr) with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) stream.accept_waveform(input_sr, samples) while keyword_spotter.is_ready(stream): keyword_spotter.decode_stream(stream) result = keyword_spotter.get_result(stream) if result: print(f\"检测到唤醒词: {result}\") keyword_spotter.reset_stream(stream) return True # ------------------ 语音识别（保留原有函数可按需使用） ------------------ def speech_recognition(extractor, speaker_manager, input_device: int, input_sr: int) -\u003e Tuple[str, str]: target_sr = 16000 # VAD/ASR 采样率 recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer( paraformer=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx\"), tokens=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt\"), num_threads=1, sample_rate=target_sr, feature_dim=80, decoding_method=\"greedy_search\", debug=False, ) vad_config = sherpa_onnx.VadModelConfig() vad_config.silero_vad.model = get_relative_path(\"/root/room/silero_vad.onnx\") vad_config.silero_vad.min_silence_duration = 0.5 vad_config.silero_vad.min_speech_duration = 0.5 vad_config.sample_rate = target_sr vad = sherpa_onnx.VoiceActivityDetector(vad_config, buffer_size_in_seconds=100) window_size = vad_config.silero_vad.window_size print(\"请说出您的指令...\") chunk = int(0.1 * input_sr) audio_buffer = np.array([], dtype=\"float32\") with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) resampled = resampy.resample(samples, input_sr, target_sr) audio_buffer = np.concatenate([audio_buffer, resampled]) while len(audio_buffer) \u003e window_size: vad.accept_waveform(audio_buffer[:window_size]) audio_buffer = audio_buffer[window_size:] while not vad.empty(): if len(vad.front.samples) \u003c 0.5 * target_sr: vad.pop() continue stream = extractor.create_stream() stream.accept_waveform(target_sr, vad.front.samples) stream.input_finished() emb = np.array(extractor.compute(stream)) speaker = speaker_manager.search(emb, threshold=0.6) or \"unknown\" asr_stream = recognizer.create_stream() asr_stream.accept_waveform(target_sr, vad.front.samples) recognizer.decode_stream(asr_stream) text = asr_stream.result.text.strip() vad.pop() return text, speaker # ------------------ TTS ------------------ buffer = queue.Queue() started = False stopped = False killed = False event = threading.Event() tts_sr = 24000 output_sr = 48000 def generated_audio_callback(samples: np.ndarray, progress: float) -\u003e int: global started resampled = resampy.resample(samples, tts_sr, output_sr) buffer.put(resampled) if not started: started = True return 0 if killed else 1 def play_audio_callback(outdata: np.ndarray, frames: int, time, status): if killed or (started and buffer.empty() and stopped): event.set() if buffer.empty(): outdata.fill(0) return n = 0 while n \u003c frames and not buffer.empty(): remaining = frames - n k = buffer.queue[0].shape[0] if remaining \u003c= k: outdata[n:, 0] = buffer.queue[0][:remaining] buffer.queue[0] = buffer.queue[0][remaining:] n = frames if buffer.queue[0].shape[0] == 0: buffer.get() break outdata[n:n + k, 0] = buffer.get() n += k if n \u003c frames: outdata[n:, 0] = 0 def tts_synthesis(text: str, output_device: int): global tts_sr, started, stopped tts_config = sherpa_onnx.OfflineTtsConfig( model=sherpa_onnx.OfflineTtsModelConfig( vits=sherpa_onnx.OfflineTtsVitsModelConfig( model=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/zh_CN-huayan-medium.onnx\"), tokens=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/tokens.txt\"), data_dir=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/espeak-ng-data\"), ), provider=\"cpu\", debug=False, num_threads=1, ), max_num_sentences=1, ) tts = sherpa_onnx.OfflineTts(tts_config) tts_sr = tts.sample_rate threading.Thread(target=lambda: sd.OutputStream( device=output_device, channels=1, callback=play_audio_callback, dtype=\"float32\", samplerate=output_sr, blocksize=1024 ).start()).start() event.clear() started = False stopped = False tts.generate(text, sid=0, speed=1, callback=generated_audio_callback) stopped = True event.wait() # ------------------ SpeechBrain 分离：初始化与工具函数 ------------------ print(\"初始化 SpeechBrain 分离模型（Sepformer）... 这可能需要下载模型到 pretrained_models/sepformer-wsj02mix\") try: sep_model = separator.from_hparams( source=\"speechbrain/sepformer-wsj02mix\", savedir=\"pretrained_models/sepformer-wsj02mix\", run_opts={\"device\": \"cpu\"}, ) SEP_SR = getattr(sep_model.hparams, \"sample_rate\", 8000) # 通常 sepformer 用 8k print(f\"Sepformer 采样率: {SEP_SR}\") except Exception as e: print(\"加载 Sepformer 模型失败:\", e) sep_model = None SEP_SR = 8000 # 我们的 ASR / 声纹 统一用 16k（与 原逻辑一致） ASR_SR = 16000 MIN_AUDIO_SECONDS = 0.3 # 跳过太短片段（秒） MIN_AUDIO_SAMPLES = int(MIN_AUDIO_SECONDS * ASR_SR) def _extract_sources_from_sep_output(est_np: np.ndarray, max_sources=16) -\u003e List[np.ndarray]: \"\"\" 将 sep_model 输出 ndarray 转为 [n_src, time] 的 list（numpy 1D float32）。 尽量自动识别哪个轴是源、哪个轴是时间。 \"\"\" if est_np is None: return [] if est_np.ndim == 1: return [] # 先尝试找到较小的轴（\u003e1 且 \u003c= max_sources）作为 source 轴 src_axis = None time_axis = None for i, s in enumerate(est_np.shape): if 1 \u003c s \u003c= max_sources: src_axis = i if s \u003e 1000: time_axis = i if src_axis is None: # 退化策略：2D 情况下，较小维度当作 src if est_np.ndim == 2: if est_np.shape[0] \u003c est_np.shape[1]: src_axis, time_axis = 0, 1 else: src_axis, time_axis = 1, 0 elif est_np.ndim == 3: # 常见： (1, n_src, time) 或 (channels, time, n_src) # 优先选 size 小的轴作为 src sizes = list(est_np.shape) min_idx = int(np.argmin(sizes)) # 保证不是 time axis src_axis = min_idx # time axis 取最大轴 time_axis = int(np.argmax(sizes)) else: # 最保守策略 src_axis = 0 time_axis = est_np.ndim - 1 if time_axis is None: time_axis = max(range(est_np.ndim), key=lambda i: est_np.shape[i] if i != src_axis else -1) # 将 array 重新排列为 (src, time, ...) # 我们把 time 移到 axis=1，src 移到 axis=0 arr = np.moveaxis(est_np, (src_axis, time_axis), (0, 1)) # 现在 arr.shape[0] = n_src, arr.shape[1] = time n_src = arr.shape[0] sources = [] for i in range(n_src): s = arr[i] # 若还有额外轴（如 channels），把它们 flatten 或取第一通道 if s.ndim \u003e 1: # 把多通道求均值为单通道 s = np.mean(s, axis=tuple(range(1, s.ndim))) sources.append(s.astype(np.float32).reshape(-1)) return sources def separate_speakers(mixed_audio: np.ndarray, input_sr: int) -\u003e List[np.ndarray]: \"\"\" 返回 list，每项为以 SEP_SR 为采样率的 numpy 1D float32 信号（单通道）。 （后续处理会把它重采样为 ASR_SR） \"\"\" if sep_model is None: print(\"Sepformer 未初始化，跳过分离，直接返回原始音频\") return [mixed_audio] # 准备模型输入：sep_model 要求的形状可能是 [batch, time] 或 [batch, 1, time] 等 # 我们统一提供 [1, time] if input_sr != SEP_SR: # 用 resampy 将输入混音重采样到 sep 模型采样率 mixed_audio_sep = resampy.resample(mixed_audio, input_sr, SEP_SR).astype(np.float32) else: mixed_audio_sep = mixed_audio.astype(np.float32) audio_tensor = torch.from_numpy(mixed_audio_sep).float().unsqueeze(0) # [1, time] with torch.no_grad(): est = sep_model.separate_batch(audio_tensor) # 返回 tensor，shape 可能多样 est_np = est.cpu().numpy() sources = _extract_sources_from_sep_output(est_np, max_sources=16) # 过滤极短或静音的源（避免后续造成 extractor 报错） filtered = [] for i, s in enumerate(sources): if s is None or s.size == 0: continue power = float(np.mean(np.abs(s))) if s.shape[0] \u003c 10: # 非常短 continue # 过滤非常静的（阈值可根据实际调整） if power \u003c 1e-5: continue filtered.append(s) return filtered # ------------------ 主流程相关（设备选择 / 录音） ------------------ def select_devices(): print(\"=== 可用输入输出设备列表 ===\") print(sd.query_devices()) input_device = int(input(\"请输入输入设备ID: \")) output_device = int(input(\"请输入输出设备ID: \")) input_info = sd.query_devices(input_device) output_info = sd.query_devices(output_device) input_sr = int(input_info['default_samplerate']) print(f\"选择输入采样率: {input_sr} Hz\") return input_device, output_device, input_sr def record_mixed_audio(input_device: int, input_sr: int, duration: float = 5.0): \"\"\"从麦克风录制一段混合音频（单声道 float32）\"\"\" print(f\"开始录音 {duration} 秒...\") with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: frames = [] total_chunks = int(duration / 0.1) for _ in range(total_chunks): samples, _ = s.read(int(0.1 * input_sr)) frames.append(samples.reshape(-1)) mixed_audio = np.concatenate(frames).astype(np.float32) print(\"录音完成\") return mixed_audio # ------------------ 主流程 ------------------ def main(): global killed try: print(\"初始化声纹识别...\") extractor, speaker_manager = init_speaker_identification() input_device, output_device, input_sr = select_devices() # 预创建 ASR recognizer（避免在每个说话人上重复创建） recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer( paraformer=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx\"), tokens=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt\"), num_threads=1, sample_rate=ASR_SR, feature_dim=80, decoding_method=\"greedy_search\", debug=False, ) speaker = wake_word_detection_with_speaker(input_device, input_sr, extractor, speaker_manager) if speaker: # 1) 录制混合音频 mixed_audio = record_mixed_audio(input_device, input_sr, duration=5.0) # 2) 分离 separated = separate_speakers(mixed_audio, input_sr) print(f\"分离后有效说话人数量: {len(separated)}\") if len(separated) == 0: print(\"未检测到有效分离结果，回退为直接识别整段语音\") # 回退：把混合音频重采样到 ASR_SR 并识别 mixed_resampled = resampy.resample(mixed_audio, input_sr, ASR_SR).astype(np.float32) # 声纹与识别（与单路流程相同） if mixed_resampled.size \u003e= MIN_AUDIO_SAMPLES: try: emb_stream = extractor.create_stream() emb_stream.accept_waveform(ASR_SR, mixed_resampled) emb_stream.input_finished() emb = np.array(extractor.compute(emb_stream)) spk_name = speaker_manager.search(emb, threshold=0.6) or \"unknown\" except Exception as e: print(\"声纹识别失败:\", e) spk_name = \"unknown\" try: asr_stream = recognizer.create_stream() asr_stream.accept_waveform(ASR_SR, mixed_resampled) recognizer.decode_stream(asr_stream) text = asr_stream.result.text.strip() except Exception as e: print(\"ASR 失败:\", e) text = \"\" print(f\"识别结果: {text} (说话人: {spk_name})\") keyword, kid = fuzzy_match(text) if keyword: reply = f\"已为您操作【{keyword}】，编号{kid}\" else: reply = qwen_chat(text) print(\"回复:\", reply) tts_synthesis(reply, output_device) else: print(\"录音过短，无法识别。\") return # 3) 对每一路分别做 声纹 -\u003e ASR -\u003e reply -\u003e TTS for i, sp_audio_sep in enumerate(separated): print(f\"\\n=== 处理第 {i+1} 个说话人 ===\") # 先把 sep 模型的采样率转换到 ASR_SR（如果 sep 模型采样率不同） if SEP_SR != ASR_SR: try: sp_audio = resampy.resample(sp_audio_sep, SEP_SR, ASR_SR).astype(np.float32) except Exception as e: print(\"重采样出错，跳过该说话人:\", e) continue else: sp_audio = sp_audio_sep.astype(np.float32) # 过滤太短或静音 if sp_audio.size \u003c MIN_AUDIO_SAMPLES: print(f\"第 {i+1} 路音频过短（{sp_audio.size} samples），跳过\") continue if float(np.mean(np.abs(sp_audio))) \u003c 1e-5: print(f\"第 {i+1} 路音频过静，跳过\") continue # 声纹识别（保护性 try） try: emb_stream = extractor.create_stream() emb_stream.accept_waveform(ASR_SR, sp_audio) emb_stream.input_finished() emb = np.array(extractor.compute(emb_stream)) spk_name = speaker_manager.search(emb, threshold=0.6) or \"unknown\" except Exception as e: print(\"声纹识别出错（跳过声纹或标为 unknown）:\", e) spk_name = \"unknown\" # ASR try: asr_stream = recognizer.create_stream() asr_stream.accept_waveform(ASR_SR, sp_audio) recognizer.decode_stream(asr_stream) text = asr_stream.result.text.strip() except Exception as e: print(\"ASR 失败:\", e) text = \"\" print(f\"识别结果: {text} (说话人: {spk_name})\") # 生成回复并 TTS keyword, kid = fuzzy_match(text) if keyword: reply = f\"已为您操作【{keyword}】，编号{kid}\" else: reply = qwen_chat(text) if text.strip() else \"抱歉，我没有听清楚。\" print(\"回复:\", reply) tts_synthesis(reply, output_device) except KeyboardInterrupt: killed = True print(\"\\n程序已终止\") sys.exit(0) if __name__ == \"__main__\": main() TTS线程优化 调用都复用同一个流来播放\n#!/usr/bin/env python3 # -*- coding: utf-8 -*- import argparse import sys import os import json from pathlib import Path import numpy as np import sherpa_onnx import sounddevice as sd import threading import queue import time from collections import defaultdict import soundfile as sf from typing import Dict, List, Optional, Tuple from pypinyin import lazy_pinyin from fuzzywuzzy import fuzz import torch from transformers import AutoTokenizer, AutoModelForCausalLM import resampy # 用于重采样 # ========== 新增：SpeechBrain 分离相关 ========== from speechbrain.inference import SepformerSeparation as separator import torchaudio.transforms as T # ------------------ 路径工具 ------------------ current_dir = os.path.dirname(os.path.abspath(__file__)) parent_dir = os.path.dirname(current_dir) def get_relative_path(relative_path: str) -\u003e str: return os.path.join(parent_dir, relative_path) # ------------------ 加载关键词配置 ------------------ def load_keywords() -\u003e Dict[str, int]: keywords_file = get_relative_path(\"/root/room/keywords.json\") if not os.path.exists(keywords_file): raise FileNotFoundError(f\"未找到关键词文件: {keywords_file}\") with open(keywords_file, \"r\", encoding=\"utf-8\") as f: return json.load(f) KEYWORD2ID = load_keywords() def fuzzy_match(text: str) -\u003e Tuple[Optional[str], int]: txt_py = \"\".join(lazy_pinyin(text)) for kw, kid in KEYWORD2ID.items(): if fuzz.partial_ratio(\"\".join(lazy_pinyin(kw)), txt_py) \u003e= 70: return kw, kid return None, 0 # ------------------ 加载 system prompt ------------------ def load_system_prompt() -\u003e str: prompt_file = get_relative_path(\"/root/room/system_prompt.txt\") if not os.path.exists(prompt_file): raise FileNotFoundError(f\"未找到 system prompt 文件: {prompt_file}\") with open(prompt_file, \"r\", encoding=\"utf-8\") as f: return f.read().strip() SYSTEM_PROMPT = load_system_prompt() # ------------------ 大模型初始化 ------------------ QWEN_PATH = get_relative_path(\"/root/room/models/qwen2.5-0.5b\") tok = AutoTokenizer.from_pretrained(QWEN_PATH, trust_remote_code=True) chat_model = AutoModelForCausalLM.from_pretrained( \"Qwen/Qwen2.5-0.5B\", cache_dir=\"/root/room/models\", torch_dtype=\"auto\", device_map=\"auto\" ) def qwen_chat(prompt: str) -\u003e str: msgs = [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": prompt} ] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) inputs = tok(text, return_tensors=\"pt\") with torch.no_grad(): out = chat_model.generate( **inputs, max_new_tokens=64, do_sample=True, temperature=0.7, pad_token_id=tok.eos_token_id ) raw_output = tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip() for token in [\"\", \"\", \".user\", \"🤨\", \"## 128000\", \"\\x0c\"]: raw_output = raw_output.replace(token, \"\") reply = raw_output.split(\"\\n\")[0].strip() return reply # ------------------ 声纹相关 ------------------ def load_speaker_embedding_model(model_path: str): config = sherpa_onnx.SpeakerEmbeddingExtractorConfig( model=model_path, num_threads=1, debug=False, provider=\"cpu\", ) if not config.validate(): raise ValueError(f\"Invalid config. {config}\") return sherpa_onnx.SpeakerEmbeddingExtractor(config) def load_speaker_file(speaker_file_path: str) -\u003e Dict[str, List[str]]: ans = defaultdict(list) with open(speaker_file_path) as f: for line in f: line = line.strip() if not line: continue fields = line.split() if len(fields) != 2: raise ValueError(f\"Invalid line: {line}\") speaker_name, filename = fields ans[speaker_name].append(filename) return ans def compute_speaker_embedding(filenames: List[str], extractor) -\u003e np.ndarray: ans = None for filename in filenames: samples, sr = sf.read(filename, always_2d=True, dtype=\"float32\") samples = samples[:, 0] samples = np.ascontiguousarray(samples) stream = extractor.create_stream() stream.accept_waveform(sr, samples) stream.input_finished() embedding = np.array(extractor.compute(stream)) if ans is None: ans = embedding else: ans += embedding return ans / len(filenames) def init_speaker_identification(): speaker_model = get_relative_path(\"/root/room/wespeaker_zh_cnceleb_resnet34.onnx\") speaker_file = get_relative_path(\"/root/room/speaker.txt\") extractor = load_speaker_embedding_model(speaker_model) speaker_data = load_speaker_file(speaker_file) manager = sherpa_onnx.SpeakerEmbeddingManager(extractor.dim) for name, filelist in speaker_data.items(): emb = compute_speaker_embedding(filelist, extractor) status = manager.add(name, emb) if not status: raise RuntimeError(f\"Failed to register speaker {name}\") return extractor, manager def wake_word_detection_with_speaker(input_device: int, input_sr: int, extractor, speaker_manager) -\u003e Optional[str]: kws_dir = get_relative_path(\"/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01\") keyword_spotter = sherpa_onnx.KeywordSpotter( tokens=os.path.join(kws_dir, \"tokens.txt\"), encoder=os.path.join(kws_dir, \"encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), decoder=os.path.join(kws_dir, \"decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), joiner=os.path.join(kws_dir, \"joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx\"), num_threads=1, max_active_paths=4, keywords_file=os.path.join(kws_dir, \"keywords.txt\"), keywords_score=1.0, keywords_threshold=0.25, num_trailing_blanks=1, provider=\"cpu\", ) print(\"等待唤醒词（需声纹匹配）...\") stream = keyword_spotter.create_stream() chunk = int(0.1 * input_sr) buffer_seconds = 2.0 # 缓存唤醒词语音用于声纹识别 max_buffer_len = int(buffer_seconds * input_sr) audio_buffer = np.zeros((0,), dtype=np.float32) with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) stream.accept_waveform(input_sr, samples) # 语音缓存 audio_buffer = np.concatenate((audio_buffer, samples)) if len(audio_buffer) \u003e max_buffer_len: audio_buffer = audio_buffer[-max_buffer_len:] while keyword_spotter.is_ready(stream): keyword_spotter.decode_stream(stream) result = keyword_spotter.get_result(stream) if result: print(f\"检测到唤醒词: {result}\") keyword_spotter.reset_stream(stream) # 进行声纹识别 print(\"正在进行声纹识别验证...\") audio = np.ascontiguousarray(audio_buffer) emb_stream = extractor.create_stream() emb_stream.accept_waveform(input_sr, audio) emb_stream.input_finished() emb = np.array(extractor.compute(emb_stream)) speaker = speaker_manager.search(emb, threshold=0.6) if speaker: print(f\"声纹识别成功，说话人: {speaker}\") return speaker else: print(\"声纹识别失败，忽略本次唤醒\") return None # ------------------ 唤醒词（不带声纹，仅示例备用） ------------------ def wake_word_detection(input_device: int, input_sr: int) -\u003e bool: kws_dir = get_relative_path(\"/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01\") keyword_spotter = sherpa_onnx.KeywordSpotter( tokens=os.path.join(kws_dir, \"tokens.txt\"), encoder=os.path.join(kws_dir, \"encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), decoder=os.path.join(kws_dir, \"decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx\"), joiner=os.path.join(kws_dir, \"joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx\"), num_threads=1, max_active_paths=4, keywords_file=os.path.join(kws_dir, \"keywords.txt\"), keywords_score=1.0, keywords_threshold=0.25, num_trailing_blanks=1, provider=\"cpu\", ) print(\"等待唤醒词...\") stream = keyword_spotter.create_stream() chunk = int(0.1 * input_sr) with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) stream.accept_waveform(input_sr, samples) while keyword_spotter.is_ready(stream): keyword_spotter.decode_stream(stream) result = keyword_spotter.get_result(stream) if result: print(f\"检测到唤醒词: {result}\") keyword_spotter.reset_stream(stream) return True # ------------------ 语音识别（保留原有函数可按需使用） ------------------ def speech_recognition(extractor, speaker_manager, input_device: int, input_sr: int) -\u003e Tuple[str, str]: target_sr = 16000 # VAD/ASR 采样率 recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer( paraformer=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx\"), tokens=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt\"), num_threads=1, sample_rate=target_sr, feature_dim=80, decoding_method=\"greedy_search\", debug=False, ) vad_config = sherpa_onnx.VadModelConfig() vad_config.silero_vad.model = get_relative_path(\"/root/room/silero_vad.onnx\") vad_config.silero_vad.min_silence_duration = 0.5 vad_config.silero_vad.min_speech_duration = 0.5 vad_config.sample_rate = target_sr vad = sherpa_onnx.VoiceActivityDetector(vad_config, buffer_size_in_seconds=100) window_size = vad_config.silero_vad.window_size print(\"请说出您的指令...\") chunk = int(0.1 * input_sr) audio_buffer = np.array([], dtype=\"float32\") with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: while True: samples, _ = s.read(chunk) samples = samples.reshape(-1) resampled = resampy.resample(samples, input_sr, target_sr) audio_buffer = np.concatenate([audio_buffer, resampled]) while len(audio_buffer) \u003e window_size: vad.accept_waveform(audio_buffer[:window_size]) audio_buffer = audio_buffer[window_size:] while not vad.empty(): if len(vad.front.samples) \u003c 0.5 * target_sr: vad.pop() continue stream = extractor.create_stream() stream.accept_waveform(target_sr, vad.front.samples) stream.input_finished() emb = np.array(extractor.compute(stream)) speaker = speaker_manager.search(emb, threshold=0.6) or \"unknown\" asr_stream = recognizer.create_stream() asr_stream.accept_waveform(target_sr, vad.front.samples) recognizer.decode_stream(asr_stream) text = asr_stream.result.text.strip() vad.pop() return text, speaker # ------------------ TTS ------------------ buffer = queue.Queue() started = False stopped = False killed = False event = threading.Event() tts_sr = 24000 output_sr = 48000 def generated_audio_callback(samples: np.ndarray, progress: float) -\u003e int: global started resampled = resampy.resample(samples, tts_sr, output_sr) buffer.put(resampled) if not started: started = True return 0 if killed else 1 def play_audio_callback(outdata: np.ndarray, frames: int, time, status): if killed or (started and buffer.empty() and stopped): event.set() if buffer.empty(): outdata.fill(0) return n = 0 while n \u003c frames and not buffer.empty(): remaining = frames - n k = buffer.queue[0].shape[0] if remaining \u003c= k: outdata[n:, 0] = buffer.queue[0][:remaining] buffer.queue[0] = buffer.queue[0][remaining:] n = frames if buffer.queue[0].shape[0] == 0: buffer.get() break outdata[n:n + k, 0] = buffer.get() n += k if n \u003c frames: outdata[n:, 0] = 0 # ------------------ SpeechBrain 分离：初始化与工具函数 ------------------ print(\"初始化 SpeechBrain 分离模型（Sepformer）... 这可能需要下载模型到 pretrained_models/sepformer-wsj02mix\") try: sep_model = separator.from_hparams( source=\"speechbrain/sepformer-wsj02mix\", savedir=\"pretrained_models/sepformer-wsj02mix\", run_opts={\"device\": \"cpu\"}, ) SEP_SR = getattr(sep_model.hparams, \"sample_rate\", 8000) # 通常 sepformer 用 8k print(f\"Sepformer 采样率: {SEP_SR}\") except Exception as e: print(\"加载 Sepformer 模型失败:\", e) sep_model = None SEP_SR = 8000 # 我们的 ASR / 声纹 统一用 16k（与 原逻辑一致） ASR_SR = 16000 MIN_AUDIO_SECONDS = 0.3 # 跳过太短片段（秒） MIN_AUDIO_SAMPLES = int(MIN_AUDIO_SECONDS * ASR_SR) def _extract_sources_from_sep_output(est_np: np.ndarray, max_sources=16) -\u003e List[np.ndarray]: \"\"\" 将 sep_model 输出 ndarray 转为 [n_src, time] 的 list（numpy 1D float32）。 尽量自动识别哪个轴是源、哪个轴是时间。 \"\"\" if est_np is None: return [] if est_np.ndim == 1: return [] # 先尝试找到较小的轴（\u003e1 且 \u003c= max_sources）作为 source 轴 src_axis = None time_axis = None for i, s in enumerate(est_np.shape): if 1 \u003c s \u003c= max_sources: src_axis = i if s \u003e 1000: time_axis = i if src_axis is None: # 退化策略：2D 情况下，较小维度当作 src if est_np.ndim == 2: if est_np.shape[0] \u003c est_np.shape[1]: src_axis, time_axis = 0, 1 else: src_axis, time_axis = 1, 0 elif est_np.ndim == 3: # 常见： (1, n_src, time) 或 (channels, time, n_src) # 优先选 size 小的轴作为 src sizes = list(est_np.shape) min_idx = int(np.argmin(sizes)) # 保证不是 time axis src_axis = min_idx # time axis 取最大轴 time_axis = int(np.argmax(sizes)) else: # 最保守策略 src_axis = 0 time_axis = est_np.ndim - 1 if time_axis is None: time_axis = max(range(est_np.ndim), key=lambda i: est_np.shape[i] if i != src_axis else -1) # 将 array 重新排列为 (src, time, ...) # 我们把 time 移到 axis=1，src 移到 axis=0 arr = np.moveaxis(est_np, (src_axis, time_axis), (0, 1)) # 现在 arr.shape[0] = n_src, arr.shape[1] = time n_src = arr.shape[0] sources = [] for i in range(n_src): s = arr[i] # 若还有额外轴（如 channels），把它们 flatten 或取第一通道 if s.ndim \u003e 1: # 把多通道求均值为单通道 s = np.mean(s, axis=tuple(range(1, s.ndim))) sources.append(s.astype(np.float32).reshape(-1)) return sources def separate_speakers(mixed_audio: np.ndarray, input_sr: int) -\u003e List[np.ndarray]: \"\"\" 返回 list，每项为以 SEP_SR 为采样率的 numpy 1D float32 信号（单通道）。 （后续处理会把它重采样为 ASR_SR） \"\"\" if sep_model is None: print(\"Sepformer 未初始化，跳过分离，直接返回原始音频\") return [mixed_audio] # 准备模型输入：sep_model 要求的形状可能是 [batch, time] 或 [batch, 1, time] 等 # 我们统一提供 [1, time] if input_sr != SEP_SR: # 用 resampy 将输入混音重采样到 sep 模型采样率 mixed_audio_sep = resampy.resample(mixed_audio, input_sr, SEP_SR).astype(np.float32) else: mixed_audio_sep = mixed_audio.astype(np.float32) audio_tensor = torch.from_numpy(mixed_audio_sep).float().unsqueeze(0) # [1, time] with torch.no_grad(): est = sep_model.separate_batch(audio_tensor) # 返回 tensor，shape 可能多样 est_np = est.cpu().numpy() sources = _extract_sources_from_sep_output(est_np, max_sources=16) # 过滤极短或静音的源（避免后续造成 extractor 报错） filtered = [] for i, s in enumerate(sources): if s is None or s.size == 0: continue power = float(np.mean(np.abs(s))) if s.shape[0] \u003c 10: # 非常短 continue # 过滤非常静的（阈值可根据实际调整） if power \u003c 1e-5: continue filtered.append(s) return filtered # ------------------ 主流程相关（设备选择 / 录音） ------------------ def select_devices(): print(\"=== 可用输入输出设备列表 ===\") print(sd.query_devices()) input_device = int(input(\"请输入输入设备ID: \")) output_device = int(input(\"请输入输出设备ID: \")) input_info = sd.query_devices(input_device) output_info = sd.query_devices(output_device) input_sr = int(input_info['default_samplerate']) print(f\"选择输入采样率: {input_sr} Hz\") return input_device, output_device, input_sr def record_mixed_audio(input_device: int, input_sr: int, duration: float = 5.0): \"\"\"从麦克风录制一段混合音频（单声道 float32）\"\"\" print(f\"开始录音 {duration} 秒...\") with sd.InputStream(device=input_device, channels=1, dtype=\"float32\", samplerate=input_sr) as s: frames = [] total_chunks = int(duration / 0.1) for _ in range(total_chunks): samples, _ = s.read(int(0.1 * input_sr)) frames.append(samples.reshape(-1)) mixed_audio = np.concatenate(frames).astype(np.float32) print(\"录音完成\") return mixed_audio output_stream = None def init_output_stream(output_device: int): global output_stream if output_stream is None: output_stream = sd.OutputStream( device=output_device, channels=1, callback=play_audio_callback, dtype=\"float32\", samplerate=output_sr, blocksize=1024 ) output_stream.start() def tts_synthesis(text: str, output_device: int): global tts_sr, started, stopped init_output_stream(output_device) # 复用全局输出流 tts_config = sherpa_onnx.OfflineTtsConfig( model=sherpa_onnx.OfflineTtsModelConfig( vits=sherpa_onnx.OfflineTtsVitsModelConfig( model=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/zh_CN-huayan-medium.onnx\"), tokens=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/tokens.txt\"), data_dir=get_relative_path(\"/root/room/vits-piper-zh_CN-huayan-medium/espeak-ng-data\"), ), provider=\"cpu\", debug=False, num_threads=1, ), max_num_sentences=1, ) tts = sherpa_onnx.OfflineTts(tts_config) tts_sr = tts.sample_rate event.clear() started = False stopped = False tts.generate(text, sid=0, speed=1, callback=generated_audio_callback) stopped = True event.wait() # ------------------ 主流程 ------------------ def main(): global killed try: print(\"初始化声纹识别...\") extractor, speaker_manager = init_speaker_identification() input_device, output_device, input_sr = select_devices() # 预创建 ASR recognizer（避免在每个说话人上重复创建） recognizer = sherpa_onnx.OfflineRecognizer.from_paraformer( paraformer=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx\"), tokens=get_relative_path(\"/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt\"), num_threads=1, sample_rate=ASR_SR, feature_dim=80, decoding_method=\"greedy_search\", debug=False, ) speaker = wake_word_detection_with_speaker(input_device, input_sr, extractor, speaker_manager) if speaker: # 1) 录制混合音频 mixed_audio = record_mixed_audio(input_device, input_sr, duration=5.0) # 2) 分离 separated = separate_speakers(mixed_audio, input_sr) print(f\"分离后有效说话人数量: {len(separated)}\") if len(separated) == 0: print(\"未检测到有效分离结果，回退为直接识别整段语音\") # 回退：把混合音频重采样到 ASR_SR 并识别 mixed_resampled = resampy.resample(mixed_audio, input_sr, ASR_SR).astype(np.float32) # 声纹与识别（与单路流程相同） if mixed_resampled.size \u003e= MIN_AUDIO_SAMPLES: try: emb_stream = extractor.create_stream() emb_stream.accept_waveform(ASR_SR, mixed_resampled) emb_stream.input_finished() emb = np.array(extractor.compute(emb_stream)) spk_name = speaker_manager.search(emb, threshold=0.6) or \"unknown\" except Exception as e: print(\"声纹识别失败:\", e) spk_name = \"unknown\" try: asr_stream = recognizer.create_stream() asr_stream.accept_waveform(ASR_SR, mixed_resampled) recognizer.decode_stream(asr_stream) text = asr_stream.result.text.strip() except Exception as e: print(\"ASR 失败:\", e) text = \"\" print(f\"识别结果: {text} (说话人: {spk_name})\") keyword, kid = fuzzy_match(text) if keyword: reply = f\"已为您操作【{keyword}】，编号{kid}\" else: reply = qwen_chat(text) print(\"回复:\", reply) tts_synthesis(reply, output_device) else: print(\"录音过短，无法识别。\") return # 3) 对每一路分别做 声纹 -\u003e ASR -\u003e reply -\u003e TTS for i, sp_audio_sep in enumerate(separated): print(f\"\\n=== 处理第 {i+1} 个说话人 ===\") # 先把 sep 模型的采样率转换到 ASR_SR（如果 sep 模型采样率不同） if SEP_SR != ASR_SR: try: sp_audio = resampy.resample(sp_audio_sep, SEP_SR, ASR_SR).astype(np.float32) except Exception as e: print(\"重采样出错，跳过该说话人:\", e) continue else: sp_audio = sp_audio_sep.astype(np.float32) # 过滤太短或静音 if sp_audio.size \u003c MIN_AUDIO_SAMPLES: print(f\"第 {i+1} 路音频过短（{sp_audio.size} samples），跳过\") continue if float(np.mean(np.abs(sp_audio))) \u003c 1e-5: print(f\"第 {i+1} 路音频过静，跳过\") continue # 声纹识别（保护性 try） try: emb_stream = extractor.create_stream() emb_stream.accept_waveform(ASR_SR, sp_audio) emb_stream.input_finished() emb = np.array(extractor.compute(emb_stream)) spk_name = speaker_manager.search(emb, threshold=0.6) or \"unknown\" except Exception as e: print(\"声纹识别出错（跳过声纹或标为 unknown）:\", e) spk_name = \"unknown\" # ASR try: asr_stream = recognizer.create_stream() asr_stream.accept_waveform(ASR_SR, sp_audio) recognizer.decode_stream(asr_stream) text = asr_stream.result.text.strip() except Exception as e: print(\"ASR 失败:\", e) text = \"\" print(f\"识别结果: {text} (说话人: {spk_name})\") # 生成回复并 TTS keyword, kid = fuzzy_match(text) if keyword: reply = f\"已为您操作【{keyword}】，编号{kid}\" else: reply = qwen_chat(text) if text.strip() else \"抱歉，我没有听清楚。\" print(\"回复:\", reply) tts_synthesis(reply, output_device) except KeyboardInterrupt: killed = True print(\"\\n程序已终止\") sys.exit(0) if __name__ == \"__main__\": main() 大模型过滤符号等 def qwen_chat(prompt: str) -\u003e str: try: msgs = [ {\"role\": \"system\", \"content\": f\"{SYSTEM_PROMPT}。请用纯中文回答，不要包含任何英文、代码、特殊符号，语句要自然通顺。\"}, {\"role\": \"user\", \"content\": prompt} ] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) inputs = tok(text, return_tensors=\"pt\") with torch.no_grad(): out = chat_model.generate( **inputs, max_new_tokens=64, do_sample=True, temperature=0.7, pad_token_id=tok.eos_token_id ) raw_output = tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip() # 加强过滤：移除所有非中文字符（保留标点） import re # 只保留中文、中文标点、数字 filtered_output = re.sub(r'[^\\u4e00-\\u9fa5，。？！,.:;？！]', '', raw_output) # 如果过滤后为空，返回默认回复 if not filtered_output.strip(): return \"抱歉，我没理解 意思，请再说一遍。\" return filtered_output except Exception as e: print(f\"大模型生成出错: {e}\") return \"抱歉，处理 请求时出现错误。\" 最小音频功率 1. 提高分离后音频的过滤阈值（过滤短片段和噪音） 修改音频过滤参数，只保留足够长、音量足够大的片段：\n# 修改全局参数（原参数值调大） MIN_AUDIO_SECONDS = 0.8 # 最小音频长度从0.2秒提高到0.8秒（过滤过短片段） MIN_AUDIO_SAMPLES = int(MIN_AUDIO_SECONDS * ASR_SR) MIN_AUDIO_POWER = 1e-4 # 新增：最小音频功率（过滤静音/噪音） 2. 在分离后过滤时应用更严格的条件 修改 separate_speakers 函数中的过滤逻辑：\ndef separate_speakers(mixed_audio: np.ndarray, input_sr: int) -\u003e List[np.ndarray]: if sep_model is None: print(\"Sepformer 未初始化，跳过分离，直接返回原始音频\") return [mixed_audio] # （原代码不变：采样率转换、模型分离） if input_sr != SEP_SR: mixed_audio_sep = resampy.resample(mixed_audio, input_sr, SEP_SR).astype(np.float32) else: mixed_audio_sep = mixed_audio.astype(np.float32) audio_tensor = torch.from_numpy(mixed_audio_sep).float().unsqueeze(0) with torch.no_grad(): est = sep_model.separate_batch(audio_tensor) est_np = est.cpu().numpy() sources = _extract_sources_from_sep_output(est_np, max_sources=16) # 更严格的过滤：只保留长音频、高音量的片段 filtered = [] for i, s in enumerate(sources): if s is None or s.size == 0: continue # 计算音频功率（音量） power = float(np.mean(np.square(s))) # 用平方均值更能反映音量 # 长度过滤（至少0.8秒）+ 音量过滤（功率足够大） if s.shape[0] \u003c MIN_AUDIO_SAMPLES * SEP_SR / ASR_SR: # 按SEP_SR换算长度 print(f\"第 {i+1} 路音频过短（{s.shape[0]/SEP_SR:.2f}秒），跳过\") continue if power \u003c MIN_AUDIO_POWER: print(f\"第 {i+1} 路音频音量过低（功率{power:.6f}），跳过\") continue filtered.append(s) # 限制最大分离数量（如果是单人场景，强制只保留1个最可能的说话人） if len(filtered) \u003e 1: # 按音量排序，保留最大音量的那个（最可能是有效说话人） filtered.sort(key=lambda x: np.mean(np.square(x)), reverse=True) filtered = filtered[:1] # 只保留1个 print(f\"检测到多个分离结果，保留最可能的1个说话人\") return filtered 关键修改：\n提高最小音频长度（0.8 秒），过滤因噪音产生的短片段；\n增加音量过滤（功率阈值），排除静音或低音量的无效片段；\n限制最大分离数量为 1（单人场景下），避免同一人语音被拆分。\n使用gtp-oos-20b 需 128g内存 下载模型\npip install huggingface-cli huggingface-cli download openai/gpt-oss-20b --include \"original/*\" --local-dir gpt-oss-20b/ 下载tokenizer 文件\nhuggingface-cli download openai/gpt-oss-20b --include \"tokenizer*\" --local-dir /root/room/gpt-oss-20b/original python version 3.13 使用 pyenv 管理版本 pyenv 可以方便地安装和切换多个 Python 版本：\n安装 pyenv\n# 克隆 pyenv 仓库 git clone https://github.com/pyenv/pyenv.git ~/.pyenv # 配置环境变量（根据shell类型选择，如bash/zsh） echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' \u003e\u003e ~/.bashrc echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc # 刷新配置 安装 Python 3.13 如果 3.13 已在 pyenv 支持列表中：\npyenv install 3.13.0 # 替换为实际版本号 # 全局启用 3.13 pyenv global 3.13.0 # 验证 python --version 为了确保 Python 3.13 编译时相关模块（ctypes、lzma、ssl、bz2 等）都能正常编译，推荐一次装齐依赖：\napt-get update apt-get install -y \\ build-essential \\ libffi-dev \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ libncurses5-dev \\ libncursesw5-dev \\ xz-utils \\ tk-dev \\ liblzma-dev 安装依赖后重新编译 Python 安装完依赖包后，重新用 pyenv 编译：\npyenv uninstall 3.13.0 pyenv install 3.13.0 # 然后激活环境： pyenv global 3.13.0 source myenv/bin/activate pip install sounddevice numpy resampy transformers torch fuzzywuzzy pypinyin soundfile sounddevice sherpa_onnx numpy scipy # 大模型gpt-oss-20b GPT_OSS_PATH = \"/root/room/gpt-oss-20b/original\" tok = AutoTokenizer.from_pretrained(GPT_OSS_PATH, trust_remote_code=True) chat_model = AutoModelForCausalLM.from_pretrained( GPT_OSS_PATH, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, device_map=\"auto\" if torch.cuda.is_available() else None ) def gptoss_chat(prompt: str) -\u003e str: msgs = [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": prompt} ] text = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True) inputs = tok(text, return_tensors=\"pt\") inputs = inputs.to(chat_model.device) with torch.no_grad(): out = chat_model.generate( **inputs, max_new_tokens=64, do_sample=True, temperature=0.7, pad_token_id=tok.eos_token_id ) raw = tok.decode(out[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip() for token in [\"\", \"\", \".user\", \"## 128000\", \"\\x0c\"]: raw = raw.replace(token, \"\") return raw.split(\"\\n\")[0].strip() # ... speaker = wake_word_detection_with_speaker(input_device, input_sr, extractor, speaker_manager) if speaker: text, detected_speaker = speech_recognition(extractor, speaker_manager, input_device, input_sr) print(f\"识别结果: {text} (说话人: {detected_speaker})\") keyword, kid = fuzzy_match(text) if keyword: reply = f\"已为您操作【{keyword}】，编号{kid}\" else: reply = gptoss_chat(text) print(\"回复:\", reply) tts_synthesis(reply, output_device) 升级到 transformers 的 最新开发版，因为 gpt-oss 是非常新的模型类型：\npip install --upgrade pip setuptools pip uninstall transformers -y pip install git+https://github.com/huggingface/transformers.git python asrtts.py 模型本地运行参考 Qwen Qwen2.5-0.5B：至少 4GB 内存，可使用 CPU 部署，若有 NVIDIA GeForce GT 1030 等显卡则更好。CPU 方面没有特别高要求，普通家用电脑 CPU 即可。 Qwen2.5-1.5B：8GB 内存起步，建议使用 Intel i5 或 AMD Ryzen 5 及以上 CPU，若有 NVIDIA GeForce GTX 1660 或同等性能显卡（显存≥6GB）可加速运算。 Qwen2.5-3B：建议 12GB 及以上内存，推荐使用 NVIDIA GeForce RTX 2060 及以上显卡，CPU 建议为 Intel i7 或 AMD Ryzen 7 及以上型号，以保证处理更复杂语义理解、多轮对话等任务时的效率。 Qwen2.5-7B：至少 16GB 内存，推荐使用 NVIDIA GeForce RTX 3060 及以上显卡。CPU 至少 8 核心的高性能处理器，如 Intel Core i7 或 AMD Ryzen 7 系列。 Qwen2.5-14B：32GB 内存起步，可使用 NVIDIA GeForce RTX 3060 及以上显卡。建议配备至少 16 核 CPU，如 AMD EPYC 或 Intel Xeon 系列处理器，以应对复杂长文本处理等任务。 Qwen2.5-32B：64GB 及以上内存，需搭配 NVIDIA GeForce RTX 40 系列及以上高端显卡。CPU 建议为 16 核以上的高性能处理器，如 AMD EPYC 7xxx 或 Intel Xeon Scalable 系列。 Qwen2.5-72B：128GB 及以上内存，推荐使用 NVIDIA GeForce RTX 40 系列及以上显卡，且多卡并行更佳。CPU 建议至少 32 核，如 AMD Ryzen Threadripper 或 Intel Core i9 系列，若能使用 64 核以上的 AMD EPYC 或 Intel Xeon 系列处理器则更好 DeepSeek-R1 DeepSeek-R1-1.5B：CPU 最低 4 核（推荐 Intel/AMD 多核处理器），内存 8GB+。 DeepSeek-R1-7B/8B：建议 CPU 为 8 核以上（推荐现代多核 CPU），内存 16GB+。 DeepSeek-R1-14B：建议配备 12 核以上 CPU，内存 32GB+。 DeepSeek-R1-32B：需要 16 核以上 CPU（如 AMD Ryzen 9 或 Intel i9），内存 64GB+。 DeepSeek-R1-70B：建议使用 32 核以上服务器级 CPU，内存 128GB+。 DeepSeek-R1-671B：需 64 核以上的服务器集群，内存 512GB+。 ","wordCount":"9176","inLanguage":"en","datePublished":"2025-07-17T21:36:01+08:00","dateModified":"2025-07-17T21:36:01+08:00","author":{"@type":"Person","name":"dwd"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qfsyso.github.io/posts/vosk-stt/"},"publisher":{"@type":"Organization","name":"MLOG","logo":{"@type":"ImageObject","url":"https://qfsyso.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://qfsyso.github.io/ accesskey=h title="MLOG (Alt + H)">MLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://qfsyso.github.io/posts/ title=Archive><span>Archive</span></a></li><li><a href=https://qfsyso.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://qfsyso.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Vosk STT</h1><div class=post-meta><span title='2025-07-17 21:36:01 +0800 +0800'>July 17, 2025</span>&nbsp;·&nbsp;44 min&nbsp;·&nbsp;dwd</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#vosk-stt aria-label="Vosk STT">Vosk STT</a><ul><li><a href=#%e4%b8%80%e5%87%86%e5%a4%87%e7%8e%af%e5%a2%83debian aria-label=一、准备环境（Debian）>一、准备环境（Debian）</a></li><li><a href=#%e4%ba%8c%e4%b8%8b%e8%bd%bd%e5%be%ae%e5%9e%8b%e4%b8%ad%e6%96%87%e6%a8%a1%e5%9e%8b-40-mb aria-label="二、下载微型中文模型（≈ 40 MB）">二、下载微型中文模型（≈ 40 MB）</a></li><li><a href=#%e4%b8%89%e4%bf%9d%e5%ad%98%e4%bb%a5%e4%b8%8b%e8%84%9a%e6%9c%ac%e4%b8%ba-listenpy aria-label="三、保存以下脚本为 listen.py">三、保存以下脚本为 listen.py</a></li><li><a href=#%e5%9b%9b%e8%bf%90%e8%a1%8c%e7%a4%ba%e4%be%8b aria-label=四、运行示例>四、运行示例</a></li></ul></li><li><a href=#%e6%89%93%e5%8c%85docker%e8%bf%90%e8%a1%8c aria-label=打包docker运行>打包docker运行</a><ul><li><a href=#listenpy aria-label=listen.py>listen.py</a></li><li><a href=#dockerfile aria-label=dockerfile>dockerfile</a></li><li><a href=#%e6%9e%84%e5%bb%ba%e4%bd%bf%e7%94%a8 aria-label=构建使用>构建使用</a></li></ul></li><li><a href=#%e4%bd%bf%e7%94%a8pypinyin%e8%bf%9b%e8%a1%8c%e7%ba%a0%e9%94%99 aria-label=使用pypinyin进行纠错>使用pypinyin进行纠错</a><ul><li><a href=#1-%e5%ae%89%e8%a3%85%e4%be%9d%e8%b5%96dockerfile-%e9%87%8c%e5%8a%a0%e4%b8%80%e8%a1%8c%e5%8d%b3%e5%8f%af aria-label="1. 安装依赖（Dockerfile 里加一行即可）">1. 安装依赖（Dockerfile 里加一行即可）</a></li><li><a href=#2-%e6%96%b0%e7%9a%84-listenpy%e7%9b%b4%e6%8e%a5%e6%9b%bf%e6%8d%a2 aria-label="2. 新的 listen.py（直接替换）">2. 新的 listen.py（直接替换）</a></li><li><a href=#3-%e6%9e%84%e5%bb%ba--%e8%bf%90%e8%a1%8c aria-label="3. 构建 & 运行">3. 构建 & 运行</a></li><li><a href=#4-%e6%95%88%e6%9e%9c%e8%af%b4%e6%98%8e aria-label="4. 效果说明">4. 效果说明</a></li><li><a href=#%e7%bb%93%e6%9e%9c aria-label=结果>结果</a></li></ul></li><li><a href=#web-api aria-label="web API">web API</a><ul><li><a href=#1-%e7%9b%ae%e5%bd%95%e7%bb%93%e6%9e%84 aria-label="1. 目录结构">1. 目录结构</a></li><li><a href=#2-apppy aria-label="2. app.py">2. app.py</a></li><li><a href=#3-requirementstxt aria-label="3. requirements.txt">3. requirements.txt</a></li><li><a href=#4-dockerfile aria-label="4. Dockerfile">4. Dockerfile</a></li><li><a href=#5-%e6%9e%84%e5%bb%ba--%e8%bf%90%e8%a1%8c aria-label="5. 构建 & 运行">5. 构建 & 运行</a></li><li><a href=#6-%e8%b0%83%e7%94%a8%e7%a4%ba%e4%be%8b aria-label="6. 调用示例">6. 调用示例</a></li></ul></li><li><a href=#debian-nodocker-webapi aria-label="debian nodocker webapi">debian nodocker webapi</a><ul><li><a href=#1-%e7%b3%bb%e7%bb%9f%e4%be%9d%e8%b5%96%e4%b8%80%e6%ac%a1%e6%80%a7 aria-label="1. 系统依赖（一次性）">1. 系统依赖（一次性）</a></li><li><a href=#2-%e4%b8%8b%e8%bd%bd%e5%b9%b6%e6%94%be%e7%bd%ae%e4%b8%ad%e6%96%87%e5%b0%8f%e6%a8%a1%e5%9e%8b aria-label="2. 下载并放置中文小模型">2. 下载并放置中文小模型</a></li><li><a href=#3-%e5%ae%89%e8%a3%85-python-%e4%be%9d%e8%b5%96 aria-label="3. 安装 Python 依赖">3. 安装 Python 依赖</a></li><li><a href=#4-%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81-apppy aria-label="4. 完整代码 app.py">4. 完整代码 app.py</a></li><li><a href=#5-%e5%90%af%e5%8a%a8%e6%9c%8d%e5%8a%a1 aria-label="5. 启动服务">5. 启动服务</a></li><li><a href=#6-%e8%b0%83%e7%94%a8%e7%a4%ba%e4%be%8b-1 aria-label="6. 调用示例">6. 调用示例</a></li><li><a href=#7-%e5%bc%80%e6%9c%ba%e8%87%aa%e5%90%af%e5%8f%af%e9%80%89 aria-label="7. 开机自启（可选）">7. 开机自启（可选）</a></li></ul></li><li><a href=#pep668 aria-label=PEP668>PEP668</a></li><li><a href=#whisper%e7%89%88%e6%9c%ac aria-label=Whisper版本>Whisper版本</a><ul><li><a href=#1-%e7%9b%ae%e5%bd%95%e7%bb%93%e6%9e%84-1 aria-label="1. 目录结构">1. 目录结构</a></li><li><a href=#2-requirementstxt aria-label="2. requirements.txt">2. requirements.txt</a></li><li><a href=#3-apppy%e5%ae%8c%e5%85%a8%e6%9b%bf%e6%8d%a2%e6%97%a7%e7%89%88%e6%9c%ac aria-label="3. app.py（完全替换旧版本）">3. app.py（完全替换旧版本）</a></li><li><a href=#4-dockerfile-1 aria-label="4. Dockerfile">4. Dockerfile</a></li><li><a href=#5-%e6%9e%84%e5%bb%ba--%e8%bf%90%e8%a1%8c-1 aria-label="5. 构建 & 运行">5. 构建 & 运行</a></li><li><a href=#6-%e8%b0%83%e7%94%a8%e7%a4%ba%e4%be%8b%e4%b8%8e%e4%b9%8b%e5%89%8d%e5%ae%8c%e5%85%a8%e4%b8%80%e8%87%b4 aria-label="6. 调用示例（与之前完全一致）">6. 调用示例（与之前完全一致）</a></li><li><a href=#7-%e6%a8%a1%e5%9e%8b%e5%a4%a7%e5%b0%8f%e8%af%b4%e6%98%8e aria-label="7. 模型大小说明">7. 模型大小说明</a></li></ul></li><li><a href=#vosk-qwen aria-label=vosk-qwen>vosk-qwen</a><ul><li><a href=#1-%e7%b3%bb%e7%bb%9f%e4%be%9d%e8%b5%96%e4%b8%80%e6%ac%a1%e6%80%a7-1 aria-label="1. 系统依赖（一次性）">1. 系统依赖（一次性）</a></li><li><a href=#2-%e8%99%9a%e6%8b%9f%e7%8e%af%e5%a2%83--%e4%be%9d%e8%b5%96 aria-label="2. 虚拟环境 & 依赖">2. 虚拟环境 & 依赖</a></li><li><a href=#3-%e4%b8%8b%e8%bd%bd%e6%a8%a1%e5%9e%8b-15-gb aria-label="3. 下载模型（≈ 1.5 GB）">3. 下载模型（≈ 1.5 GB）</a></li><li><a href=#4-%e5%8d%95%e6%96%87%e4%bb%b6-apppy aria-label="4. 单文件 app.py">4. 单文件 app.py</a></li><li><a href=#%e5%90%af%e5%8a%a8 aria-label=启动>启动</a></li><li><a href=#%e8%b0%83%e7%94%a8%e7%a4%ba%e4%be%8b aria-label=调用示例>调用示例</a></li><li><a href=#multipart-err aria-label="multipart err">multipart err</a></li></ul></li><li><a href=#qwen25-15b aria-label=Qwen2.5-1.5B>Qwen2.5-1.5B</a></li><li><a href=#%e9%87%87%e6%a0%b7%e7%8e%87-16khz%e4%bf%9d%e8%af%81vad-asr aria-label="采样率 16kHz，保证vad ASR">采样率 16kHz，保证vad ASR</a></li><li><a href=#qwen%e5%8e%bb%e9%99%a4%e6%97%a0%e5%85%b3%e7%9a%84%e7%ac%a6%e5%8f%b7 aria-label=qwen去除无关的符号>qwen去除无关的符号</a><ul><li><a href=#%e8%af%ad%e9%80%9f%e8%b0%83%e6%95%b4 aria-label=语速调整>语速调整</a></li></ul></li><li><a href=#%e5%88%86%e7%a6%bb%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6json-prompt aria-label="分离配置文件json prompt">分离配置文件json prompt</a></li><li><a href=#%e5%85%88%e5%8a%a0%e8%bd%bd%e5%a3%b0%e7%ba%b9%e5%86%8d%e9%80%89%e8%ae%be%e5%a4%87 aria-label=先加载声纹再选设备>先加载声纹再选设备</a></li><li><a href=#%e8%af%ad%e9%9f%b3%e8%af%86%e5%88%ab-%e5%88%86%e7%a6%bb aria-label="语音识别 分离">语音识别 分离</a></li><li><a href=#stt-%e5%88%86%e7%a6%bb-%e8%8b%b1%e6%96%87-web aria-label="stt 分离 英文 web">stt 分离 英文 web</a><ul><li><a href=#%e9%a1%b9%e7%9b%ae%e7%bb%93%e6%9e%84 aria-label=项目结构>项目结构</a></li><li><a href=#%e6%96%87%e4%bb%b6%e5%86%85%e5%ae%b9%e7%a4%ba%e4%be%8b aria-label=文件内容示例>文件内容示例</a></li><li><a href=#%e5%90%af%e5%8a%a8%e4%b8%8e%e8%b0%83%e8%af%95 aria-label=启动与调试>启动与调试</a></li><li><a href=#%e6%89%a9%e5%b1%95 aria-label=扩展>扩展</a></li></ul></li><li><a href=#%e4%b8%ad%e6%96%87%e8%af%86%e5%88%ab%e6%a8%a1%e5%9e%8b aria-label=中文识别模型>中文识别模型</a><ul><li><a href=#asr-wav2vec2-commonvoice-14-zh-cn aria-label=asr-wav2vec2-commonvoice-14-zh-CN>asr-wav2vec2-commonvoice-14-zh-CN</a></li><li><a href=#whisper aria-label=Whisper>Whisper</a></li></ul></li><li><a href=#speechbrain---tts aria-label="SpeechBrain  & TTS">SpeechBrain & TTS</a></li><li><a href=#tts%e7%ba%bf%e7%a8%8b%e4%bc%98%e5%8c%96 aria-label=TTS线程优化>TTS线程优化</a></li><li><a href=#%e5%a4%a7%e6%a8%a1%e5%9e%8b%e8%bf%87%e6%bb%a4%e7%ac%a6%e5%8f%b7%e7%ad%89 aria-label=大模型过滤符号等>大模型过滤符号等</a></li><li><a href=#%e6%9c%80%e5%b0%8f%e9%9f%b3%e9%a2%91%e5%8a%9f%e7%8e%87 aria-label=最小音频功率>最小音频功率</a><ul><li><a href=#1-%e6%8f%90%e9%ab%98%e5%88%86%e7%a6%bb%e5%90%8e%e9%9f%b3%e9%a2%91%e7%9a%84%e8%bf%87%e6%bb%a4%e9%98%88%e5%80%bc%e8%bf%87%e6%bb%a4%e7%9f%ad%e7%89%87%e6%ae%b5%e5%92%8c%e5%99%aa%e9%9f%b3 aria-label="1. 提高分离后音频的过滤阈值（过滤短片段和噪音）">1. 提高分离后音频的过滤阈值（过滤短片段和噪音）</a></li><li><a href=#2-%e5%9c%a8%e5%88%86%e7%a6%bb%e5%90%8e%e8%bf%87%e6%bb%a4%e6%97%b6%e5%ba%94%e7%94%a8%e6%9b%b4%e4%b8%a5%e6%a0%bc%e7%9a%84%e6%9d%a1%e4%bb%b6 aria-label="2. 在分离后过滤时应用更严格的条件">2. 在分离后过滤时应用更严格的条件</a></li></ul></li><li><a href=#%e4%bd%bf%e7%94%a8gtp-oos-20b---%e9%9c%80-128g%e5%86%85%e5%ad%98 aria-label="使用gtp-oos-20b   需 128g内存">使用gtp-oos-20b 需 128g内存</a><ul><li><a href=#python-version--313 aria-label="python version  3.13">python version 3.13</a></li><li><a href=#%e5%ae%89%e8%a3%85-python-313 aria-label="安装 Python 3.13">安装 Python 3.13</a></li></ul></li><li><a href=#%e6%a8%a1%e5%9e%8b%e6%9c%ac%e5%9c%b0%e8%bf%90%e8%a1%8c%e5%8f%82%e8%80%83 aria-label=模型本地运行参考>模型本地运行参考</a><ul><li><a href=#qwen aria-label=Qwen>Qwen</a></li><li><a href=#deepseek-r1 aria-label=DeepSeek-R1>DeepSeek-R1</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=vosk-stt>Vosk STT<a hidden class=anchor aria-hidden=true href=#vosk-stt>#</a></h1><h2 id=一准备环境debian>一、准备环境（Debian）<a hidden class=anchor aria-hidden=true href=#一准备环境debian>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo apt update
</span></span><span style=display:flex><span>sudo apt install -y python3 python3-pip ffmpeg
</span></span><span style=display:flex><span>pip3 install --user vosk
</span></span></code></pre></div><h2 id=二下载微型中文模型-40-mb>二、下载微型中文模型（≈ 40 MB）<a hidden class=anchor aria-hidden=true href=#二下载微型中文模型-40-mb>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip
</span></span><span style=display:flex><span>unzip vosk-model-small-cn-0.22.zip
</span></span></code></pre></div><h2 id=三保存以下脚本为-listenpy>三、保存以下脚本为 listen.py<a hidden class=anchor aria-hidden=true href=#三保存以下脚本为-listenpy>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sys<span style=color:#f92672>,</span> json<span style=color:#f92672>,</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> vosk <span style=color:#f92672>import</span> Model, KaldiRecognizer
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> wave
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MAP <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wav2text</span>(path):
</span></span><span style=display:flex><span>    wf <span style=color:#f92672>=</span> wave<span style=color:#f92672>.</span>open(path, <span style=color:#e6db74>&#34;rb&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> wf<span style=color:#f92672>.</span>getnchannels() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getsampwidth() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getframerate() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>16000</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;音频必须是 16 kHz 16-bit 单声道 WAV&#34;</span>)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> Model(<span style=color:#e6db74>&#34;vosk-model-small-cn-0.22&#34;</span>)
</span></span><span style=display:flex><span>    rec <span style=color:#f92672>=</span> KaldiRecognizer(model, wf<span style=color:#f92672>.</span>getframerate())
</span></span><span style=display:flex><span>    rec<span style=color:#f92672>.</span>SetWords(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> wf<span style=color:#f92672>.</span>readframes(<span style=color:#ae81ff>4000</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(data) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> rec<span style=color:#f92672>.</span>AcceptWaveform(data):
</span></span><span style=display:flex><span>            text <span style=color:#f92672>+=</span> json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>Result())[<span style=color:#e6db74>&#34;text&#34;</span>]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>+=</span> json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>FinalResult())[<span style=color:#e6db74>&#34;text&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34; &#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>find_index</span>(text):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> key, idx <span style=color:#f92672>in</span> MAP<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> key <span style=color:#f92672>in</span> text:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> idx
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(sys<span style=color:#f92672>.</span>argv) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;用法: python3 listen.py &lt;wav路径&gt;&#34;</span>)
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>exit(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    t <span style=color:#f92672>=</span> wav2text(sys<span style=color:#f92672>.</span>argv[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;识别文本:&#34;</span>, t)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;对应序号:&#34;</span>, find_index(t))
</span></span></code></pre></div><h2 id=四运行示例>四、运行示例<a hidden class=anchor aria-hidden=true href=#四运行示例>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 listen.py wsd.wav
</span></span></code></pre></div><p>如果音频内容是「我是登」，识别结果里包含「卧室灯」，脚本会输出</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>识别文本: 我是登卧室灯
</span></span><span style=display:flex><span>对应序号: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>若未命中任何关键词则输出 对应序号: 0。
</span></span></code></pre></div><h1 id=打包docker运行>打包docker运行<a hidden class=anchor aria-hidden=true href=#打包docker运行>#</a></h1><h2 id=listenpy>listen.py<a hidden class=anchor aria-hidden=true href=#listenpy>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sys<span style=color:#f92672>,</span> json<span style=color:#f92672>,</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> vosk <span style=color:#f92672>import</span> Model, KaldiRecognizer
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> wave
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MAP <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wav2text</span>(path):
</span></span><span style=display:flex><span>    wf <span style=color:#f92672>=</span> wave<span style=color:#f92672>.</span>open(path, <span style=color:#e6db74>&#34;rb&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> wf<span style=color:#f92672>.</span>getnchannels() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getsampwidth() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getframerate() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>16000</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;音频必须是 16 kHz 16-bit 单声道 WAV&#34;</span>)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> Model(<span style=color:#e6db74>&#34;/opt&#34;</span>)
</span></span><span style=display:flex><span>    rec <span style=color:#f92672>=</span> KaldiRecognizer(model, wf<span style=color:#f92672>.</span>getframerate())
</span></span><span style=display:flex><span>    rec<span style=color:#f92672>.</span>SetWords(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> wf<span style=color:#f92672>.</span>readframes(<span style=color:#ae81ff>4000</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(data) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> rec<span style=color:#f92672>.</span>AcceptWaveform(data):
</span></span><span style=display:flex><span>            text <span style=color:#f92672>+=</span> json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>Result())[<span style=color:#e6db74>&#34;text&#34;</span>]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>+=</span> json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>FinalResult())[<span style=color:#e6db74>&#34;text&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34; &#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>find_index</span>(text):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> key, idx <span style=color:#f92672>in</span> MAP<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> key <span style=color:#f92672>in</span> text:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> idx
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(sys<span style=color:#f92672>.</span>argv) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;用法: python3 listen.py &lt;wav路径&gt;&#34;</span>)
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>exit(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    t <span style=color:#f92672>=</span> wav2text(sys<span style=color:#f92672>.</span>argv[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;识别文本:&#34;</span>, t)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;对应序号:&#34;</span>, find_index(t))
</span></span></code></pre></div><h2 id=dockerfile>dockerfile<a hidden class=anchor aria-hidden=true href=#dockerfile>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># ---- 1. 基础镜像 ----</span>
</span></span><span style=display:flex><span>FROM python:3.11-slim-bookworm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ---- 2. 系统依赖 &amp; ffmpeg ----</span>
</span></span><span style=display:flex><span>RUN apt-get update <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    apt-get install -y --no-install-recommends <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        ffmpeg <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        wget <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        unzip <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm -rf /var/lib/apt/lists/*
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ---- 3. 安装 Vosk ----</span>
</span></span><span style=display:flex><span>RUN pip install --no-cache-dir vosk<span style=color:#f92672>==</span>0.3.45
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ---- 4. 下载并解压中文小模型 ----</span>
</span></span><span style=display:flex><span>WORKDIR /opt
</span></span><span style=display:flex><span>RUN wget -q https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    unzip -q vosk-model-small-cn-0.22.zip <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    mv vosk-model-small-cn-0.22/* ./ <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rmdir vosk-model-small-cn-0.22 <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm vosk-model-small-cn-0.22.zip
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ---- 5. 复制脚本 ----</span>
</span></span><span style=display:flex><span>COPY listen.py /opt/listen.py
</span></span><span style=display:flex><span>RUN chmod +x /opt/listen.py
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ---- 6. 入口脚本：自动转码 + 识别 ----</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 创建一个小脚本作为 ENTRYPOINT</span>
</span></span><span style=display:flex><span>RUN printf <span style=color:#e6db74>&#39;#!/bin/bash\n\
</span></span></span><span style=display:flex><span><span style=color:#e6db74>set -e\n\
</span></span></span><span style=display:flex><span><span style=color:#e6db74>IN=&#34;$1&#34;\n\
</span></span></span><span style=display:flex><span><span style=color:#e6db74>BASE=$(basename &#34;$IN&#34; .wav)\n\
</span></span></span><span style=display:flex><span><span style=color:#e6db74>OUT=&#34;${BASE}_16k.wav&#34;\n\
</span></span></span><span style=display:flex><span><span style=color:#e6db74>echo &#34;Converting $IN -&gt; $OUT ...&#34;\n\
</span></span></span><span style=display:flex><span><span style=color:#e6db74>ffmpeg -y -i &#34;$IN&#34; -ar 16000 -ac 1 -sample_fmt s16 &#34;$OUT&#34; &gt;/dev/null 2&gt;&amp;1\n\
</span></span></span><span style=display:flex><span><span style=color:#e6db74>echo &#34;Running Vosk on $OUT ...&#34;\n\
</span></span></span><span style=display:flex><span><span style=color:#e6db74>exec python3 /opt/listen.py &#34;$OUT&#34;\n&#39;</span> &gt; /opt/entry.sh <span style=color:#f92672>&amp;&amp;</span> chmod +x /opt/entry.sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>WORKDIR /workspace
</span></span><span style=display:flex><span>ENTRYPOINT <span style=color:#f92672>[</span><span style=color:#e6db74>&#34;/opt/entry.sh&#34;</span><span style=color:#f92672>]</span>
</span></span></code></pre></div><h2 id=构建使用>构建使用<a hidden class=anchor aria-hidden=true href=#构建使用>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>docker build -t stt-cn-mini .
</span></span><span style=display:flex><span>docker run --rm -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>:/workspace stt-cn-mini wsd.wav
</span></span></code></pre></div><h1 id=使用pypinyin进行纠错>使用pypinyin进行纠错<a hidden class=anchor aria-hidden=true href=#使用pypinyin进行纠错>#</a></h1><p>下面给出「整句拼音容错」的完整实现：
用 pypinyin 把整句话转拼音（带声调），
再与标准关键词的拼音做模糊匹配，
支持「登」→「灯」，「客厅」→「客厅灯」等任意同音/近音组合。
只改 listen.py，其余不变。</p><h2 id=1-安装依赖dockerfile-里加一行即可>1. 安装依赖（Dockerfile 里加一行即可）<a hidden class=anchor aria-hidden=true href=#1-安装依赖dockerfile-里加一行即可>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#66d9ef>RUN</span> pip install --no-cache-dir vosk<span style=color:#f92672>==</span>0.3.45 pypinyin fuzzywuzzy python-Levenshtein<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><h2 id=2-新的-listenpy直接替换>2. 新的 listen.py（直接替换）<a hidden class=anchor aria-hidden=true href=#2-新的-listenpy直接替换>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sys<span style=color:#f92672>,</span> json<span style=color:#f92672>,</span> os<span style=color:#f92672>,</span> re
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> vosk <span style=color:#f92672>import</span> Model, KaldiRecognizer
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> wave
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 关键词→序号</span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 预先把关键词转成拼音列表，避免每次重复计算</span>
</span></span><span style=display:flex><span>KW_PINYIN <span style=color:#f92672>=</span> {kw: <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)) <span style=color:#66d9ef>for</span> kw <span style=color:#f92672>in</span> KEYWORD2ID}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wav2text</span>(path):
</span></span><span style=display:flex><span>    wf <span style=color:#f92672>=</span> wave<span style=color:#f92672>.</span>open(path, <span style=color:#e6db74>&#34;rb&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> wf<span style=color:#f92672>.</span>getnchannels() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getsampwidth() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getframerate() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>16000</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;音频必须是 16 kHz 16-bit 单声道 WAV&#34;</span>)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> Model(<span style=color:#e6db74>&#34;/opt&#34;</span>)
</span></span><span style=display:flex><span>    rec <span style=color:#f92672>=</span> KaldiRecognizer(model, wf<span style=color:#f92672>.</span>getframerate())
</span></span><span style=display:flex><span>    rec<span style=color:#f92672>.</span>SetWords(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> wf<span style=color:#f92672>.</span>readframes(<span style=color:#ae81ff>4000</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(data) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> rec<span style=color:#f92672>.</span>AcceptWaveform(data):
</span></span><span style=display:flex><span>            text <span style=color:#f92672>+=</span> json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>Result())[<span style=color:#e6db74>&#34;text&#34;</span>]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>+=</span> json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>FinalResult())[<span style=color:#e6db74>&#34;text&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34; &#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str) <span style=color:#f92672>-&gt;</span> int:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;基于拼音的模糊匹配，返回最相似关键词的序号&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    text_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))  <span style=color:#75715e># 整句话转拼音</span>
</span></span><span style=display:flex><span>    best_kw, best_score <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kw_py <span style=color:#f92672>in</span> KW_PINYIN<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        score <span style=color:#f92672>=</span> fuzz<span style=color:#f92672>.</span>partial_ratio(text_py, kw_py)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> score <span style=color:#f92672>&gt;</span> best_score:
</span></span><span style=display:flex><span>            best_kw, best_score <span style=color:#f92672>=</span> kw, score
</span></span><span style=display:flex><span>    <span style=color:#75715e># 阈值可自己调，80 以上基本可接受</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> best_score <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>80</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> KEYWORD2ID[best_kw]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(sys<span style=color:#f92672>.</span>argv) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;用法: python3 listen.py &lt;wav路径&gt;&#34;</span>)
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>exit(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    t <span style=color:#f92672>=</span> wav2text(sys<span style=color:#f92672>.</span>argv[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>    idx <span style=color:#f92672>=</span> fuzzy_match(t)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;识别文本:&#34;</span>, t)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;对应序号:&#34;</span>, idx)
</span></span></code></pre></div><h2 id=3-构建--运行>3. 构建 & 运行<a hidden class=anchor aria-hidden=true href=#3-构建--运行>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker build -t stt-cn-mini .
</span></span><span style=display:flex><span>docker run --rm -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>:/workspace stt-cn-mini wsd.wav
</span></span></code></pre></div><p>示例结果</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>识别文本: 我是登
</span></span><span style=display:flex><span>对应序号: <span style=color:#ae81ff>4</span>          <span style=color:#75715e># 因为 &#34;woshideng&#34; 与 &#34;woshideng&#34; 完全匹配 &#34;卧室灯&#34;</span>
</span></span></code></pre></div><h2 id=4-效果说明>4. 效果说明<a hidden class=anchor aria-hidden=true href=#4-效果说明>#</a></h2><p>pypinyin.lazy_pinyin() → 把整句话转拼音（带数字声调）。
fuzz.partial_ratio() → 计算句子拼音与关键词拼音的相似度。</p><p>阈值 80 以上即可覆盖常见同音/近音错误，调低或调高视实际效果。</p><h2 id=结果>结果<a hidden class=anchor aria-hidden=true href=#结果>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --rm -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>:/workspace stt-cn-mini test_wsd.wav
</span></span><span style=display:flex><span>Converting test_wsd.wav -&gt; test_wsd_16k.wav ...
</span></span><span style=display:flex><span>Running Vosk on test_wsd_16k.wav ...
</span></span><span style=display:flex><span>识别文本: 小爱同学帮我打开我是灯
</span></span><span style=display:flex><span>对应序号: <span style=color:#ae81ff>4</span>
</span></span></code></pre></div><h1 id=web-api>web API<a hidden class=anchor aria-hidden=true href=#web-api>#</a></h1><p>轻量级 FastAPI 服务
容器化部署（Debian 基础镜像 + Vosk + 中文拼音容错）。
通过 HTTP POST /recognize 提交 JSON：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-JSON data-lang=JSON><span style=display:flex><span>{<span style=color:#f92672>&#34;url&#34;</span>: <span style=color:#e6db74>&#34;http://&lt;host&gt;/&lt;file&gt;.wav&#34;</span>}
</span></span></code></pre></div><p>返回同音容错后的结果：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-JSON data-lang=JSON><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;我是登&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;matched&#34;</span>: <span style=color:#e6db74>&#34;卧室灯&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=1-目录结构>1. 目录结构<a hidden class=anchor aria-hidden=true href=#1-目录结构>#</a></h2><p>stt-api/
├── Dockerfile
├── requirements.txt
└── app.py</p><h2 id=2-apppy>2. app.py<a hidden class=anchor aria-hidden=true href=#2-apppy>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fastapi <span style=color:#f92672>import</span> FastAPI, HTTPException
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> BaseModel, HttpUrl
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tempfile<span style=color:#f92672>,</span> os<span style=color:#f92672>,</span> requests<span style=color:#f92672>,</span> wave<span style=color:#f92672>,</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> vosk <span style=color:#f92672>import</span> Model, KaldiRecognizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>app <span style=color:#f92672>=</span> FastAPI(title<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;STT-CN-API&#34;</span>, version<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;1.0&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 关键词配置</span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>, <span style=color:#e6db74>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>, <span style=color:#e6db74>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>, <span style=color:#e6db74>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>}
</span></span><span style=display:flex><span>KW_PINYIN <span style=color:#f92672>=</span> {kw: <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)) <span style=color:#66d9ef>for</span> kw <span style=color:#f92672>in</span> KEYWORD2ID}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 全局加载一次模型</span>
</span></span><span style=display:flex><span>MODEL <span style=color:#f92672>=</span> Model(<span style=color:#e6db74>&#34;/opt/models&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wav2text</span>(path: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> wave<span style=color:#f92672>.</span>open(path, <span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> wf:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> wf<span style=color:#f92672>.</span>getnchannels() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getsampwidth() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getframerate() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>16000</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;音频必须是 16 kHz 16-bit 单声道 WAV&#34;</span>)
</span></span><span style=display:flex><span>        rec <span style=color:#f92672>=</span> KaldiRecognizer(MODEL, wf<span style=color:#f92672>.</span>getframerate())
</span></span><span style=display:flex><span>        rec<span style=color:#f92672>.</span>SetWords(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            data <span style=color:#f92672>=</span> wf<span style=color:#f92672>.</span>readframes(<span style=color:#ae81ff>4000</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(data) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> rec<span style=color:#f92672>.</span>AcceptWaveform(data):
</span></span><span style=display:flex><span>                result<span style=color:#f92672>.</span>append(json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>Result())[<span style=color:#e6db74>&#34;text&#34;</span>])
</span></span><span style=display:flex><span>        result<span style=color:#f92672>.</span>append(json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>FinalResult())[<span style=color:#e6db74>&#34;text&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(result)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str):
</span></span><span style=display:flex><span>    text_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))
</span></span><span style=display:flex><span>    best_kw, best_score <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kw_py <span style=color:#f92672>in</span> KW_PINYIN<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        score <span style=color:#f92672>=</span> fuzz<span style=color:#f92672>.</span>partial_ratio(text_py, kw_py)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> score <span style=color:#f92672>&gt;</span> best_score:
</span></span><span style=display:flex><span>            best_kw, best_score <span style=color:#f92672>=</span> kw, score
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> best_kw <span style=color:#66d9ef>if</span> best_score <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>70</span> <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RecognizeRequest</span>(BaseModel):
</span></span><span style=display:flex><span>    url: HttpUrl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@app.post</span>(<span style=color:#e6db74>&#34;/recognize&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>recognize</span>(req: RecognizeRequest):
</span></span><span style=display:flex><span>    wav_url <span style=color:#f92672>=</span> str(req<span style=color:#f92672>.</span>url)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 下载到临时文件</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> tempfile<span style=color:#f92672>.</span>NamedTemporaryFile(suffix<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;.wav&#34;</span>, delete<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>) <span style=color:#66d9ef>as</span> tmp:
</span></span><span style=display:flex><span>            tmp<span style=color:#f92672>.</span>write(requests<span style=color:#f92672>.</span>get(wav_url, timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)<span style=color:#f92672>.</span>content)
</span></span><span style=display:flex><span>            tmp_path <span style=color:#f92672>=</span> tmp<span style=color:#f92672>.</span>name
</span></span><span style=display:flex><span>        <span style=color:#75715e># 转码（Vosk 只认 16k/16bit/单声道）</span>
</span></span><span style=display:flex><span>        out_path <span style=color:#f92672>=</span> tmp_path <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;_16k.wav&#34;</span>
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>system(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;ffmpeg -y -i </span><span style=color:#e6db74>{</span>tmp_path<span style=color:#e6db74>}</span><span style=color:#e6db74> -ar 16000 -ac 1 -sample_fmt s16 </span><span style=color:#e6db74>{</span>out_path<span style=color:#e6db74>}</span><span style=color:#e6db74> &gt;/dev/null 2&gt;&amp;1&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># 识别</span>
</span></span><span style=display:flex><span>        text <span style=color:#f92672>=</span> wav2text(out_path)
</span></span><span style=display:flex><span>        matched <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>        resp <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;text&#34;</span>: text, <span style=color:#e6db74>&#34;matched&#34;</span>: matched, <span style=color:#e6db74>&#34;id&#34;</span>: KEYWORD2ID<span style=color:#f92672>.</span>get(matched, <span style=color:#ae81ff>0</span>)}
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> HTTPException(status_code<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, detail<span style=color:#f92672>=</span>str(e))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>finally</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> (tmp_path, out_path):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(p):
</span></span><span style=display:flex><span>                os<span style=color:#f92672>.</span>remove(p)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> resp
</span></span></code></pre></div><h2 id=3-requirementstxt>3. requirements.txt<a hidden class=anchor aria-hidden=true href=#3-requirementstxt>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>fastapi<span style=color:#f92672>==</span>0.110.2
</span></span><span style=display:flex><span>uvicorn<span style=color:#f92672>[</span>standard<span style=color:#f92672>]==</span>0.29.0
</span></span><span style=display:flex><span>vosk<span style=color:#f92672>==</span>0.3.45
</span></span><span style=display:flex><span>pypinyin<span style=color:#f92672>==</span>0.51.0
</span></span><span style=display:flex><span>fuzzywuzzy<span style=color:#f92672>==</span>0.18.0
</span></span><span style=display:flex><span>python-Levenshtein<span style=color:#f92672>==</span>0.25.1
</span></span><span style=display:flex><span>requests<span style=color:#f92672>==</span>2.32.3
</span></span></code></pre></div><h2 id=4-dockerfile>4. Dockerfile<a hidden class=anchor aria-hidden=true href=#4-dockerfile>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> python:3.11-slim-bookworm</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    apt-get install -y --no-install-recommends <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        ffmpeg wget unzip <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm -rf /var/lib/apt/lists/*<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /opt</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 下载并解压模型到 /opt/models</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> wget -q https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    unzip -q vosk-model-small-cn-0.22.zip <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    mv vosk-model-small-cn-0.22 models <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm vosk-model-small-cn-0.22.zip<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /app</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> requirements.txt .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> pip install --no-cache-dir -r requirements.txt<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> app.py .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>EXPOSE</span><span style=color:#e6db74> 8000</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>CMD</span> [<span style=color:#e6db74>&#34;uvicorn&#34;</span>, <span style=color:#e6db74>&#34;app:app&#34;</span>, <span style=color:#e6db74>&#34;--host&#34;</span>, <span style=color:#e6db74>&#34;0.0.0.0&#34;</span>, <span style=color:#e6db74>&#34;--port&#34;</span>, <span style=color:#e6db74>&#34;8000&#34;</span>]<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><h2 id=5-构建--运行>5. 构建 & 运行<a hidden class=anchor aria-hidden=true href=#5-构建--运行>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker build -t stt-api .
</span></span><span style=display:flex><span>docker run -d --name stt -p 8000:8000 stt-api
</span></span></code></pre></div><h2 id=6-调用示例>6. 调用示例<a hidden class=anchor aria-hidden=true href=#6-调用示例>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>curl -X POST http://localhost:8000/recognize <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -d <span style=color:#e6db74>&#39;{&#34;url&#34;:&#34;http://192.168.80.47:16552/wsd.wav&#34;}&#39;</span>
</span></span></code></pre></div><p>返回：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-JSON data-lang=JSON><span style=display:flex><span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;我是登&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;matched&#34;</span>: <span style=color:#e6db74>&#34;卧室灯&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>可使用 tts 进行生成语音测试
ttsfm
<a href=https://github.com/dbccccccc/ttsfm>https://github.com/dbccccccc/ttsfm</a>
ChatTTS-ui
<a href=https://github.com/tianhm/ChatTTS-ui>https://github.com/tianhm/ChatTTS-ui</a></p><h1 id=debian-nodocker-webapi>debian nodocker webapi<a hidden class=anchor aria-hidden=true href=#debian-nodocker-webapi>#</a></h1><p>直接裸机部署、无需 Docker” 的 Vosk + 中文拼音容错 WebAPI 完整代码。
已在 Ubuntu / Debian ≥20.04 + Python 3.8+ 验证通过，仅需 5 步即可上线。</p><h2 id=1-系统依赖一次性>1. 系统依赖（一次性）<a hidden class=anchor aria-hidden=true href=#1-系统依赖一次性>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo apt update
</span></span><span style=display:flex><span>sudo apt install -y python3 python3-pip ffmpeg wget unzip
</span></span></code></pre></div><h2 id=2-下载并放置中文小模型>2. 下载并放置中文小模型<a hidden class=anchor aria-hidden=true href=#2-下载并放置中文小模型>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 在任意目录，例如 /opt/vosk-model</span>
</span></span><span style=display:flex><span>sudo mkdir -p /opt/vosk-model
</span></span><span style=display:flex><span>cd /opt/vosk-model
</span></span><span style=display:flex><span>sudo wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip
</span></span><span style=display:flex><span>sudo unzip vosk-model-small-cn-0.22.zip
</span></span><span style=display:flex><span>sudo mv vosk-model-small-cn-0.22/* ./ <span style=color:#f92672>&amp;&amp;</span> sudo rmdir vosk-model-small-cn-0.22
</span></span></code></pre></div><h2 id=3-安装-python-依赖>3. 安装 Python 依赖<a hidden class=anchor aria-hidden=true href=#3-安装-python-依赖>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip3 install --user <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  fastapi uvicorn vosk pypinyin fuzzywuzzy python-Levenshtein requests
</span></span></code></pre></div><h2 id=4-完整代码-apppy>4. 完整代码 app.py<a hidden class=anchor aria-hidden=true href=#4-完整代码-apppy>#</a></h2><p>保存为 ~/stt_api/app.py（路径随意）。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os<span style=color:#f92672>,</span> tempfile<span style=color:#f92672>,</span> wave<span style=color:#f92672>,</span> json<span style=color:#f92672>,</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fastapi <span style=color:#f92672>import</span> FastAPI, HTTPException
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> BaseModel, HttpUrl
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> vosk <span style=color:#f92672>import</span> Model, KaldiRecognizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------- 全局配置 -------------</span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>, <span style=color:#e6db74>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>, <span style=color:#e6db74>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>, <span style=color:#e6db74>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>}
</span></span><span style=display:flex><span>KW_PINYIN <span style=color:#f92672>=</span> {kw: <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)) <span style=color:#66d9ef>for</span> kw <span style=color:#f92672>in</span> KEYWORD2ID}
</span></span><span style=display:flex><span>MODEL_DIR <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/opt/vosk-model&#34;</span>                     <span style=color:#75715e># 与步骤2一致</span>
</span></span><span style=display:flex><span>MODEL <span style=color:#f92672>=</span> Model(MODEL_DIR)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>app <span style=color:#f92672>=</span> FastAPI(title<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Vosk-CN-API&#34;</span>, version<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;1.0&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------- 工具函数 -------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wav2text</span>(path: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> wave<span style=color:#f92672>.</span>open(path, <span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> wf:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> wf<span style=color:#f92672>.</span>getnchannels() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getsampwidth() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getframerate() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>16000</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;音频需 16kHz/16-bit/单声道&#34;</span>)
</span></span><span style=display:flex><span>        rec <span style=color:#f92672>=</span> KaldiRecognizer(MODEL, wf<span style=color:#f92672>.</span>getframerate())
</span></span><span style=display:flex><span>        rec<span style=color:#f92672>.</span>SetWords(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            data <span style=color:#f92672>=</span> wf<span style=color:#f92672>.</span>readframes(<span style=color:#ae81ff>4000</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(data) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> rec<span style=color:#f92672>.</span>AcceptWaveform(data):
</span></span><span style=display:flex><span>                result<span style=color:#f92672>.</span>append(json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>Result())[<span style=color:#e6db74>&#34;text&#34;</span>])
</span></span><span style=display:flex><span>        result<span style=color:#f92672>.</span>append(json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>FinalResult())[<span style=color:#e6db74>&#34;text&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(result)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str):
</span></span><span style=display:flex><span>    txt_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))
</span></span><span style=display:flex><span>    best_kw, best_score <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kw_py <span style=color:#f92672>in</span> KW_PINYIN<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        score <span style=color:#f92672>=</span> fuzz<span style=color:#f92672>.</span>partial_ratio(txt_py, kw_py)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> score <span style=color:#f92672>&gt;</span> best_score:
</span></span><span style=display:flex><span>            best_kw, best_score <span style=color:#f92672>=</span> kw, score
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> best_kw <span style=color:#66d9ef>if</span> best_score <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>70</span> <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------- 接口 -------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RecognizeRequest</span>(BaseModel):
</span></span><span style=display:flex><span>    url: HttpUrl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@app.post</span>(<span style=color:#e6db74>&#34;/recognize&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>recognize</span>(req: RecognizeRequest):
</span></span><span style=display:flex><span>    wav_url <span style=color:#f92672>=</span> str(req<span style=color:#f92672>.</span>url)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> tempfile<span style=color:#f92672>.</span>NamedTemporaryFile(suffix<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;.wav&#34;</span>, delete<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>) <span style=color:#66d9ef>as</span> tmp:
</span></span><span style=display:flex><span>            tmp<span style=color:#f92672>.</span>write(requests<span style=color:#f92672>.</span>get(wav_url, timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)<span style=color:#f92672>.</span>content)
</span></span><span style=display:flex><span>            tmp_path <span style=color:#f92672>=</span> tmp<span style=color:#f92672>.</span>name
</span></span><span style=display:flex><span>        <span style=color:#75715e># 转码</span>
</span></span><span style=display:flex><span>        out_path <span style=color:#f92672>=</span> tmp_path <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;_16k.wav&#34;</span>
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>system(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;ffmpeg -y -i </span><span style=color:#e6db74>{</span>tmp_path<span style=color:#e6db74>}</span><span style=color:#e6db74> -ar 16000 -ac 1 -sample_fmt s16 </span><span style=color:#e6db74>{</span>out_path<span style=color:#e6db74>}</span><span style=color:#e6db74> &gt;/dev/null 2&gt;&amp;1&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        text <span style=color:#f92672>=</span> wav2text(out_path)
</span></span><span style=display:flex><span>        matched <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;text&#34;</span>: text, <span style=color:#e6db74>&#34;matched&#34;</span>: matched, <span style=color:#e6db74>&#34;id&#34;</span>: KEYWORD2ID<span style=color:#f92672>.</span>get(matched, <span style=color:#ae81ff>0</span>)}
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> HTTPException(status_code<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, detail<span style=color:#f92672>=</span>str(e))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>finally</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> (tmp_path, out_path):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(p):
</span></span><span style=display:flex><span>                os<span style=color:#f92672>.</span>remove(p)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------- 本地调试 -------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> uvicorn
</span></span><span style=display:flex><span>    uvicorn<span style=color:#f92672>.</span>run(app, host<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;0.0.0.0&#34;</span>, port<span style=color:#f92672>=</span><span style=color:#ae81ff>8000</span>)
</span></span></code></pre></div><h2 id=5-启动服务>5. 启动服务<a hidden class=anchor aria-hidden=true href=#5-启动服务>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p ~/stt_api <span style=color:#f92672>&amp;&amp;</span> cd ~/stt_api
</span></span><span style=display:flex><span>nano app.py          <span style=color:#75715e># 粘贴上面代码</span>
</span></span><span style=display:flex><span>python3 app.py       <span style=color:#75715e># 前台运行，或后台用 nohup / systemd</span>
</span></span></code></pre></div><h2 id=6-调用示例-1>6. 调用示例<a hidden class=anchor aria-hidden=true href=#6-调用示例-1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST http://&lt;服务器IP&gt;:8000/recognize <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -d <span style=color:#e6db74>&#39;{&#34;url&#34;:&#34;http://192.168.80.47:16552/wsd.wav&#34;}&#39;</span>
</span></span></code></pre></div><p>返回：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-JSON data-lang=JSON><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;我是登&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;matched&#34;</span>: <span style=color:#e6db74>&#34;卧室灯&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=7-开机自启可选>7. 开机自启（可选）<a hidden class=anchor aria-hidden=true href=#7-开机自启可选>#</a></h2><p>创建 systemd 文件 /etc/systemd/system/stt.service：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Unit]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Description</span><span style=color:#f92672>=</span><span style=color:#e6db74>Vosk-CN-API</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>After</span><span style=color:#f92672>=</span><span style=color:#e6db74>network.target</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Service]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Type</span><span style=color:#f92672>=</span><span style=color:#e6db74>exec</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ExecStart</span><span style=color:#f92672>=</span><span style=color:#e6db74>/usr/bin/python3 /home/ubuntu/stt_api/app.py</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Restart</span><span style=color:#f92672>=</span><span style=color:#e6db74>always</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>User</span><span style=color:#f92672>=</span><span style=color:#e6db74>ubuntu</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>WorkingDirectory</span><span style=color:#f92672>=</span><span style=color:#e6db74>/home/ubuntu/stt_api</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Install]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>WantedBy</span><span style=color:#f92672>=</span><span style=color:#e6db74>multi-user.target</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo systemctl daemon-reload
</span></span><span style=display:flex><span>sudo systemctl enable --now stt
</span></span></code></pre></div><p>至此，无需 Docker 的 Vosk WebAPI 已部署完成。</p><h1 id=pep668>PEP668<a hidden class=anchor aria-hidden=true href=#pep668>#</a></h1><p>Debian 12 / Ubuntu 23.04+ 启用了 PEP668，禁止用 pip3 &ndash;user 或 sudo pip 往系统 Python 里装包。
虚拟环境</p><p>安装系统包（只需一次）</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo apt update
</span></span><span style=display:flex><span>sudo apt install -y python3-venv python3-pip ffmpeg wget unzip
</span></span></code></pre></div><p>创建并激活虚拟环境</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 -m venv ~/stt_venv
</span></span><span style=display:flex><span>source ~/stt_venv/bin/activate   <span style=color:#75715e># 以后每次运行前都先激活</span>
</span></span></code></pre></div><p>在虚拟环境里安装依赖</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>pip install --upgrade pip
</span></span><span style=display:flex><span>pip install fastapi uvicorn vosk pypinyin fuzzywuzzy python-Levenshtein requests
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>cd ~/stt_api
</span></span><span style=display:flex><span>~/stt_venv/bin/python app.py          <span style=color:#75715e># 或 uvicorn app:app --host 0.0.0.0 --port 8000</span>
</span></span></code></pre></div><p>测试</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST http://localhost:8000/recognize <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -d <span style=color:#e6db74>&#39;{&#34;url&#34;:&#34;http://192.168.80.47:16552/wsd.wav&#34;}&#39;</span>
</span></span></code></pre></div><h1 id=whisper版本>Whisper版本<a hidden class=anchor aria-hidden=true href=#whisper版本>#</a></h1><p>Whisper 是 OpenAI 出的开源语音识别模型（支持多语言，含中文）</p><p>whisper.cpp 是 Whisper 的轻量 C/C++ 版本，专门针对 CPU 做了优化</p><p>有 Python binding，也可命令行直接跑</p><p>体积小，tiny/base 模型几十 MB</p><p><strong>优点</strong></p><p>开源，完全离线，专门针对 CPU 做了优化</p><p><strong>缺点</strong></p><p>对比 Vosk 稍微吃一点 CPU（但 tiny 模型负载很低）</p><p>不支持流式识别（一次识别一段音频）</p><p><strong>whisper-api</strong></p><p>使用 轻量版 Whisper（openai-whisper CPU 推理即可，无需 GPU）。
其余逻辑不变：接收 URL → 下载 → 转码 → 中文同音纠错 → 返回 JSON。</p><h2 id=1-目录结构-1>1. 目录结构<a hidden class=anchor aria-hidden=true href=#1-目录结构-1>#</a></h2><p>whisper-api/
├── Dockerfile
├── requirements.txt
└── app.py</p><h2 id=2-requirementstxt>2. requirements.txt<a hidden class=anchor aria-hidden=true href=#2-requirementstxt>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>fastapi<span style=color:#f92672>==</span>0.110.2
</span></span><span style=display:flex><span>uvicorn<span style=color:#f92672>[</span>standard<span style=color:#f92672>]==</span>0.29.0
</span></span><span style=display:flex><span>openai-whisper<span style=color:#f92672>==</span><span style=color:#ae81ff>20231117</span>
</span></span><span style=display:flex><span>pypinyin<span style=color:#f92672>==</span>0.51.0
</span></span><span style=display:flex><span>fuzzywuzzy<span style=color:#f92672>==</span>0.18.0
</span></span><span style=display:flex><span>python-Levenshtein<span style=color:#f92672>==</span>0.25.1
</span></span><span style=display:flex><span>requests<span style=color:#f92672>==</span>2.32.3
</span></span></code></pre></div><h2 id=3-apppy完全替换旧版本>3. app.py（完全替换旧版本）<a hidden class=anchor aria-hidden=true href=#3-apppy完全替换旧版本>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Python data-lang=Python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fastapi <span style=color:#f92672>import</span> FastAPI, HTTPException
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> BaseModel, HttpUrl
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tempfile<span style=color:#f92672>,</span> os<span style=color:#f92672>,</span> requests
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> whisper
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>app <span style=color:#f92672>=</span> FastAPI(title<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Whisper-CN-API&#34;</span>, version<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;1.1&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 关键词配置</span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>, <span style=color:#e6db74>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>, <span style=color:#e6db74>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>, <span style=color:#e6db74>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>}
</span></span><span style=display:flex><span>KW_PINYIN <span style=color:#f92672>=</span> {kw: <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)) <span style=color:#66d9ef>for</span> kw <span style=color:#f92672>in</span> KEYWORD2ID}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 全局加载一次模型（base 体积小，中文够用）</span>
</span></span><span style=display:flex><span>MODEL <span style=color:#f92672>=</span> whisper<span style=color:#f92672>.</span>load_model(<span style=color:#e6db74>&#34;base&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RecognizeRequest</span>(BaseModel):
</span></span><span style=display:flex><span>    url: HttpUrl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str):
</span></span><span style=display:flex><span>    text_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))
</span></span><span style=display:flex><span>    best_kw, best_score <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kw_py <span style=color:#f92672>in</span> KW_PINYIN<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        score <span style=color:#f92672>=</span> fuzz<span style=color:#f92672>.</span>partial_ratio(text_py, kw_py)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> score <span style=color:#f92672>&gt;</span> best_score:
</span></span><span style=display:flex><span>            best_kw, best_score <span style=color:#f92672>=</span> kw, score
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> best_kw <span style=color:#66d9ef>if</span> best_score <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>70</span> <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@app.post</span>(<span style=color:#e6db74>&#34;/recognize&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>recognize</span>(req: RecognizeRequest):
</span></span><span style=display:flex><span>    wav_url <span style=color:#f92672>=</span> str(req<span style=color:#f92672>.</span>url)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 下载到临时文件</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> tempfile<span style=color:#f92672>.</span>NamedTemporaryFile(suffix<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;.wav&#34;</span>, delete<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>) <span style=color:#66d9ef>as</span> tmp:
</span></span><span style=display:flex><span>            tmp<span style=color:#f92672>.</span>write(requests<span style=color:#f92672>.</span>get(wav_url, timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)<span style=color:#f92672>.</span>content)
</span></span><span style=display:flex><span>            tmp_path <span style=color:#f92672>=</span> tmp<span style=color:#f92672>.</span>name
</span></span><span style=display:flex><span>        <span style=color:#75715e># 转码 16k/16bit/单声道（Whisper 会自动重采样，但统一格式更稳）</span>
</span></span><span style=display:flex><span>        out_path <span style=color:#f92672>=</span> tmp_path <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;_16k.wav&#34;</span>
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>system(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;ffmpeg -y -i </span><span style=color:#e6db74>{</span>tmp_path<span style=color:#e6db74>}</span><span style=color:#e6db74> -ar 16000 -ac 1 -sample_fmt s16 </span><span style=color:#e6db74>{</span>out_path<span style=color:#e6db74>}</span><span style=color:#e6db74> &gt;/dev/null 2&gt;&amp;1&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        <span style=color:#75715e># Whisper 识别</span>
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> MODEL<span style=color:#f92672>.</span>transcribe(out_path, language<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;zh&#34;</span>, fp16<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        text <span style=color:#f92672>=</span> result[<span style=color:#e6db74>&#34;text&#34;</span>]<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>        matched <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>        resp <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;text&#34;</span>: text, <span style=color:#e6db74>&#34;matched&#34;</span>: matched, <span style=color:#e6db74>&#34;id&#34;</span>: KEYWORD2ID<span style=color:#f92672>.</span>get(matched, <span style=color:#ae81ff>0</span>)}
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> HTTPException(status_code<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, detail<span style=color:#f92672>=</span>str(e))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>finally</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> (tmp_path, out_path):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(p):
</span></span><span style=display:flex><span>                os<span style=color:#f92672>.</span>remove(p)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> resp
</span></span></code></pre></div><h2 id=4-dockerfile-1>4. Dockerfile<a hidden class=anchor aria-hidden=true href=#4-dockerfile-1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> python:3.11-slim-bookworm</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> apt-get update <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    apt-get install -y --no-install-recommends <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        ffmpeg wget <span style=color:#f92672>&amp;&amp;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    rm -rf /var/lib/apt/lists/*<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /app</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> requirements.txt .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> pip install --no-cache-dir -r requirements.txt<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> app.py .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>EXPOSE</span><span style=color:#e6db74> 8000</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>CMD</span> [<span style=color:#e6db74>&#34;uvicorn&#34;</span>, <span style=color:#e6db74>&#34;app:app&#34;</span>, <span style=color:#e6db74>&#34;--host&#34;</span>, <span style=color:#e6db74>&#34;0.0.0.0&#34;</span>, <span style=color:#e6db74>&#34;--port&#34;</span>, <span style=color:#e6db74>&#34;8000&#34;</span>]<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><h2 id=5-构建--运行-1>5. 构建 & 运行<a hidden class=anchor aria-hidden=true href=#5-构建--运行-1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker build -t whisper-api .
</span></span><span style=display:flex><span>docker run -d --name whisper -p 8000:8000 whisper-api
</span></span></code></pre></div><h2 id=6-调用示例与之前完全一致>6. 调用示例（与之前完全一致）<a hidden class=anchor aria-hidden=true href=#6-调用示例与之前完全一致>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST http://localhost:8000/recognize <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -d <span style=color:#e6db74>&#39;{&#34;url&#34;:&#34;http://192.168.80.47:16552/wsd.wav&#34;}&#39;</span>
</span></span></code></pre></div><p>返回示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-JSON data-lang=JSON><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;我是登&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;matched&#34;</span>: <span style=color:#e6db74>&#34;卧室灯&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=7-模型大小说明>7. 模型大小说明<a hidden class=anchor aria-hidden=true href=#7-模型大小说明>#</a></h2><p>模型 体积 CPU 推理速度
tiny 39 MB 非常快
base 74 MB 推荐（已用）
small 244 MB 更准但更慢</p><p>如需更准，把 load_model(&ldquo;base&rdquo;) 换成 &ldquo;small&rdquo; 即可，无需改其他代码。</p><h1 id=vosk-qwen>vosk-qwen<a hidden class=anchor aria-hidden=true href=#vosk-qwen>#</a></h1><h2 id=1-系统依赖一次性-1>1. 系统依赖（一次性）<a hidden class=anchor aria-hidden=true href=#1-系统依赖一次性-1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo apt update
</span></span><span style=display:flex><span>sudo apt install -y python3-venv ffmpeg wget unzip git git-lfs
</span></span></code></pre></div><h2 id=2-虚拟环境--依赖>2. 虚拟环境 & 依赖<a hidden class=anchor aria-hidden=true href=#2-虚拟环境--依赖>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 -m venv ~/qwen_stt
</span></span><span style=display:flex><span>source ~/qwen_stt/bin/activate
</span></span><span style=display:flex><span>pip install --upgrade pip
</span></span><span style=display:flex><span>pip install vosk transformers torch accelerate <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>            pypinyin fuzzywuzzy python-Levenshtein <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>            fastapi uvicorn aiohttp aiofiles requests
</span></span></code></pre></div><h2 id=3-下载模型-15-gb>3. 下载模型（≈ 1.5 GB）<a hidden class=anchor aria-hidden=true href=#3-下载模型-15-gb>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p ~/qwen_stt/models <span style=color:#f92672>&amp;&amp;</span> cd ~/qwen_stt/models
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 1. Vosk 中文小模型</span>
</span></span><span style=display:flex><span>wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip
</span></span><span style=display:flex><span>unzip vosk-model-small-cn-0.22.zip <span style=color:#f92672>&amp;&amp;</span> mv vosk-model-small-cn-0.22 vosk
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 2. Qwen2.5-0.5B-it</span>
</span></span><span style=display:flex><span>git lfs install
</span></span><span style=display:flex><span>git clone https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct qwen2.5-0.5b
</span></span></code></pre></div><h2 id=4-单文件-apppy>4. 单文件 app.py<a hidden class=anchor aria-hidden=true href=#4-单文件-apppy>#</a></h2><p>保存为 ~/qwen_stt/app.py 即可运行。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Vosk 关键词（拼音匹配） + Qwen2.5-0.5B（未命中）
</span></span></span><span style=display:flex><span><span style=color:#e6db74>- 命中关键词 → 返回 id
</span></span></span><span style=display:flex><span><span style=color:#e6db74>- 未命中 → 用 Vosk 原文对话
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os<span style=color:#f92672>,</span> tempfile<span style=color:#f92672>,</span> wave<span style=color:#f92672>,</span> json<span style=color:#f92672>,</span> asyncio<span style=color:#f92672>,</span> aiofiles<span style=color:#f92672>,</span> aiohttp
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fastapi <span style=color:#f92672>import</span> FastAPI, HTTPException, UploadFile, File
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pydantic <span style=color:#f92672>import</span> BaseModel, HttpUrl
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> vosk <span style=color:#f92672>import</span> Model, KaldiRecognizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>###############################################################################</span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>, <span style=color:#e6db74>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>, <span style=color:#e6db74>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>, <span style=color:#e6db74>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>}
</span></span><span style=display:flex><span>QWEN_PATH <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(__file__), <span style=color:#e6db74>&#34;models&#34;</span>, <span style=color:#e6db74>&#34;qwen2.5-0.5b&#34;</span>)
</span></span><span style=display:flex><span>VOSK_PATH <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(__file__), <span style=color:#e6db74>&#34;models&#34;</span>, <span style=color:#e6db74>&#34;vosk&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>vosk_model <span style=color:#f92672>=</span> Model(VOSK_PATH)
</span></span><span style=display:flex><span>tok <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(QWEN_PATH, trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>chat_model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    QWEN_PATH,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float16,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>app <span style=color:#f92672>=</span> FastAPI(title<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Vosk-Key-Qwen&#34;</span>, version<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;1.0&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>###############################################################################</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wav2text</span>(path: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> wave<span style=color:#f92672>.</span>open(path, <span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> wf:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> wf<span style=color:#f92672>.</span>getnchannels() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getsampwidth() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>or</span> wf<span style=color:#f92672>.</span>getframerate() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>16000</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;音频需 16 kHz 16-bit 单声道&#34;</span>)
</span></span><span style=display:flex><span>        rec <span style=color:#f92672>=</span> KaldiRecognizer(vosk_model, wf<span style=color:#f92672>.</span>getframerate())
</span></span><span style=display:flex><span>        res <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            data <span style=color:#f92672>=</span> wf<span style=color:#f92672>.</span>readframes(<span style=color:#ae81ff>4000</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> data:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> rec<span style=color:#f92672>.</span>AcceptWaveform(data):
</span></span><span style=display:flex><span>                res<span style=color:#f92672>.</span>append(json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>Result())[<span style=color:#e6db74>&#34;text&#34;</span>])
</span></span><span style=display:flex><span>        res<span style=color:#f92672>.</span>append(json<span style=color:#f92672>.</span>loads(rec<span style=color:#f92672>.</span>FinalResult())[<span style=color:#e6db74>&#34;text&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(res)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;拼音匹配&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    txt_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kid <span style=color:#f92672>in</span> KEYWORD2ID<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> fuzz<span style=color:#f92672>.</span>partial_ratio(<span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)), txt_py) <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>70</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> kw, kid
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>qwen_chat</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    msgs <span style=color:#f92672>=</span> [{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tok(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> chat_model<span style=color:#f92672>.</span>generate(<span style=color:#f92672>**</span>inputs, max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> tok<span style=color:#f92672>.</span>decode(out[<span style=color:#ae81ff>0</span>][len(inputs<span style=color:#f92672>.</span>input_ids[<span style=color:#ae81ff>0</span>]):], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>###############################################################################</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RecURL</span>(BaseModel):
</span></span><span style=display:flex><span>    url: HttpUrl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_do_recognize</span>(audio_path: str) <span style=color:#f92672>-&gt;</span> dict:
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>to_thread(wav2text, audio_path)
</span></span><span style=display:flex><span>    kw, kid <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> kw:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;text&#34;</span>: text, <span style=color:#e6db74>&#34;keyword&#34;</span>: kw, <span style=color:#e6db74>&#34;id&#34;</span>: kid}
</span></span><span style=display:flex><span>    reply <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>to_thread(qwen_chat, text)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;text&#34;</span>: text, <span style=color:#e6db74>&#34;keyword&#34;</span>: <span style=color:#66d9ef>None</span>, <span style=color:#e6db74>&#34;id&#34;</span>: <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#34;chat&#34;</span>: reply}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>###############################################################################</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@app.post</span>(<span style=color:#e6db74>&#34;/recognize&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>recognize_url</span>(req: RecURL):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>with</span> aiohttp<span style=color:#f92672>.</span>ClientSession(timeout<span style=color:#f92672>=</span>aiohttp<span style=color:#f92672>.</span>ClientTimeout(total<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)) <span style=color:#66d9ef>as</span> session:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>with</span> session<span style=color:#f92672>.</span>get(str(req<span style=color:#f92672>.</span>url)) <span style=color:#66d9ef>as</span> resp:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> resp<span style=color:#f92672>.</span>status <span style=color:#f92672>!=</span> <span style=color:#ae81ff>200</span>:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>raise</span> HTTPException(status_code<span style=color:#f92672>=</span><span style=color:#ae81ff>400</span>, detail<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;远程文件 </span><span style=color:#e6db74>{</span>resp<span style=color:#f92672>.</span>status<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                data <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> resp<span style=color:#f92672>.</span>read()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> tempfile<span style=color:#f92672>.</span>NamedTemporaryFile(suffix<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;.wav&#34;</span>, delete<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>) <span style=color:#66d9ef>as</span> tmp:
</span></span><span style=display:flex><span>            tmp<span style=color:#f92672>.</span>write(data)
</span></span><span style=display:flex><span>            tmp_path <span style=color:#f92672>=</span> tmp<span style=color:#f92672>.</span>name
</span></span><span style=display:flex><span>        out_path <span style=color:#f92672>=</span> tmp_path <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;_16k.wav&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>to_thread(<span style=color:#66d9ef>lambda</span>: os<span style=color:#f92672>.</span>system(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;ffmpeg -y -i </span><span style=color:#e6db74>{</span>tmp_path<span style=color:#e6db74>}</span><span style=color:#e6db74> -ar 16000 -ac 1 -sample_fmt s16 </span><span style=color:#e6db74>{</span>out_path<span style=color:#e6db74>}</span><span style=color:#e6db74> &gt;/dev/null 2&gt;&amp;1&#34;</span>))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>wait_for(_do_recognize(out_path), timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> asyncio<span style=color:#f92672>.</span>TimeoutError:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> HTTPException(status_code<span style=color:#f92672>=</span><span style=color:#ae81ff>504</span>, detail<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;处理超时&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> HTTPException(status_code<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, detail<span style=color:#f92672>=</span>str(e))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>finally</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> (tmp_path, out_path) <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;tmp_path&#39;</span> <span style=color:#f92672>in</span> locals() <span style=color:#66d9ef>else</span> ():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(p):
</span></span><span style=display:flex><span>                os<span style=color:#f92672>.</span>remove(p)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@app.post</span>(<span style=color:#e6db74>&#34;/upload&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>upload_file</span>(file: UploadFile <span style=color:#f92672>=</span> File(<span style=color:#f92672>...</span>)):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> tempfile<span style=color:#f92672>.</span>NamedTemporaryFile(suffix<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;.wav&#34;</span>, delete<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>) <span style=color:#66d9ef>as</span> tmp:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>with</span> aiofiles<span style=color:#f92672>.</span>open(tmp<span style=color:#f92672>.</span>name, <span style=color:#e6db74>&#34;wb&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>await</span> f<span style=color:#f92672>.</span>write(<span style=color:#66d9ef>await</span> file<span style=color:#f92672>.</span>read())
</span></span><span style=display:flex><span>            out <span style=color:#f92672>=</span> tmp<span style=color:#f92672>.</span>name <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;_16k.wav&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>to_thread(<span style=color:#66d9ef>lambda</span>: os<span style=color:#f92672>.</span>system(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;ffmpeg -y -i </span><span style=color:#e6db74>{</span>tmp<span style=color:#f92672>.</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74> -ar 16000 -ac 1 -sample_fmt s16 </span><span style=color:#e6db74>{</span>out<span style=color:#e6db74>}</span><span style=color:#e6db74> &gt;/dev/null 2&gt;&amp;1&#34;</span>))
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>wait_for(_do_recognize(out), timeout<span style=color:#f92672>=</span><span style=color:#ae81ff>15</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> asyncio<span style=color:#f92672>.</span>TimeoutError:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> HTTPException(status_code<span style=color:#f92672>=</span><span style=color:#ae81ff>504</span>, detail<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;处理超时&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> HTTPException(status_code<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, detail<span style=color:#f92672>=</span>str(e))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>finally</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> (tmp<span style=color:#f92672>.</span>name, out) <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;tmp.name&#39;</span> <span style=color:#f92672>in</span> locals() <span style=color:#66d9ef>else</span> ():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(p):
</span></span><span style=display:flex><span>                os<span style=color:#f92672>.</span>remove(p)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>###############################################################################</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>import</span> uvicorn
</span></span><span style=display:flex><span>    uvicorn<span style=color:#f92672>.</span>run(app, host<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;0.0.0.0&#34;</span>, port<span style=color:#f92672>=</span><span style=color:#ae81ff>8000</span>)
</span></span></code></pre></div><h2 id=启动>启动<a hidden class=anchor aria-hidden=true href=#启动>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>source ~/qwen_stt/bin/activate
</span></span><span style=display:flex><span>python ~/qwen_stt/app.py
</span></span></code></pre></div><h2 id=调用示例>调用示例<a hidden class=anchor aria-hidden=true href=#调用示例>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 远程</span>
</span></span><span style=display:flex><span>curl -X POST http://localhost:8000/recognize <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -H <span style=color:#e6db74>&#34;Content-Type: application/json&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -d <span style=color:#e6db74>&#39;{&#34;url&#34;:&#34;http://192.168.80.47:16552/test.wav&#34;}&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 本地</span>
</span></span><span style=display:flex><span>curl -X POST http://localhost:8000/upload <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>     -F <span style=color:#e6db74>&#34;file=@/home/ubuntu/test.wav&#34;</span>
</span></span></code></pre></div><p>返回（命中卧室灯）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-JSON data-lang=JSON><span style=display:flex><span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;帮我打开卧室灯&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;keyword&#34;</span>: <span style=color:#e6db74>&#34;卧室灯&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;chat&#34;</span>: <span style=color:#e6db74>&#34;已为您打开卧室灯！&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>未命中：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-JSON data-lang=JSON><span style=display:flex><span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;今天天气怎么样&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;keyword&#34;</span>: <span style=color:#66d9ef>null</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;id&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;chat&#34;</span>: <span style=color:#e6db74>&#34;今天北京晴，25℃。&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=multipart-err>multipart err<a hidden class=anchor aria-hidden=true href=#multipart-err>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install python-multipart
</span></span></code></pre></div><h1 id=qwen25-15b>Qwen2.5-1.5B<a hidden class=anchor aria-hidden=true href=#qwen25-15b>#</a></h1><p>0.5b->1.5b</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git lfs install
</span></span><span style=display:flex><span>git clone https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct qwen2.5-1.5b
</span></span></code></pre></div><h1 id=采样率-16khz保证vad-asr>采样率 16kHz，保证vad ASR<a hidden class=anchor aria-hidden=true href=#采样率-16khz保证vad-asr>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sherpa_onnx
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sounddevice <span style=color:#66d9ef>as</span> sd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> threading
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> queue
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> defaultdict
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> soundfile <span style=color:#66d9ef>as</span> sf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Dict, List, Optional, Tuple
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> resampy  <span style=color:#75715e># 用于重采样</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 路径工具 ------------------</span>
</span></span><span style=display:flex><span>current_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>abspath(__file__))
</span></span><span style=display:flex><span>parent_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(current_dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_relative_path</span>(relative_path: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(parent_dir, relative_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 关键词配置 ------------------</span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>, <span style=color:#e6db74>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>, <span style=color:#e6db74>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>, <span style=color:#e6db74>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str) <span style=color:#f92672>-&gt;</span> Tuple[Optional[str], int]:
</span></span><span style=display:flex><span>    txt_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kid <span style=color:#f92672>in</span> KEYWORD2ID<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> fuzz<span style=color:#f92672>.</span>partial_ratio(<span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)), txt_py) <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>70</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> kw, kid
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 大模型初始化 ------------------</span>
</span></span><span style=display:flex><span>QWEN_PATH <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/models/qwen2.5-0.5b&#34;</span>)
</span></span><span style=display:flex><span>tok <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(QWEN_PATH, trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chat_model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Qwen/Qwen2.5-0.5B&#34;</span>,
</span></span><span style=display:flex><span>    cache_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/root/room/models&#34;</span>,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>qwen_chat</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    msgs <span style=color:#f92672>=</span> [{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tok(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> chat_model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>            <span style=color:#f92672>**</span>inputs,
</span></span><span style=display:flex><span>            max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>            do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> tok<span style=color:#f92672>.</span>decode(out[<span style=color:#ae81ff>0</span>][len(inputs<span style=color:#f92672>.</span>input_ids[<span style=color:#ae81ff>0</span>]):], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 声纹相关 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_speaker_embedding_model</span>(model_path: str):
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingExtractorConfig(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model_path,
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> config<span style=color:#f92672>.</span>validate():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Invalid config. </span><span style=color:#e6db74>{</span>config<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingExtractor(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_speaker_file</span>(speaker_file_path: str) <span style=color:#f92672>-&gt;</span> Dict[str, List[str]]:
</span></span><span style=display:flex><span>    ans <span style=color:#f92672>=</span> defaultdict(list)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(speaker_file_path) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> f:
</span></span><span style=display:flex><span>            line <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> line:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            fields <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(fields) <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Invalid line: </span><span style=color:#e6db74>{</span>line<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            speaker_name, filename <span style=color:#f92672>=</span> fields
</span></span><span style=display:flex><span>            ans[speaker_name]<span style=color:#f92672>.</span>append(filename)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ans
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_speaker_embedding</span>(filenames: List[str], extractor) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>ndarray:
</span></span><span style=display:flex><span>    ans <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> filename <span style=color:#f92672>in</span> filenames:
</span></span><span style=display:flex><span>        samples, sr <span style=color:#f92672>=</span> sf<span style=color:#f92672>.</span>read(filename, always_2d<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        samples <span style=color:#f92672>=</span> samples[:, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        samples <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ascontiguousarray(samples)
</span></span><span style=display:flex><span>        stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>        stream<span style=color:#f92672>.</span>accept_waveform(sr, samples)
</span></span><span style=display:flex><span>        stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>        embedding <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(stream))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ans <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            ans <span style=color:#f92672>=</span> embedding
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            ans <span style=color:#f92672>+=</span> embedding
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ans <span style=color:#f92672>/</span> len(filenames)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_speaker_identification</span>():
</span></span><span style=display:flex><span>    speaker_model <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/wespeaker_zh_cnceleb_resnet34.onnx&#34;</span>)
</span></span><span style=display:flex><span>    speaker_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/speaker.txt&#34;</span>)
</span></span><span style=display:flex><span>    extractor <span style=color:#f92672>=</span> load_speaker_embedding_model(speaker_model)
</span></span><span style=display:flex><span>    speaker_data <span style=color:#f92672>=</span> load_speaker_file(speaker_file)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    manager <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingManager(extractor<span style=color:#f92672>.</span>dim)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> name, filelist <span style=color:#f92672>in</span> speaker_data<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        emb <span style=color:#f92672>=</span> compute_speaker_embedding(filelist, extractor)
</span></span><span style=display:flex><span>        status <span style=color:#f92672>=</span> manager<span style=color:#f92672>.</span>add(name, emb)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> status:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>RuntimeError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Failed to register speaker </span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> extractor, manager
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 唤醒词 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wake_word_detection</span>(input_device: int, input_sr: int) <span style=color:#f92672>-&gt;</span> bool:
</span></span><span style=display:flex><span>    kws_dir <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01&#34;</span>)
</span></span><span style=display:flex><span>    keyword_spotter <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>KeywordSpotter(
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        encoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        decoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        joiner<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        max_active_paths<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        keywords_file<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;keywords.txt&#34;</span>),
</span></span><span style=display:flex><span>        keywords_score<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>        keywords_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.25</span>,
</span></span><span style=display:flex><span>        num_trailing_blanks<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;等待唤醒词...&#34;</span>)
</span></span><span style=display:flex><span>    stream <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            stream<span style=color:#f92672>.</span>accept_waveform(input_sr, samples)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> keyword_spotter<span style=color:#f92672>.</span>is_ready(stream):
</span></span><span style=display:flex><span>                keyword_spotter<span style=color:#f92672>.</span>decode_stream(stream)
</span></span><span style=display:flex><span>                result <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>get_result(stream)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> result:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;检测到唤醒词: </span><span style=color:#e6db74>{</span>result<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                    keyword_spotter<span style=color:#f92672>.</span>reset_stream(stream)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 语音识别 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>speech_recognition</span>(extractor, speaker_manager, input_device: int, input_sr: int) <span style=color:#f92672>-&gt;</span> Tuple[str, str]:
</span></span><span style=display:flex><span>    target_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>16000</span>  <span style=color:#75715e># VAD/ASR 采样率</span>
</span></span><span style=display:flex><span>    recognizer <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineRecognizer<span style=color:#f92672>.</span>from_paraformer(
</span></span><span style=display:flex><span>        paraformer<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        sample_rate<span style=color:#f92672>=</span>target_sr,
</span></span><span style=display:flex><span>        feature_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>80</span>,
</span></span><span style=display:flex><span>        decoding_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;greedy_search&#34;</span>,
</span></span><span style=display:flex><span>        debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    vad_config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>VadModelConfig()
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/silero_vad.onnx&#34;</span>)
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>min_silence_duration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>min_speech_duration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>sample_rate <span style=color:#f92672>=</span> target_sr
</span></span><span style=display:flex><span>    vad <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>VoiceActivityDetector(vad_config, buffer_size_in_seconds<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    window_size <span style=color:#f92672>=</span> vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>window_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;请说出您的指令...&#34;</span>)
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>    audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(samples, input_sr, target_sr)
</span></span><span style=display:flex><span>            audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate([audio_buffer, resampled])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> len(audio_buffer) <span style=color:#f92672>&gt;</span> window_size:
</span></span><span style=display:flex><span>                vad<span style=color:#f92672>.</span>accept_waveform(audio_buffer[:window_size])
</span></span><span style=display:flex><span>                audio_buffer <span style=color:#f92672>=</span> audio_buffer[window_size:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> vad<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> len(vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> target_sr:
</span></span><span style=display:flex><span>                    vad<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                stream<span style=color:#f92672>.</span>accept_waveform(target_sr, vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples)
</span></span><span style=display:flex><span>                stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(stream))
</span></span><span style=display:flex><span>                speaker <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>) <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>                asr_stream <span style=color:#f92672>=</span> recognizer<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                asr_stream<span style=color:#f92672>.</span>accept_waveform(target_sr, vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples)
</span></span><span style=display:flex><span>                recognizer<span style=color:#f92672>.</span>decode_stream(asr_stream)
</span></span><span style=display:flex><span>                text <span style=color:#f92672>=</span> asr_stream<span style=color:#f92672>.</span>result<span style=color:#f92672>.</span>text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                vad<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> text, speaker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ TTS ------------------</span>
</span></span><span style=display:flex><span>buffer <span style=color:#f92672>=</span> queue<span style=color:#f92672>.</span>Queue()
</span></span><span style=display:flex><span>started <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>killed <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>event <span style=color:#f92672>=</span> threading<span style=color:#f92672>.</span>Event()
</span></span><span style=display:flex><span>tts_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>24000</span>
</span></span><span style=display:flex><span>output_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>48000</span>  <span style=color:#75715e># 假设声卡输出 48kHz</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generated_audio_callback</span>(samples: np<span style=color:#f92672>.</span>ndarray, progress: float) <span style=color:#f92672>-&gt;</span> int:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> started
</span></span><span style=display:flex><span>    <span style=color:#75715e># 这里重采样 TTS 输出到声卡的采样率</span>
</span></span><span style=display:flex><span>    resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(samples, tts_sr, output_sr)
</span></span><span style=display:flex><span>    buffer<span style=color:#f92672>.</span>put(resampled)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> started:
</span></span><span style=display:flex><span>        started <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> killed <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>play_audio_callback</span>(outdata: np<span style=color:#f92672>.</span>ndarray, frames: int, time, status):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> killed <span style=color:#f92672>or</span> (started <span style=color:#f92672>and</span> buffer<span style=color:#f92672>.</span>empty() <span style=color:#f92672>and</span> stopped):
</span></span><span style=display:flex><span>        event<span style=color:#f92672>.</span>set()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> buffer<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>        outdata<span style=color:#f92672>.</span>fill(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> n <span style=color:#f92672>&lt;</span> frames <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> buffer<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>        remaining <span style=color:#f92672>=</span> frames <span style=color:#f92672>-</span> n
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> remaining <span style=color:#f92672>&lt;=</span> k:
</span></span><span style=display:flex><span>            outdata[n:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>][:remaining]
</span></span><span style=display:flex><span>            buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>][remaining:]
</span></span><span style=display:flex><span>            n <span style=color:#f92672>=</span> frames
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                buffer<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        outdata[n:n <span style=color:#f92672>+</span> k, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>        n <span style=color:#f92672>+=</span> k
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> n <span style=color:#f92672>&lt;</span> frames:
</span></span><span style=display:flex><span>        outdata[n:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tts_synthesis</span>(text: str, output_device: int):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> tts_sr, started, stopped
</span></span><span style=display:flex><span>    tts_config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsConfig(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsModelConfig(
</span></span><span style=display:flex><span>            vits<span style=color:#f92672>=</span>sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsVitsModelConfig(
</span></span><span style=display:flex><span>                model<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/zh_CN-huayan-medium.onnx&#34;</span>),
</span></span><span style=display:flex><span>                tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>                data_dir<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/espeak-ng-data&#34;</span>),
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>            debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        max_num_sentences<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    tts <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineTts(tts_config)
</span></span><span style=display:flex><span>    tts_sr <span style=color:#f92672>=</span> tts<span style=color:#f92672>.</span>sample_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    threading<span style=color:#f92672>.</span>Thread(target<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span>: sd<span style=color:#f92672>.</span>OutputStream(
</span></span><span style=display:flex><span>        device<span style=color:#f92672>=</span>output_device,
</span></span><span style=display:flex><span>        channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        callback<span style=color:#f92672>=</span>play_audio_callback,
</span></span><span style=display:flex><span>        dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>,
</span></span><span style=display:flex><span>        samplerate<span style=color:#f92672>=</span>output_sr,
</span></span><span style=display:flex><span>        blocksize<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>    )<span style=color:#f92672>.</span>start())<span style=color:#f92672>.</span>start()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    event<span style=color:#f92672>.</span>clear()
</span></span><span style=display:flex><span>    started <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    tts<span style=color:#f92672>.</span>generate(text, sid<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, speed<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>, callback<span style=color:#f92672>=</span>generated_audio_callback)
</span></span><span style=display:flex><span>    stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    event<span style=color:#f92672>.</span>wait()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 主流程 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>select_devices</span>():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;=== 可用输入输出设备列表 ===&#34;</span>)
</span></span><span style=display:flex><span>    print(sd<span style=color:#f92672>.</span>query_devices())
</span></span><span style=display:flex><span>    input_device <span style=color:#f92672>=</span> int(input(<span style=color:#e6db74>&#34;请输入输入设备ID: &#34;</span>))
</span></span><span style=display:flex><span>    output_device <span style=color:#f92672>=</span> int(input(<span style=color:#e6db74>&#34;请输入输出设备ID: &#34;</span>))
</span></span><span style=display:flex><span>    input_info <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>query_devices(input_device)
</span></span><span style=display:flex><span>    output_info <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>query_devices(output_device)
</span></span><span style=display:flex><span>    input_sr <span style=color:#f92672>=</span> int(input_info[<span style=color:#e6db74>&#39;default_samplerate&#39;</span>])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;选择输入采样率: </span><span style=color:#e6db74>{</span>input_sr<span style=color:#e6db74>}</span><span style=color:#e6db74> Hz&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> input_device, output_device, input_sr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> killed
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        input_device, output_device, input_sr <span style=color:#f92672>=</span> select_devices()
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;初始化声纹识别...&#34;</span>)
</span></span><span style=display:flex><span>        extractor, speaker_manager <span style=color:#f92672>=</span> init_speaker_identification()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> wake_word_detection(input_device, input_sr):
</span></span><span style=display:flex><span>            text, speaker <span style=color:#f92672>=</span> speech_recognition(extractor, speaker_manager, input_device, input_sr)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;识别结果: </span><span style=color:#e6db74>{</span>text<span style=color:#e6db74>}</span><span style=color:#e6db74> (说话人: </span><span style=color:#e6db74>{</span>speaker<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            keyword, kid <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> keyword:
</span></span><span style=display:flex><span>                reply <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;已为您操作【</span><span style=color:#e6db74>{</span>keyword<span style=color:#e6db74>}</span><span style=color:#e6db74>】，编号</span><span style=color:#e6db74>{</span>kid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                reply <span style=color:#f92672>=</span> qwen_chat(text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;回复:&#34;</span>, reply)
</span></span><span style=display:flex><span>            tts_synthesis(reply, output_device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>KeyboardInterrupt</span>:
</span></span><span style=display:flex><span>        killed <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>程序已终止&#34;</span>)
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>exit(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><h1 id=qwen去除无关的符号>qwen去除无关的符号<a hidden class=anchor aria-hidden=true href=#qwen去除无关的符号>#</a></h1><p>Qwen等大模型输出太自由，修改prompt</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>msgs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;你是一个简洁的中文助手，只输出简短的答案，不要解释。&#34;</span>},
</span></span><span style=display:flex><span>    {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p>完整代码</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>qwen_chat</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    msgs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;你是一个简洁的助手，只回答核心内容，用中文回答。&#34;</span>},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tok(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> chat_model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>            <span style=color:#f92672>**</span>inputs,
</span></span><span style=display:flex><span>            max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>            do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>,
</span></span><span style=display:flex><span>            pad_token_id<span style=color:#f92672>=</span>tok<span style=color:#f92672>.</span>eos_token_id  
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    raw_output <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>decode(out[<span style=color:#ae81ff>0</span>][len(inputs<span style=color:#f92672>.</span>input_ids[<span style=color:#ae81ff>0</span>]):], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#75715e># 额外清理</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;&lt;/s&gt;&#34;</span>, <span style=color:#e6db74>&#34;&lt;/user&gt;&#34;</span>, <span style=color:#e6db74>&#34;.user&#34;</span>, <span style=color:#e6db74>&#34;🤨&#34;</span>, <span style=color:#e6db74>&#34;## 128000&#34;</span>]:
</span></span><span style=display:flex><span>        raw_output <span style=color:#f92672>=</span> raw_output<span style=color:#f92672>.</span>replace(token, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>    reply <span style=color:#f92672>=</span> raw_output<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> reply
</span></span></code></pre></div><h2 id=语速调整>语速调整<a hidden class=anchor aria-hidden=true href=#语速调整>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    tts<span style=color:#f92672>.</span>generate(text, sid<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, speed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, callback<span style=color:#f92672>=</span>generated_audio_callback)
</span></span></code></pre></div><h1 id=分离配置文件json-prompt>分离配置文件json prompt<a hidden class=anchor aria-hidden=true href=#分离配置文件json-prompt>#</a></h1><p>KEYWORD2ID 改成从 keywords.json 文件读取，把 system prompt 变成从外部 system_prompt.txt 文件读取，并且在程序里做错误处理和相应的加载。
下面是修改后的完整示例代码，我保持了结构清晰，你可以直接替换：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sherpa_onnx
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sounddevice <span style=color:#66d9ef>as</span> sd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> threading
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> queue
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> defaultdict
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> soundfile <span style=color:#66d9ef>as</span> sf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Dict, List, Optional, Tuple
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> resampy  <span style=color:#75715e># 用于重采样</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 路径工具 ------------------</span>
</span></span><span style=display:flex><span>current_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>abspath(__file__))
</span></span><span style=display:flex><span>parent_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(current_dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_relative_path</span>(relative_path: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(parent_dir, relative_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 加载关键词配置 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_keywords</span>() <span style=color:#f92672>-&gt;</span> Dict[str, int]:
</span></span><span style=display:flex><span>    keywords_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;keywords.json&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(keywords_file):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>FileNotFoundError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;未找到关键词文件: </span><span style=color:#e6db74>{</span>keywords_file<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(keywords_file, <span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> json<span style=color:#f92672>.</span>load(f)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> load_keywords()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str) <span style=color:#f92672>-&gt;</span> Tuple[Optional[str], int]:
</span></span><span style=display:flex><span>    txt_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kid <span style=color:#f92672>in</span> KEYWORD2ID<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> fuzz<span style=color:#f92672>.</span>partial_ratio(<span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)), txt_py) <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>70</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> kw, kid
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 加载 system prompt ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_system_prompt</span>() <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    prompt_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;system_prompt.txt&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(prompt_file):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>FileNotFoundError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;未找到 system prompt 文件: </span><span style=color:#e6db74>{</span>prompt_file<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(prompt_file, <span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> f<span style=color:#f92672>.</span>read()<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>SYSTEM_PROMPT <span style=color:#f92672>=</span> load_system_prompt()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 大模型初始化 ------------------</span>
</span></span><span style=display:flex><span>QWEN_PATH <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/models/qwen2.5-0.5b&#34;</span>)
</span></span><span style=display:flex><span>tok <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(QWEN_PATH, trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chat_model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Qwen/Qwen2.5-0.5B&#34;</span>,
</span></span><span style=display:flex><span>    cache_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/root/room/models&#34;</span>,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>qwen_chat</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    msgs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: SYSTEM_PROMPT},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tok(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> chat_model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>            <span style=color:#f92672>**</span>inputs,
</span></span><span style=display:flex><span>            max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>            do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>,
</span></span><span style=display:flex><span>            pad_token_id<span style=color:#f92672>=</span>tok<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    raw_output <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>decode(out[<span style=color:#ae81ff>0</span>][len(inputs<span style=color:#f92672>.</span>input_ids[<span style=color:#ae81ff>0</span>]):], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;&lt;/s&gt;&#34;</span>, <span style=color:#e6db74>&#34;&lt;/user&gt;&#34;</span>, <span style=color:#e6db74>&#34;.user&#34;</span>, <span style=color:#e6db74>&#34;🤨&#34;</span>, <span style=color:#e6db74>&#34;## 128000&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\x0c</span><span style=color:#e6db74>&#34;</span>]:
</span></span><span style=display:flex><span>        raw_output <span style=color:#f92672>=</span> raw_output<span style=color:#f92672>.</span>replace(token, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>    reply <span style=color:#f92672>=</span> raw_output<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> reply
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 声纹相关 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_speaker_embedding_model</span>(model_path: str):
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingExtractorConfig(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model_path,
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> config<span style=color:#f92672>.</span>validate():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Invalid config. </span><span style=color:#e6db74>{</span>config<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingExtractor(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_speaker_file</span>(speaker_file_path: str) <span style=color:#f92672>-&gt;</span> Dict[str, List[str]]:
</span></span><span style=display:flex><span>    ans <span style=color:#f92672>=</span> defaultdict(list)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(speaker_file_path) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> f:
</span></span><span style=display:flex><span>            line <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> line:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            fields <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(fields) <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Invalid line: </span><span style=color:#e6db74>{</span>line<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            speaker_name, filename <span style=color:#f92672>=</span> fields
</span></span><span style=display:flex><span>            ans[speaker_name]<span style=color:#f92672>.</span>append(filename)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ans
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_speaker_embedding</span>(filenames: List[str], extractor) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>ndarray:
</span></span><span style=display:flex><span>    ans <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> filename <span style=color:#f92672>in</span> filenames:
</span></span><span style=display:flex><span>        samples, sr <span style=color:#f92672>=</span> sf<span style=color:#f92672>.</span>read(filename, always_2d<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        samples <span style=color:#f92672>=</span> samples[:, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        samples <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ascontiguousarray(samples)
</span></span><span style=display:flex><span>        stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>        stream<span style=color:#f92672>.</span>accept_waveform(sr, samples)
</span></span><span style=display:flex><span>        stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>        embedding <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(stream))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ans <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            ans <span style=color:#f92672>=</span> embedding
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            ans <span style=color:#f92672>+=</span> embedding
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ans <span style=color:#f92672>/</span> len(filenames)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_speaker_identification</span>():
</span></span><span style=display:flex><span>    speaker_model <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/wespeaker_zh_cnceleb_resnet34.onnx&#34;</span>)
</span></span><span style=display:flex><span>    speaker_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/speaker.txt&#34;</span>)
</span></span><span style=display:flex><span>    extractor <span style=color:#f92672>=</span> load_speaker_embedding_model(speaker_model)
</span></span><span style=display:flex><span>    speaker_data <span style=color:#f92672>=</span> load_speaker_file(speaker_file)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    manager <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingManager(extractor<span style=color:#f92672>.</span>dim)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> name, filelist <span style=color:#f92672>in</span> speaker_data<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        emb <span style=color:#f92672>=</span> compute_speaker_embedding(filelist, extractor)
</span></span><span style=display:flex><span>        status <span style=color:#f92672>=</span> manager<span style=color:#f92672>.</span>add(name, emb)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> status:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>RuntimeError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Failed to register speaker </span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> extractor, manager
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 唤醒词 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wake_word_detection</span>(input_device: int, input_sr: int) <span style=color:#f92672>-&gt;</span> bool:
</span></span><span style=display:flex><span>    kws_dir <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01&#34;</span>)
</span></span><span style=display:flex><span>    keyword_spotter <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>KeywordSpotter(
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        encoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        decoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        joiner<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        max_active_paths<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        keywords_file<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;keywords.txt&#34;</span>),
</span></span><span style=display:flex><span>        keywords_score<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>        keywords_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.25</span>,
</span></span><span style=display:flex><span>        num_trailing_blanks<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;等待唤醒词...&#34;</span>)
</span></span><span style=display:flex><span>    stream <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            stream<span style=color:#f92672>.</span>accept_waveform(input_sr, samples)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> keyword_spotter<span style=color:#f92672>.</span>is_ready(stream):
</span></span><span style=display:flex><span>                keyword_spotter<span style=color:#f92672>.</span>decode_stream(stream)
</span></span><span style=display:flex><span>                result <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>get_result(stream)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> result:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;检测到唤醒词: </span><span style=color:#e6db74>{</span>result<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                    keyword_spotter<span style=color:#f92672>.</span>reset_stream(stream)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 语音识别 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>speech_recognition</span>(extractor, speaker_manager, input_device: int, input_sr: int) <span style=color:#f92672>-&gt;</span> Tuple[str, str]:
</span></span><span style=display:flex><span>    target_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>16000</span>  <span style=color:#75715e># VAD/ASR 采样率</span>
</span></span><span style=display:flex><span>    recognizer <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineRecognizer<span style=color:#f92672>.</span>from_paraformer(
</span></span><span style=display:flex><span>        paraformer<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        sample_rate<span style=color:#f92672>=</span>target_sr,
</span></span><span style=display:flex><span>        feature_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>80</span>,
</span></span><span style=display:flex><span>        decoding_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;greedy_search&#34;</span>,
</span></span><span style=display:flex><span>        debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    vad_config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>VadModelConfig()
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/silero_vad.onnx&#34;</span>)
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>min_silence_duration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>min_speech_duration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>sample_rate <span style=color:#f92672>=</span> target_sr
</span></span><span style=display:flex><span>    vad <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>VoiceActivityDetector(vad_config, buffer_size_in_seconds<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    window_size <span style=color:#f92672>=</span> vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>window_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;请说出您的指令...&#34;</span>)
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>    audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(samples, input_sr, target_sr)
</span></span><span style=display:flex><span>            audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate([audio_buffer, resampled])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> len(audio_buffer) <span style=color:#f92672>&gt;</span> window_size:
</span></span><span style=display:flex><span>                vad<span style=color:#f92672>.</span>accept_waveform(audio_buffer[:window_size])
</span></span><span style=display:flex><span>                audio_buffer <span style=color:#f92672>=</span> audio_buffer[window_size:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> vad<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> len(vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> target_sr:
</span></span><span style=display:flex><span>                    vad<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                stream<span style=color:#f92672>.</span>accept_waveform(target_sr, vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples)
</span></span><span style=display:flex><span>                stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(stream))
</span></span><span style=display:flex><span>                speaker <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>) <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>                asr_stream <span style=color:#f92672>=</span> recognizer<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                asr_stream<span style=color:#f92672>.</span>accept_waveform(target_sr, vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples)
</span></span><span style=display:flex><span>                recognizer<span style=color:#f92672>.</span>decode_stream(asr_stream)
</span></span><span style=display:flex><span>                text <span style=color:#f92672>=</span> asr_stream<span style=color:#f92672>.</span>result<span style=color:#f92672>.</span>text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                vad<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> text, speaker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ TTS ------------------</span>
</span></span><span style=display:flex><span>buffer <span style=color:#f92672>=</span> queue<span style=color:#f92672>.</span>Queue()
</span></span><span style=display:flex><span>started <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>killed <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>event <span style=color:#f92672>=</span> threading<span style=color:#f92672>.</span>Event()
</span></span><span style=display:flex><span>tts_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>24000</span>
</span></span><span style=display:flex><span>output_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>48000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generated_audio_callback</span>(samples: np<span style=color:#f92672>.</span>ndarray, progress: float) <span style=color:#f92672>-&gt;</span> int:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> started
</span></span><span style=display:flex><span>    resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(samples, tts_sr, output_sr)
</span></span><span style=display:flex><span>    buffer<span style=color:#f92672>.</span>put(resampled)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> started:
</span></span><span style=display:flex><span>        started <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> killed <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>play_audio_callback</span>(outdata: np<span style=color:#f92672>.</span>ndarray, frames: int, time, status):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> killed <span style=color:#f92672>or</span> (started <span style=color:#f92672>and</span> buffer<span style=color:#f92672>.</span>empty() <span style=color:#f92672>and</span> stopped):
</span></span><span style=display:flex><span>        event<span style=color:#f92672>.</span>set()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> buffer<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>        outdata<span style=color:#f92672>.</span>fill(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> n <span style=color:#f92672>&lt;</span> frames <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> buffer<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>        remaining <span style=color:#f92672>=</span> frames <span style=color:#f92672>-</span> n
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> remaining <span style=color:#f92672>&lt;=</span> k:
</span></span><span style=display:flex><span>            outdata[n:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>][:remaining]
</span></span><span style=display:flex><span>            buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>][remaining:]
</span></span><span style=display:flex><span>            n <span style=color:#f92672>=</span> frames
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                buffer<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        outdata[n:n <span style=color:#f92672>+</span> k, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>        n <span style=color:#f92672>+=</span> k
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> n <span style=color:#f92672>&lt;</span> frames:
</span></span><span style=display:flex><span>        outdata[n:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tts_synthesis</span>(text: str, output_device: int):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> tts_sr, started, stopped
</span></span><span style=display:flex><span>    tts_config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsConfig(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsModelConfig(
</span></span><span style=display:flex><span>            vits<span style=color:#f92672>=</span>sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsVitsModelConfig(
</span></span><span style=display:flex><span>                model<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/zh_CN-huayan-medium.onnx&#34;</span>),
</span></span><span style=display:flex><span>                tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>                data_dir<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/espeak-ng-data&#34;</span>),
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>            debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        max_num_sentences<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    tts <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineTts(tts_config)
</span></span><span style=display:flex><span>    tts_sr <span style=color:#f92672>=</span> tts<span style=color:#f92672>.</span>sample_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    threading<span style=color:#f92672>.</span>Thread(target<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span>: sd<span style=color:#f92672>.</span>OutputStream(
</span></span><span style=display:flex><span>        device<span style=color:#f92672>=</span>output_device,
</span></span><span style=display:flex><span>        channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        callback<span style=color:#f92672>=</span>play_audio_callback,
</span></span><span style=display:flex><span>        dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>,
</span></span><span style=display:flex><span>        samplerate<span style=color:#f92672>=</span>output_sr,
</span></span><span style=display:flex><span>        blocksize<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>    )<span style=color:#f92672>.</span>start())<span style=color:#f92672>.</span>start()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    event<span style=color:#f92672>.</span>clear()
</span></span><span style=display:flex><span>    started <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    tts<span style=color:#f92672>.</span>generate(text, sid<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, speed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, callback<span style=color:#f92672>=</span>generated_audio_callback)
</span></span><span style=display:flex><span>    stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    event<span style=color:#f92672>.</span>wait()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 主流程 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>select_devices</span>():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;=== 可用输入输出设备列表 ===&#34;</span>)
</span></span><span style=display:flex><span>    print(sd<span style=color:#f92672>.</span>query_devices())
</span></span><span style=display:flex><span>    input_device <span style=color:#f92672>=</span> int(input(<span style=color:#e6db74>&#34;请输入输入设备ID: &#34;</span>))
</span></span><span style=display:flex><span>    output_device <span style=color:#f92672>=</span> int(input(<span style=color:#e6db74>&#34;请输入输出设备ID: &#34;</span>))
</span></span><span style=display:flex><span>    input_info <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>query_devices(input_device)
</span></span><span style=display:flex><span>    output_info <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>query_devices(output_device)
</span></span><span style=display:flex><span>    input_sr <span style=color:#f92672>=</span> int(input_info[<span style=color:#e6db74>&#39;default_samplerate&#39;</span>])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;选择输入采样率: </span><span style=color:#e6db74>{</span>input_sr<span style=color:#e6db74>}</span><span style=color:#e6db74> Hz&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> input_device, output_device, input_sr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> killed
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        input_device, output_device, input_sr <span style=color:#f92672>=</span> select_devices()
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;初始化声纹识别...&#34;</span>)
</span></span><span style=display:flex><span>        extractor, speaker_manager <span style=color:#f92672>=</span> init_speaker_identification()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> wake_word_detection(input_device, input_sr):
</span></span><span style=display:flex><span>            text, speaker <span style=color:#f92672>=</span> speech_recognition(extractor, speaker_manager, input_device, input_sr)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;识别结果: </span><span style=color:#e6db74>{</span>text<span style=color:#e6db74>}</span><span style=color:#e6db74> (说话人: </span><span style=color:#e6db74>{</span>speaker<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            keyword, kid <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> keyword:
</span></span><span style=display:flex><span>                reply <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;已为您操作【</span><span style=color:#e6db74>{</span>keyword<span style=color:#e6db74>}</span><span style=color:#e6db74>】，编号</span><span style=color:#e6db74>{</span>kid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                reply <span style=color:#f92672>=</span> qwen_chat(text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;回复:&#34;</span>, reply)
</span></span><span style=display:flex><span>            tts_synthesis(reply, output_device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>KeyboardInterrupt</span>:
</span></span><span style=display:flex><span>        killed <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>程序已终止&#34;</span>)
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>exit(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><p>keywords.json</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;空调&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;电视&#34;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;风扇&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;卧室灯&#34;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;客厅灯&#34;</span>: <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>system_prompt.txt</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>你是一个简洁的助手，只回答核心内容，用中文回答。
</span></span></code></pre></div><p>外部编辑 keywords.json 和 system_prompt.txt 既可</p><h1 id=先加载声纹再选设备>先加载声纹再选设备<a hidden class=anchor aria-hidden=true href=#先加载声纹再选设备>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>        extractor, speaker_manager <span style=color:#f92672>=</span> init_speaker_identification()
</span></span><span style=display:flex><span>        input_device, output_device, input_sr <span style=color:#f92672>=</span> select_devices()
</span></span></code></pre></div><h1 id=语音识别-分离>语音识别 分离<a hidden class=anchor aria-hidden=true href=#语音识别-分离>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> speechbrain.inference.separation <span style=color:#f92672>import</span> SepformerSeparation <span style=color:#66d9ef>as</span> separator
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchaudio
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchaudio.transforms <span style=color:#66d9ef>as</span> T
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 输入混合音频文件</span>
</span></span><span style=display:flex><span>input_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;your_mix.wav&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 加载原始音频</span>
</span></span><span style=display:flex><span>waveform, sample_rate <span style=color:#f92672>=</span> torchaudio<span style=color:#f92672>.</span>load(input_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 加载模型</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> separator<span style=color:#f92672>.</span>from_hparams(
</span></span><span style=display:flex><span>    source<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;speechbrain/sepformer-wsj02mix&#34;</span>,  <span style=color:#75715e># 这个模型要求 8k 采样率</span>
</span></span><span style=display:flex><span>    savedir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pretrained_models/sepformer-wsj02mix&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 从模型配置中获取目标采样率</span>
</span></span><span style=display:flex><span>target_sample_rate <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>hparams<span style=color:#f92672>.</span>sample_rate <span style=color:#66d9ef>if</span> hasattr(model<span style=color:#f92672>.</span>hparams, <span style=color:#e6db74>&#34;sample_rate&#34;</span>) <span style=color:#66d9ef>else</span> sample_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 如果采样率不同，自动转换</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> sample_rate <span style=color:#f92672>!=</span> target_sample_rate:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;采样率不匹配：</span><span style=color:#e6db74>{</span>sample_rate<span style=color:#e6db74>}</span><span style=color:#e6db74> → </span><span style=color:#e6db74>{</span>target_sample_rate<span style=color:#e6db74>}</span><span style=color:#e6db74>，正在转换...&#34;</span>)
</span></span><span style=display:flex><span>    resampler <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>Resample(orig_freq<span style=color:#f92672>=</span>sample_rate, new_freq<span style=color:#f92672>=</span>target_sample_rate)
</span></span><span style=display:flex><span>    waveform <span style=color:#f92672>=</span> resampler(waveform)
</span></span><span style=display:flex><span>    sample_rate <span style=color:#f92672>=</span> target_sample_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 临时保存转换后的音频（因为 SpeechBrain 的接口要求文件路径）</span>
</span></span><span style=display:flex><span>tmp_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;temp_resampled.wav&#34;</span>
</span></span><span style=display:flex><span>torchaudio<span style=color:#f92672>.</span>save(tmp_path, waveform, sample_rate)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 分离</span>
</span></span><span style=display:flex><span>est_sources <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>separate_file(path<span style=color:#f92672>=</span>tmp_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 自动检测人数</span>
</span></span><span style=display:flex><span>num_speakers <span style=color:#f92672>=</span> est_sources<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;检测到 </span><span style=color:#e6db74>{</span>num_speakers<span style=color:#e6db74>}</span><span style=color:#e6db74> 个声源&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 保存每个分离结果（用原采样率保存）</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>makedirs(<span style=color:#e6db74>&#34;separated&#34;</span>, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_speakers):
</span></span><span style=display:flex><span>    out_path <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;separated/speaker_</span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>.wav&#34;</span>
</span></span><span style=display:flex><span>    torchaudio<span style=color:#f92672>.</span>save(out_path, est_sources[:, :, i]<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu(), sample_rate)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;已保存: </span><span style=color:#e6db74>{</span>out_path<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 删除临时文件</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>remove(tmp_path)
</span></span></code></pre></div><p>依赖</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo apt update
</span></span><span style=display:flex><span>sudo apt install ffmpeg
</span></span><span style=display:flex><span>pip install soundfile
</span></span><span style=display:flex><span>sudo apt install libsndfile1
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python3 app.py
</span></span></code></pre></div><h1 id=stt-分离-英文-web>stt 分离 英文 web<a hidden class=anchor aria-hidden=true href=#stt-分离-英文-web>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install speechbrain torchaudio fastapi uvicorn python-multipart
</span></span></code></pre></div><h2 id=项目结构>项目结构<a hidden class=anchor aria-hidden=true href=#项目结构>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>speechbrain_stt_project/
</span></span><span style=display:flex><span>├── app.py                  <span style=color:#75715e># FastAPI 后端主程序</span>
</span></span><span style=display:flex><span>├── static/
</span></span><span style=display:flex><span>│   └── index.html          <span style=color:#75715e># 前端静态页面</span>
</span></span><span style=display:flex><span>├── pretrained_models/      <span style=color:#75715e># 预训练模型目录（自动下载或手动放置）</span>
</span></span><span style=display:flex><span>├── uploads/                <span style=color:#75715e># 上传文件目录（运行时自动创建）</span>
</span></span><span style=display:flex><span>├── separated/              <span style=color:#75715e># 分离音频输出目录（运行时自动创建）</span>
</span></span><span style=display:flex><span>├── requirements.txt        <span style=color:#75715e># Python依赖</span>
</span></span><span style=display:flex><span>└── Dockerfile              <span style=color:#75715e># 容器构建文件</span>
</span></span></code></pre></div><h2 id=文件内容示例>文件内容示例<a hidden class=anchor aria-hidden=true href=#文件内容示例>#</a></h2><p>app.py</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fastapi <span style=color:#f92672>import</span> FastAPI, UploadFile, File
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fastapi.responses <span style=color:#f92672>import</span> JSONResponse, HTMLResponse
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fastapi.staticfiles <span style=color:#f92672>import</span> StaticFiles
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchaudio
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchaudio.transforms <span style=color:#66d9ef>as</span> T
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> shutil
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> speechbrain.inference <span style=color:#f92672>import</span> SepformerSeparation <span style=color:#66d9ef>as</span> separator
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> speechbrain.pretrained <span style=color:#f92672>import</span> EncoderDecoderASR
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>app <span style=color:#f92672>=</span> FastAPI()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 挂载静态文件夹，访问 http://host:port/</span>
</span></span><span style=display:flex><span>app<span style=color:#f92672>.</span>mount(<span style=color:#e6db74>&#34;/static&#34;</span>, StaticFiles(directory<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;static&#34;</span>), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;static&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 加载模型</span>
</span></span><span style=display:flex><span>sep_model <span style=color:#f92672>=</span> separator<span style=color:#f92672>.</span>from_hparams(
</span></span><span style=display:flex><span>    source<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;speechbrain/sepformer-wsj02mix&#34;</span>,
</span></span><span style=display:flex><span>    savedir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pretrained_models/sepformer-wsj02mix&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>asr_model <span style=color:#f92672>=</span> EncoderDecoderASR<span style=color:#f92672>.</span>from_hparams(
</span></span><span style=display:flex><span>    source<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;speechbrain/asr-transformer-transformerlm-librispeech&#34;</span>,
</span></span><span style=display:flex><span>    savedir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pretrained_models/asr-transformer-transformerlm-librispeech&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@app.get</span>(<span style=color:#e6db74>&#34;/&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>homepage</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;static/index.html&#34;</span>, <span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        html_content <span style=color:#f92672>=</span> f<span style=color:#f92672>.</span>read()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> HTMLResponse(content<span style=color:#f92672>=</span>html_content)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@app.post</span>(<span style=color:#e6db74>&#34;/upload/&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>upload_audio</span>(file: UploadFile <span style=color:#f92672>=</span> File(<span style=color:#f92672>...</span>)):
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>makedirs(<span style=color:#e6db74>&#34;uploads&#34;</span>, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    os<span style=color:#f92672>.</span>makedirs(<span style=color:#e6db74>&#34;separated&#34;</span>, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    file_path <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;uploads/</span><span style=color:#e6db74>{</span>file<span style=color:#f92672>.</span>filename<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(file_path, <span style=color:#e6db74>&#34;wb&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        shutil<span style=color:#f92672>.</span>copyfileobj(file<span style=color:#f92672>.</span>file, f)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    waveform, sample_rate <span style=color:#f92672>=</span> torchaudio<span style=color:#f92672>.</span>load(file_path)
</span></span><span style=display:flex><span>    target_sr <span style=color:#f92672>=</span> sep_model<span style=color:#f92672>.</span>hparams<span style=color:#f92672>.</span>sample_rate <span style=color:#66d9ef>if</span> hasattr(sep_model<span style=color:#f92672>.</span>hparams, <span style=color:#e6db74>&#34;sample_rate&#34;</span>) <span style=color:#66d9ef>else</span> sample_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> sample_rate <span style=color:#f92672>!=</span> target_sr:
</span></span><span style=display:flex><span>        resampler <span style=color:#f92672>=</span> T<span style=color:#f92672>.</span>Resample(orig_freq<span style=color:#f92672>=</span>sample_rate, new_freq<span style=color:#f92672>=</span>target_sr)
</span></span><span style=display:flex><span>        waveform <span style=color:#f92672>=</span> resampler(waveform)
</span></span><span style=display:flex><span>        sample_rate <span style=color:#f92672>=</span> target_sr
</span></span><span style=display:flex><span>        tmp_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;uploads/temp_resampled.wav&#34;</span>
</span></span><span style=display:flex><span>        torchaudio<span style=color:#f92672>.</span>save(tmp_path, waveform, sample_rate)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        tmp_path <span style=color:#f92672>=</span> file_path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    est_sources <span style=color:#f92672>=</span> sep_model<span style=color:#f92672>.</span>separate_file(path<span style=color:#f92672>=</span>tmp_path)
</span></span><span style=display:flex><span>    num_speakers <span style=color:#f92672>=</span> est_sources<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    results <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(num_speakers):
</span></span><span style=display:flex><span>        speaker_wav <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;separated/</span><span style=color:#e6db74>{</span>file<span style=color:#f92672>.</span>filename<span style=color:#e6db74>}</span><span style=color:#e6db74>_speaker_</span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>.wav&#34;</span>
</span></span><span style=display:flex><span>        torchaudio<span style=color:#f92672>.</span>save(speaker_wav, est_sources[:, :, i]<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu(), sample_rate)
</span></span><span style=display:flex><span>        transcription <span style=color:#f92672>=</span> asr_model<span style=color:#f92672>.</span>transcribe_file(speaker_wav)
</span></span><span style=display:flex><span>        results<span style=color:#f92672>.</span>append({
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;speaker&#34;</span>: i <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;transcript&#34;</span>: transcription
</span></span><span style=display:flex><span>        })
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> tmp_path <span style=color:#f92672>!=</span> file_path:
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>remove(tmp_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> JSONResponse(content<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;num_speakers&#34;</span>: num_speakers, <span style=color:#e6db74>&#34;results&#34;</span>: results})
</span></span></code></pre></div><p>static/index.html</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-html data-lang=html><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>&lt;!DOCTYPE html&gt;</span>
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>html</span> <span style=color:#a6e22e>lang</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;zh-CN&#34;</span>&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>head</span>&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>meta</span> <span style=color:#a6e22e>charset</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;UTF-8&#34;</span> /&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>title</span>&gt;多说话人分离及语音识别&lt;/<span style=color:#f92672>title</span>&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>style</span>&gt;
</span></span><span style=display:flex><span><span style=color:#f92672>body</span> { <span style=color:#66d9ef>font-family</span>: Arial, <span style=color:#66d9ef>sans-serif</span>; <span style=color:#66d9ef>margin</span>: <span style=color:#ae81ff>2</span><span style=color:#66d9ef>rem</span>; }
</span></span><span style=display:flex><span><span style=color:#f92672>button</span> { <span style=color:#66d9ef>padding</span>: <span style=color:#ae81ff>0.5</span><span style=color:#66d9ef>rem</span> <span style=color:#ae81ff>1</span><span style=color:#66d9ef>rem</span>; <span style=color:#66d9ef>margin-top</span>: <span style=color:#ae81ff>0.5</span><span style=color:#66d9ef>rem</span>; }
</span></span><span style=display:flex><span>#results { <span style=color:#66d9ef>margin-top</span>: <span style=color:#ae81ff>1</span><span style=color:#66d9ef>rem</span>; <span style=color:#66d9ef>white-space</span>: <span style=color:#66d9ef>pre-wrap</span>; }
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>style</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>head</span>&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>body</span>&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>h2</span>&gt;上传混合音频，自动分离并转写&lt;/<span style=color:#f92672>h2</span>&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>input</span> <span style=color:#a6e22e>type</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;file&#34;</span> <span style=color:#a6e22e>id</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;audioFile&#34;</span> <span style=color:#a6e22e>accept</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;.wav,.mp3&#34;</span> /&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>br</span> /&gt;
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>button</span> <span style=color:#a6e22e>onclick</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;upload()&#34;</span>&gt;上传识别&lt;/<span style=color:#f92672>button</span>&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>div</span> <span style=color:#a6e22e>id</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;results&#34;</span>&gt;&lt;/<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>script</span>&gt;
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>upload</span>() {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>fileInput</span> <span style=color:#f92672>=</span> document.<span style=color:#a6e22e>getElementById</span>(<span style=color:#e6db74>&#34;audioFile&#34;</span>);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>fileInput</span>.<span style=color:#a6e22e>files</span>.<span style=color:#a6e22e>length</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>alert</span>(<span style=color:#e6db74>&#34;请选择音频文件！&#34;</span>);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>file</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>fileInput</span>.<span style=color:#a6e22e>files</span>[<span style=color:#ae81ff>0</span>];
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>formData</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>FormData</span>();
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>formData</span>.<span style=color:#a6e22e>append</span>(<span style=color:#e6db74>&#34;file&#34;</span>, <span style=color:#a6e22e>file</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>resultsDiv</span> <span style=color:#f92672>=</span> document.<span style=color:#a6e22e>getElementById</span>(<span style=color:#e6db74>&#34;results&#34;</span>);
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>resultsDiv</span>.<span style=color:#a6e22e>textContent</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;识别中，请稍候...&#34;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>fetch</span>(<span style=color:#e6db74>&#34;/upload/&#34;</span>, {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>method</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;POST&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>body</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>formData</span>
</span></span><span style=display:flex><span>        });
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>ok</span>) <span style=color:#66d9ef>throw</span> <span style=color:#66d9ef>new</span> Error(<span style=color:#e6db74>&#34;上传失败&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>data</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>json</span>();
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#a6e22e>text</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>`检测到说话人数：</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>num_speakers</span><span style=color:#e6db74>}</span><span style=color:#e6db74>\n\n`</span>;
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>results</span>.<span style=color:#a6e22e>forEach</span>(<span style=color:#a6e22e>r</span> =&gt; {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>text</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`说话人 </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>r</span>.<span style=color:#a6e22e>speaker</span><span style=color:#e6db74>}</span><span style=color:#e6db74>:\n</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>r</span>.<span style=color:#a6e22e>transcript</span><span style=color:#e6db74>}</span><span style=color:#e6db74>\n\n`</span>;
</span></span><span style=display:flex><span>        });
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>resultsDiv</span>.<span style=color:#a6e22e>textContent</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>text</span>;
</span></span><span style=display:flex><span>    } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>err</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>resultsDiv</span>.<span style=color:#a6e22e>textContent</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;错误：&#34;</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>err</span>.<span style=color:#a6e22e>message</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>script</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>body</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>html</span>&gt;
</span></span></code></pre></div><p>requirements.txt</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>fastapi
</span></span><span style=display:flex><span>uvicorn
</span></span><span style=display:flex><span>speechbrain
</span></span><span style=display:flex><span>torchaudio
</span></span><span style=display:flex><span>python-multipart
</span></span></code></pre></div><p>Dockerfile</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-dockerfile data-lang=dockerfile><span style=display:flex><span><span style=color:#75715e># 基础镜像：官方 PyTorch 镜像（带 CUDA 也可以）</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>FROM</span><span style=color:#e6db74> pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 设置工作目录</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>WORKDIR</span><span style=color:#e6db74> /app</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 复制依赖文件和程序</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> requirements.txt .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> app.py .<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>COPY</span> static ./static<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 安装依赖</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> pip install --upgrade pip<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>RUN</span> pip install -r requirements.txt<span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 运行端口</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>EXPOSE</span><span style=color:#e6db74> 8000</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># 启动命令</span><span style=color:#960050;background-color:#1e0010>
</span></span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010></span><span style=color:#66d9ef>CMD</span> [<span style=color:#e6db74>&#34;uvicorn&#34;</span>, <span style=color:#e6db74>&#34;app:app&#34;</span>, <span style=color:#e6db74>&#34;--host&#34;</span>, <span style=color:#e6db74>&#34;0.0.0.0&#34;</span>, <span style=color:#e6db74>&#34;--port&#34;</span>, <span style=color:#e6db74>&#34;8000&#34;</span>]<span style=color:#960050;background-color:#1e0010>
</span></span></span></code></pre></div><h2 id=启动与调试>启动与调试<a hidden class=anchor aria-hidden=true href=#启动与调试>#</a></h2><p>本地调试：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install -r requirements.txt
</span></span><span style=display:flex><span>uvicorn app:app --reload --host 0.0.0.0 --port <span style=color:#ae81ff>8000</span>
</span></span></code></pre></div><p>浏览器打开 http://localhost:8000 上传音频测试。</p><p>Docker 构建启动：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>docker build -t speechbrain-stt .
</span></span><span style=display:flex><span>docker run -p 8000:8000 speechbrain-stt
</span></span></code></pre></div><p>访问 http://localhost:8000</p><h2 id=扩展>扩展<a hidden class=anchor aria-hidden=true href=#扩展>#</a></h2><p>前端做音频播放分离音频文件功能（接口返回的 wav 文件路径）</p><p>后端改用异步队列（如 Celery + Redis）处理分离和识别，提升吞吐</p><p>支持多种音频格式上传，后台自动转换成 wav</p><p>部署时配置 HTTPS，防止数据明文传输</p><p>加入身份验证保护接口，防止滥用</p><h1 id=中文识别模型>中文识别模型<a hidden class=anchor aria-hidden=true href=#中文识别模型>#</a></h1><h2 id=asr-wav2vec2-commonvoice-14-zh-cn>asr-wav2vec2-commonvoice-14-zh-CN<a hidden class=anchor aria-hidden=true href=#asr-wav2vec2-commonvoice-14-zh-cn>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> speechbrain.inference.ASR <span style=color:#f92672>import</span> EncoderASR
</span></span><span style=display:flex><span>m <span style=color:#f92672>=</span> EncoderASR<span style=color:#f92672>.</span>from_hparams(source<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;speechbrain/asr-wav2vec2-commonvoice-14-zh-CN&#34;</span>, savedir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;tmp&#34;</span>)
</span></span><span style=display:flex><span>print(m<span style=color:#f92672>.</span>transcribe_file(<span style=color:#e6db74>&#34;separated/speaker_1.wav&#34;</span>))
</span></span></code></pre></div><h2 id=whisper>Whisper<a hidden class=anchor aria-hidden=true href=#whisper>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> faster_whisper <span style=color:#f92672>import</span> WhisperModel
</span></span><span style=display:flex><span>m <span style=color:#f92672>=</span> WhisperModel(<span style=color:#e6db74>&#34;small&#34;</span>, device<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>)
</span></span><span style=display:flex><span>segments, info <span style=color:#f92672>=</span> m<span style=color:#f92672>.</span>transcribe(<span style=color:#e6db74>&#34;separated/xxx_speaker_1.wav&#34;</span>, beam_size<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, language<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;zh&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join([s<span style=color:#f92672>.</span>text <span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> segments]))
</span></span></code></pre></div><p>结果：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>ctranslate2<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>thread 460025<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>warning<span style=color:#f92672>]</span> The compute type inferred from the saved model is float16, but the target device or backend <span style=color:#66d9ef>do</span> not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.
</span></span><span style=display:flex><span>哈喽哈喽李华李华
</span></span></code></pre></div><h1 id=speechbrain---tts>SpeechBrain & TTS<a hidden class=anchor aria-hidden=true href=#speechbrain---tts>#</a></h1><p>SpeechBrain 分离模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- coding: utf-8 -*-</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sherpa_onnx
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sounddevice <span style=color:#66d9ef>as</span> sd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> threading
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> queue
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> defaultdict
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> soundfile <span style=color:#66d9ef>as</span> sf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Dict, List, Optional, Tuple
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> resampy  <span style=color:#75715e># 用于重采样</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ========== 新增：SpeechBrain 分离相关 ==========</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> speechbrain.inference <span style=color:#f92672>import</span> SepformerSeparation <span style=color:#66d9ef>as</span> separator
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchaudio.transforms <span style=color:#66d9ef>as</span> T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 路径工具 ------------------</span>
</span></span><span style=display:flex><span>current_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>abspath(__file__))
</span></span><span style=display:flex><span>parent_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(current_dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_relative_path</span>(relative_path: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(parent_dir, relative_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 加载关键词配置 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_keywords</span>() <span style=color:#f92672>-&gt;</span> Dict[str, int]:
</span></span><span style=display:flex><span>    keywords_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/keywords.json&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(keywords_file):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>FileNotFoundError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;未找到关键词文件: </span><span style=color:#e6db74>{</span>keywords_file<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(keywords_file, <span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> json<span style=color:#f92672>.</span>load(f)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> load_keywords()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str) <span style=color:#f92672>-&gt;</span> Tuple[Optional[str], int]:
</span></span><span style=display:flex><span>    txt_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kid <span style=color:#f92672>in</span> KEYWORD2ID<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> fuzz<span style=color:#f92672>.</span>partial_ratio(<span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)), txt_py) <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>70</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> kw, kid
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 加载 system prompt ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_system_prompt</span>() <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    prompt_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/system_prompt.txt&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(prompt_file):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>FileNotFoundError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;未找到 system prompt 文件: </span><span style=color:#e6db74>{</span>prompt_file<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(prompt_file, <span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> f<span style=color:#f92672>.</span>read()<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>SYSTEM_PROMPT <span style=color:#f92672>=</span> load_system_prompt()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 大模型初始化 ------------------</span>
</span></span><span style=display:flex><span>QWEN_PATH <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/models/qwen2.5-0.5b&#34;</span>)
</span></span><span style=display:flex><span>tok <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(QWEN_PATH, trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chat_model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Qwen/Qwen2.5-0.5B&#34;</span>,
</span></span><span style=display:flex><span>    cache_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/root/room/models&#34;</span>,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>qwen_chat</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    msgs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: SYSTEM_PROMPT},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tok(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> chat_model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>            <span style=color:#f92672>**</span>inputs,
</span></span><span style=display:flex><span>            max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>            do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>,
</span></span><span style=display:flex><span>            pad_token_id<span style=color:#f92672>=</span>tok<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    raw_output <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>decode(out[<span style=color:#ae81ff>0</span>][len(inputs<span style=color:#f92672>.</span>input_ids[<span style=color:#ae81ff>0</span>]):], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;&lt;/s&gt;&#34;</span>, <span style=color:#e6db74>&#34;&lt;/user&gt;&#34;</span>, <span style=color:#e6db74>&#34;.user&#34;</span>, <span style=color:#e6db74>&#34;🤨&#34;</span>, <span style=color:#e6db74>&#34;## 128000&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\x0c</span><span style=color:#e6db74>&#34;</span>]:
</span></span><span style=display:flex><span>        raw_output <span style=color:#f92672>=</span> raw_output<span style=color:#f92672>.</span>replace(token, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>    reply <span style=color:#f92672>=</span> raw_output<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> reply
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 声纹相关 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_speaker_embedding_model</span>(model_path: str):
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingExtractorConfig(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model_path,
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> config<span style=color:#f92672>.</span>validate():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Invalid config. </span><span style=color:#e6db74>{</span>config<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingExtractor(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_speaker_file</span>(speaker_file_path: str) <span style=color:#f92672>-&gt;</span> Dict[str, List[str]]:
</span></span><span style=display:flex><span>    ans <span style=color:#f92672>=</span> defaultdict(list)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(speaker_file_path) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> f:
</span></span><span style=display:flex><span>            line <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> line:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            fields <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(fields) <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Invalid line: </span><span style=color:#e6db74>{</span>line<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            speaker_name, filename <span style=color:#f92672>=</span> fields
</span></span><span style=display:flex><span>            ans[speaker_name]<span style=color:#f92672>.</span>append(filename)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ans
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_speaker_embedding</span>(filenames: List[str], extractor) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>ndarray:
</span></span><span style=display:flex><span>    ans <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> filename <span style=color:#f92672>in</span> filenames:
</span></span><span style=display:flex><span>        samples, sr <span style=color:#f92672>=</span> sf<span style=color:#f92672>.</span>read(filename, always_2d<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        samples <span style=color:#f92672>=</span> samples[:, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        samples <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ascontiguousarray(samples)
</span></span><span style=display:flex><span>        stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>        stream<span style=color:#f92672>.</span>accept_waveform(sr, samples)
</span></span><span style=display:flex><span>        stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>        embedding <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(stream))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ans <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            ans <span style=color:#f92672>=</span> embedding
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            ans <span style=color:#f92672>+=</span> embedding
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ans <span style=color:#f92672>/</span> len(filenames)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_speaker_identification</span>():
</span></span><span style=display:flex><span>    speaker_model <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/wespeaker_zh_cnceleb_resnet34.onnx&#34;</span>)
</span></span><span style=display:flex><span>    speaker_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/speaker.txt&#34;</span>)
</span></span><span style=display:flex><span>    extractor <span style=color:#f92672>=</span> load_speaker_embedding_model(speaker_model)
</span></span><span style=display:flex><span>    speaker_data <span style=color:#f92672>=</span> load_speaker_file(speaker_file)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    manager <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingManager(extractor<span style=color:#f92672>.</span>dim)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> name, filelist <span style=color:#f92672>in</span> speaker_data<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        emb <span style=color:#f92672>=</span> compute_speaker_embedding(filelist, extractor)
</span></span><span style=display:flex><span>        status <span style=color:#f92672>=</span> manager<span style=color:#f92672>.</span>add(name, emb)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> status:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>RuntimeError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Failed to register speaker </span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> extractor, manager
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wake_word_detection_with_speaker</span>(input_device: int, input_sr: int, extractor, speaker_manager) <span style=color:#f92672>-&gt;</span> Optional[str]:
</span></span><span style=display:flex><span>    kws_dir <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01&#34;</span>)
</span></span><span style=display:flex><span>    keyword_spotter <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>KeywordSpotter(
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        encoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        decoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        joiner<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        max_active_paths<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        keywords_file<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;keywords.txt&#34;</span>),
</span></span><span style=display:flex><span>        keywords_score<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>        keywords_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.25</span>,
</span></span><span style=display:flex><span>        num_trailing_blanks<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;等待唤醒词（需声纹匹配）...&#34;</span>)
</span></span><span style=display:flex><span>    stream <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    buffer_seconds <span style=color:#f92672>=</span> <span style=color:#ae81ff>2.0</span>  <span style=color:#75715e># 缓存唤醒词语音用于声纹识别</span>
</span></span><span style=display:flex><span>    max_buffer_len <span style=color:#f92672>=</span> int(buffer_seconds <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>    audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>0</span>,), dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            stream<span style=color:#f92672>.</span>accept_waveform(input_sr, samples)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 语音缓存</span>
</span></span><span style=display:flex><span>            audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate((audio_buffer, samples))
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(audio_buffer) <span style=color:#f92672>&gt;</span> max_buffer_len:
</span></span><span style=display:flex><span>                audio_buffer <span style=color:#f92672>=</span> audio_buffer[<span style=color:#f92672>-</span>max_buffer_len:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> keyword_spotter<span style=color:#f92672>.</span>is_ready(stream):
</span></span><span style=display:flex><span>                keyword_spotter<span style=color:#f92672>.</span>decode_stream(stream)
</span></span><span style=display:flex><span>                result <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>get_result(stream)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> result:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;检测到唤醒词: </span><span style=color:#e6db74>{</span>result<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                    keyword_spotter<span style=color:#f92672>.</span>reset_stream(stream)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#75715e># 进行声纹识别</span>
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;正在进行声纹识别验证...&#34;</span>)
</span></span><span style=display:flex><span>                    audio <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ascontiguousarray(audio_buffer)
</span></span><span style=display:flex><span>                    emb_stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                    emb_stream<span style=color:#f92672>.</span>accept_waveform(input_sr, audio)
</span></span><span style=display:flex><span>                    emb_stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                    emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(emb_stream))
</span></span><span style=display:flex><span>                    speaker <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> speaker:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;声纹识别成功，说话人: </span><span style=color:#e6db74>{</span>speaker<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>return</span> speaker
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>&#34;声纹识别失败，忽略本次唤醒&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 唤醒词（不带声纹，仅示例备用） ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wake_word_detection</span>(input_device: int, input_sr: int) <span style=color:#f92672>-&gt;</span> bool:
</span></span><span style=display:flex><span>    kws_dir <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01&#34;</span>)
</span></span><span style=display:flex><span>    keyword_spotter <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>KeywordSpotter(
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        encoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        decoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        joiner<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        max_active_paths<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        keywords_file<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;keywords.txt&#34;</span>),
</span></span><span style=display:flex><span>        keywords_score<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>        keywords_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.25</span>,
</span></span><span style=display:flex><span>        num_trailing_blanks<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;等待唤醒词...&#34;</span>)
</span></span><span style=display:flex><span>    stream <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            stream<span style=color:#f92672>.</span>accept_waveform(input_sr, samples)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> keyword_spotter<span style=color:#f92672>.</span>is_ready(stream):
</span></span><span style=display:flex><span>                keyword_spotter<span style=color:#f92672>.</span>decode_stream(stream)
</span></span><span style=display:flex><span>                result <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>get_result(stream)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> result:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;检测到唤醒词: </span><span style=color:#e6db74>{</span>result<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                    keyword_spotter<span style=color:#f92672>.</span>reset_stream(stream)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 语音识别（保留原有函数可按需使用） ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>speech_recognition</span>(extractor, speaker_manager, input_device: int, input_sr: int) <span style=color:#f92672>-&gt;</span> Tuple[str, str]:
</span></span><span style=display:flex><span>    target_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>16000</span>  <span style=color:#75715e># VAD/ASR 采样率</span>
</span></span><span style=display:flex><span>    recognizer <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineRecognizer<span style=color:#f92672>.</span>from_paraformer(
</span></span><span style=display:flex><span>        paraformer<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        sample_rate<span style=color:#f92672>=</span>target_sr,
</span></span><span style=display:flex><span>        feature_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>80</span>,
</span></span><span style=display:flex><span>        decoding_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;greedy_search&#34;</span>,
</span></span><span style=display:flex><span>        debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    vad_config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>VadModelConfig()
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/silero_vad.onnx&#34;</span>)
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>min_silence_duration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>min_speech_duration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>sample_rate <span style=color:#f92672>=</span> target_sr
</span></span><span style=display:flex><span>    vad <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>VoiceActivityDetector(vad_config, buffer_size_in_seconds<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    window_size <span style=color:#f92672>=</span> vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>window_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;请说出您的指令...&#34;</span>)
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>    audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(samples, input_sr, target_sr)
</span></span><span style=display:flex><span>            audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate([audio_buffer, resampled])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> len(audio_buffer) <span style=color:#f92672>&gt;</span> window_size:
</span></span><span style=display:flex><span>                vad<span style=color:#f92672>.</span>accept_waveform(audio_buffer[:window_size])
</span></span><span style=display:flex><span>                audio_buffer <span style=color:#f92672>=</span> audio_buffer[window_size:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> vad<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> len(vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> target_sr:
</span></span><span style=display:flex><span>                    vad<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                stream<span style=color:#f92672>.</span>accept_waveform(target_sr, vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples)
</span></span><span style=display:flex><span>                stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(stream))
</span></span><span style=display:flex><span>                speaker <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>) <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>                asr_stream <span style=color:#f92672>=</span> recognizer<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                asr_stream<span style=color:#f92672>.</span>accept_waveform(target_sr, vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples)
</span></span><span style=display:flex><span>                recognizer<span style=color:#f92672>.</span>decode_stream(asr_stream)
</span></span><span style=display:flex><span>                text <span style=color:#f92672>=</span> asr_stream<span style=color:#f92672>.</span>result<span style=color:#f92672>.</span>text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                vad<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> text, speaker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ TTS ------------------</span>
</span></span><span style=display:flex><span>buffer <span style=color:#f92672>=</span> queue<span style=color:#f92672>.</span>Queue()
</span></span><span style=display:flex><span>started <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>killed <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>event <span style=color:#f92672>=</span> threading<span style=color:#f92672>.</span>Event()
</span></span><span style=display:flex><span>tts_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>24000</span>
</span></span><span style=display:flex><span>output_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>48000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generated_audio_callback</span>(samples: np<span style=color:#f92672>.</span>ndarray, progress: float) <span style=color:#f92672>-&gt;</span> int:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> started
</span></span><span style=display:flex><span>    resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(samples, tts_sr, output_sr)
</span></span><span style=display:flex><span>    buffer<span style=color:#f92672>.</span>put(resampled)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> started:
</span></span><span style=display:flex><span>        started <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> killed <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>play_audio_callback</span>(outdata: np<span style=color:#f92672>.</span>ndarray, frames: int, time, status):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> killed <span style=color:#f92672>or</span> (started <span style=color:#f92672>and</span> buffer<span style=color:#f92672>.</span>empty() <span style=color:#f92672>and</span> stopped):
</span></span><span style=display:flex><span>        event<span style=color:#f92672>.</span>set()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> buffer<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>        outdata<span style=color:#f92672>.</span>fill(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> n <span style=color:#f92672>&lt;</span> frames <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> buffer<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>        remaining <span style=color:#f92672>=</span> frames <span style=color:#f92672>-</span> n
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> remaining <span style=color:#f92672>&lt;=</span> k:
</span></span><span style=display:flex><span>            outdata[n:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>][:remaining]
</span></span><span style=display:flex><span>            buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>][remaining:]
</span></span><span style=display:flex><span>            n <span style=color:#f92672>=</span> frames
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                buffer<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        outdata[n:n <span style=color:#f92672>+</span> k, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>        n <span style=color:#f92672>+=</span> k
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> n <span style=color:#f92672>&lt;</span> frames:
</span></span><span style=display:flex><span>        outdata[n:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tts_synthesis</span>(text: str, output_device: int):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> tts_sr, started, stopped
</span></span><span style=display:flex><span>    tts_config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsConfig(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsModelConfig(
</span></span><span style=display:flex><span>            vits<span style=color:#f92672>=</span>sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsVitsModelConfig(
</span></span><span style=display:flex><span>                model<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/zh_CN-huayan-medium.onnx&#34;</span>),
</span></span><span style=display:flex><span>                tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>                data_dir<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/espeak-ng-data&#34;</span>),
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>            debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        max_num_sentences<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    tts <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineTts(tts_config)
</span></span><span style=display:flex><span>    tts_sr <span style=color:#f92672>=</span> tts<span style=color:#f92672>.</span>sample_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    threading<span style=color:#f92672>.</span>Thread(target<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span>: sd<span style=color:#f92672>.</span>OutputStream(
</span></span><span style=display:flex><span>        device<span style=color:#f92672>=</span>output_device,
</span></span><span style=display:flex><span>        channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        callback<span style=color:#f92672>=</span>play_audio_callback,
</span></span><span style=display:flex><span>        dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>,
</span></span><span style=display:flex><span>        samplerate<span style=color:#f92672>=</span>output_sr,
</span></span><span style=display:flex><span>        blocksize<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>    )<span style=color:#f92672>.</span>start())<span style=color:#f92672>.</span>start()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    event<span style=color:#f92672>.</span>clear()
</span></span><span style=display:flex><span>    started <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    tts<span style=color:#f92672>.</span>generate(text, sid<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, speed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, callback<span style=color:#f92672>=</span>generated_audio_callback)
</span></span><span style=display:flex><span>    stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    event<span style=color:#f92672>.</span>wait()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ SpeechBrain 分离：初始化与工具函数 ------------------</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;初始化 SpeechBrain 分离模型（Sepformer）... 这可能需要下载模型到 pretrained_models/sepformer-wsj02mix&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    sep_model <span style=color:#f92672>=</span> separator<span style=color:#f92672>.</span>from_hparams(
</span></span><span style=display:flex><span>        source<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;speechbrain/sepformer-wsj02mix&#34;</span>,
</span></span><span style=display:flex><span>        savedir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pretrained_models/sepformer-wsj02mix&#34;</span>,
</span></span><span style=display:flex><span>        run_opts<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;device&#34;</span>: <span style=color:#e6db74>&#34;cpu&#34;</span>},
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    SEP_SR <span style=color:#f92672>=</span> getattr(sep_model<span style=color:#f92672>.</span>hparams, <span style=color:#e6db74>&#34;sample_rate&#34;</span>, <span style=color:#ae81ff>8000</span>)  <span style=color:#75715e># 通常 sepformer 用 8k</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Sepformer 采样率: </span><span style=color:#e6db74>{</span>SEP_SR<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;加载 Sepformer 模型失败:&#34;</span>, e)
</span></span><span style=display:flex><span>    sep_model <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    SEP_SR <span style=color:#f92672>=</span> <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 我们的 ASR / 声纹 统一用 16k（与 原逻辑一致）</span>
</span></span><span style=display:flex><span>ASR_SR <span style=color:#f92672>=</span> <span style=color:#ae81ff>16000</span>
</span></span><span style=display:flex><span>MIN_AUDIO_SECONDS <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.3</span>  <span style=color:#75715e># 跳过太短片段（秒）</span>
</span></span><span style=display:flex><span>MIN_AUDIO_SAMPLES <span style=color:#f92672>=</span> int(MIN_AUDIO_SECONDS <span style=color:#f92672>*</span> ASR_SR)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_extract_sources_from_sep_output</span>(est_np: np<span style=color:#f92672>.</span>ndarray, max_sources<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>) <span style=color:#f92672>-&gt;</span> List[np<span style=color:#f92672>.</span>ndarray]:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    将 sep_model 输出 ndarray 转为 [n_src, time] 的 list（numpy 1D float32）。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    尽量自动识别哪个轴是源、哪个轴是时间。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> est_np <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> est_np<span style=color:#f92672>.</span>ndim <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 先尝试找到较小的轴（&gt;1 且 &lt;= max_sources）作为 source 轴</span>
</span></span><span style=display:flex><span>    src_axis <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    time_axis <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, s <span style=color:#f92672>in</span> enumerate(est_np<span style=color:#f92672>.</span>shape):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>&lt;</span> s <span style=color:#f92672>&lt;=</span> max_sources:
</span></span><span style=display:flex><span>            src_axis <span style=color:#f92672>=</span> i
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1000</span>:
</span></span><span style=display:flex><span>            time_axis <span style=color:#f92672>=</span> i
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> src_axis <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 退化策略：2D 情况下，较小维度当作 src</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> est_np<span style=color:#f92672>.</span>ndim <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> est_np<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>&lt;</span> est_np<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:
</span></span><span style=display:flex><span>                src_axis, time_axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                src_axis, time_axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> est_np<span style=color:#f92672>.</span>ndim <span style=color:#f92672>==</span> <span style=color:#ae81ff>3</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 常见： (1, n_src, time) 或 (channels, time, n_src)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 优先选 size 小的轴作为 src</span>
</span></span><span style=display:flex><span>            sizes <span style=color:#f92672>=</span> list(est_np<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>            min_idx <span style=color:#f92672>=</span> int(np<span style=color:#f92672>.</span>argmin(sizes))
</span></span><span style=display:flex><span>            <span style=color:#75715e># 保证不是 time axis</span>
</span></span><span style=display:flex><span>            src_axis <span style=color:#f92672>=</span> min_idx
</span></span><span style=display:flex><span>            <span style=color:#75715e># time axis 取最大轴</span>
</span></span><span style=display:flex><span>            time_axis <span style=color:#f92672>=</span> int(np<span style=color:#f92672>.</span>argmax(sizes))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 最保守策略</span>
</span></span><span style=display:flex><span>            src_axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>            time_axis <span style=color:#f92672>=</span> est_np<span style=color:#f92672>.</span>ndim <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> time_axis <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        time_axis <span style=color:#f92672>=</span> max(range(est_np<span style=color:#f92672>.</span>ndim), key<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> i: est_np<span style=color:#f92672>.</span>shape[i] <span style=color:#66d9ef>if</span> i <span style=color:#f92672>!=</span> src_axis <span style=color:#66d9ef>else</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 将 array 重新排列为 (src, time, ...)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 我们把 time 移到 axis=1，src 移到 axis=0</span>
</span></span><span style=display:flex><span>    arr <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>moveaxis(est_np, (src_axis, time_axis), (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#75715e># 现在 arr.shape[0] = n_src, arr.shape[1] = time</span>
</span></span><span style=display:flex><span>    n_src <span style=color:#f92672>=</span> arr<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    sources <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_src):
</span></span><span style=display:flex><span>        s <span style=color:#f92672>=</span> arr[i]
</span></span><span style=display:flex><span>        <span style=color:#75715e># 若还有额外轴（如 channels），把它们 flatten 或取第一通道</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s<span style=color:#f92672>.</span>ndim <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 把多通道求均值为单通道</span>
</span></span><span style=display:flex><span>            s <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(s, axis<span style=color:#f92672>=</span>tuple(range(<span style=color:#ae81ff>1</span>, s<span style=color:#f92672>.</span>ndim)))
</span></span><span style=display:flex><span>        sources<span style=color:#f92672>.</span>append(s<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sources
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>separate_speakers</span>(mixed_audio: np<span style=color:#f92672>.</span>ndarray, input_sr: int) <span style=color:#f92672>-&gt;</span> List[np<span style=color:#f92672>.</span>ndarray]:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    返回 list，每项为以 SEP_SR 为采样率的 numpy 1D float32 信号（单通道）。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    （后续处理会把它重采样为 ASR_SR）
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> sep_model <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Sepformer 未初始化，跳过分离，直接返回原始音频&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> [mixed_audio]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 准备模型输入：sep_model 要求的形状可能是 [batch, time] 或 [batch, 1, time] 等</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 我们统一提供 [1, time]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> input_sr <span style=color:#f92672>!=</span> SEP_SR:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 用 resampy 将输入混音重采样到 sep 模型采样率</span>
</span></span><span style=display:flex><span>        mixed_audio_sep <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(mixed_audio, input_sr, SEP_SR)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        mixed_audio_sep <span style=color:#f92672>=</span> mixed_audio<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    audio_tensor <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(mixed_audio_sep)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)  <span style=color:#75715e># [1, time]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        est <span style=color:#f92672>=</span> sep_model<span style=color:#f92672>.</span>separate_batch(audio_tensor)  <span style=color:#75715e># 返回 tensor，shape 可能多样</span>
</span></span><span style=display:flex><span>    est_np <span style=color:#f92672>=</span> est<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>    sources <span style=color:#f92672>=</span> _extract_sources_from_sep_output(est_np, max_sources<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 过滤极短或静音的源（避免后续造成 extractor 报错）</span>
</span></span><span style=display:flex><span>    filtered <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, s <span style=color:#f92672>in</span> enumerate(sources):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span> <span style=color:#f92672>or</span> s<span style=color:#f92672>.</span>size <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        power <span style=color:#f92672>=</span> float(np<span style=color:#f92672>.</span>mean(np<span style=color:#f92672>.</span>abs(s)))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>10</span>:  <span style=color:#75715e># 非常短</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 过滤非常静的（阈值可根据实际调整）</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> power <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1e-5</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        filtered<span style=color:#f92672>.</span>append(s)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> filtered
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 主流程相关（设备选择 / 录音） ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>select_devices</span>():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;=== 可用输入输出设备列表 ===&#34;</span>)
</span></span><span style=display:flex><span>    print(sd<span style=color:#f92672>.</span>query_devices())
</span></span><span style=display:flex><span>    input_device <span style=color:#f92672>=</span> int(input(<span style=color:#e6db74>&#34;请输入输入设备ID: &#34;</span>))
</span></span><span style=display:flex><span>    output_device <span style=color:#f92672>=</span> int(input(<span style=color:#e6db74>&#34;请输入输出设备ID: &#34;</span>))
</span></span><span style=display:flex><span>    input_info <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>query_devices(input_device)
</span></span><span style=display:flex><span>    output_info <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>query_devices(output_device)
</span></span><span style=display:flex><span>    input_sr <span style=color:#f92672>=</span> int(input_info[<span style=color:#e6db74>&#39;default_samplerate&#39;</span>])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;选择输入采样率: </span><span style=color:#e6db74>{</span>input_sr<span style=color:#e6db74>}</span><span style=color:#e6db74> Hz&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> input_device, output_device, input_sr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>record_mixed_audio</span>(input_device: int, input_sr: int, duration: float <span style=color:#f92672>=</span> <span style=color:#ae81ff>5.0</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;从麦克风录制一段混合音频（单声道 float32）&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;开始录音 </span><span style=color:#e6db74>{</span>duration<span style=color:#e6db74>}</span><span style=color:#e6db74> 秒...&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        frames <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        total_chunks <span style=color:#f92672>=</span> int(duration <span style=color:#f92672>/</span> <span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(total_chunks):
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr))
</span></span><span style=display:flex><span>            frames<span style=color:#f92672>.</span>append(samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        mixed_audio <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate(frames)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;录音完成&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mixed_audio
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 主流程 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> killed
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;初始化声纹识别...&#34;</span>)
</span></span><span style=display:flex><span>        extractor, speaker_manager <span style=color:#f92672>=</span> init_speaker_identification()
</span></span><span style=display:flex><span>        input_device, output_device, input_sr <span style=color:#f92672>=</span> select_devices()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 预创建 ASR recognizer（避免在每个说话人上重复创建）</span>
</span></span><span style=display:flex><span>        recognizer <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineRecognizer<span style=color:#f92672>.</span>from_paraformer(
</span></span><span style=display:flex><span>            paraformer<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>            tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>            num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            sample_rate<span style=color:#f92672>=</span>ASR_SR,
</span></span><span style=display:flex><span>            feature_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>80</span>,
</span></span><span style=display:flex><span>            decoding_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;greedy_search&#34;</span>,
</span></span><span style=display:flex><span>            debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        speaker <span style=color:#f92672>=</span> wake_word_detection_with_speaker(input_device, input_sr, extractor, speaker_manager)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> speaker:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 1) 录制混合音频</span>
</span></span><span style=display:flex><span>            mixed_audio <span style=color:#f92672>=</span> record_mixed_audio(input_device, input_sr, duration<span style=color:#f92672>=</span><span style=color:#ae81ff>5.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 2) 分离</span>
</span></span><span style=display:flex><span>            separated <span style=color:#f92672>=</span> separate_speakers(mixed_audio, input_sr)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;分离后有效说话人数量: </span><span style=color:#e6db74>{</span>len(separated)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(separated) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;未检测到有效分离结果，回退为直接识别整段语音&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#75715e># 回退：把混合音频重采样到 ASR_SR 并识别</span>
</span></span><span style=display:flex><span>                mixed_resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(mixed_audio, input_sr, ASR_SR)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>                <span style=color:#75715e># 声纹与识别（与单路流程相同）</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> mixed_resampled<span style=color:#f92672>.</span>size <span style=color:#f92672>&gt;=</span> MIN_AUDIO_SAMPLES:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                        emb_stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                        emb_stream<span style=color:#f92672>.</span>accept_waveform(ASR_SR, mixed_resampled)
</span></span><span style=display:flex><span>                        emb_stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                        emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(emb_stream))
</span></span><span style=display:flex><span>                        spk_name <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>) <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>&#34;声纹识别失败:&#34;</span>, e)
</span></span><span style=display:flex><span>                        spk_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                        asr_stream <span style=color:#f92672>=</span> recognizer<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                        asr_stream<span style=color:#f92672>.</span>accept_waveform(ASR_SR, mixed_resampled)
</span></span><span style=display:flex><span>                        recognizer<span style=color:#f92672>.</span>decode_stream(asr_stream)
</span></span><span style=display:flex><span>                        text <span style=color:#f92672>=</span> asr_stream<span style=color:#f92672>.</span>result<span style=color:#f92672>.</span>text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>&#34;ASR 失败:&#34;</span>, e)
</span></span><span style=display:flex><span>                        text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;识别结果: </span><span style=color:#e6db74>{</span>text<span style=color:#e6db74>}</span><span style=color:#e6db74> (说话人: </span><span style=color:#e6db74>{</span>spk_name<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>                    keyword, kid <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> keyword:
</span></span><span style=display:flex><span>                        reply <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;已为您操作【</span><span style=color:#e6db74>{</span>keyword<span style=color:#e6db74>}</span><span style=color:#e6db74>】，编号</span><span style=color:#e6db74>{</span>kid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                        reply <span style=color:#f92672>=</span> qwen_chat(text)
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;回复:&#34;</span>, reply)
</span></span><span style=display:flex><span>                    tts_synthesis(reply, output_device)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;录音过短，无法识别。&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 3) 对每一路分别做 声纹 -&gt; ASR -&gt; reply -&gt; TTS</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i, sp_audio_sep <span style=color:#f92672>in</span> enumerate(separated):
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>=== 处理第 </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> 个说话人 ===&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#75715e># 先把 sep 模型的采样率转换到 ASR_SR（如果 sep 模型采样率不同）</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> SEP_SR <span style=color:#f92672>!=</span> ASR_SR:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                        sp_audio <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(sp_audio_sep, SEP_SR, ASR_SR)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>&#34;重采样出错，跳过该说话人:&#34;</span>, e)
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                    sp_audio <span style=color:#f92672>=</span> sp_audio_sep<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># 过滤太短或静音</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> sp_audio<span style=color:#f92672>.</span>size <span style=color:#f92672>&lt;</span> MIN_AUDIO_SAMPLES:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;第 </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> 路音频过短（</span><span style=color:#e6db74>{</span>sp_audio<span style=color:#f92672>.</span>size<span style=color:#e6db74>}</span><span style=color:#e6db74> samples），跳过&#34;</span>)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> float(np<span style=color:#f92672>.</span>mean(np<span style=color:#f92672>.</span>abs(sp_audio))) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1e-5</span>:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;第 </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> 路音频过静，跳过&#34;</span>)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># 声纹识别（保护性 try）</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                    emb_stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                    emb_stream<span style=color:#f92672>.</span>accept_waveform(ASR_SR, sp_audio)
</span></span><span style=display:flex><span>                    emb_stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                    emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(emb_stream))
</span></span><span style=display:flex><span>                    spk_name <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>) <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;声纹识别出错（跳过声纹或标为 unknown）:&#34;</span>, e)
</span></span><span style=display:flex><span>                    spk_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># ASR</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                    asr_stream <span style=color:#f92672>=</span> recognizer<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                    asr_stream<span style=color:#f92672>.</span>accept_waveform(ASR_SR, sp_audio)
</span></span><span style=display:flex><span>                    recognizer<span style=color:#f92672>.</span>decode_stream(asr_stream)
</span></span><span style=display:flex><span>                    text <span style=color:#f92672>=</span> asr_stream<span style=color:#f92672>.</span>result<span style=color:#f92672>.</span>text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;ASR 失败:&#34;</span>, e)
</span></span><span style=display:flex><span>                    text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;识别结果: </span><span style=color:#e6db74>{</span>text<span style=color:#e6db74>}</span><span style=color:#e6db74> (说话人: </span><span style=color:#e6db74>{</span>spk_name<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># 生成回复并 TTS</span>
</span></span><span style=display:flex><span>                keyword, kid <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> keyword:
</span></span><span style=display:flex><span>                    reply <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;已为您操作【</span><span style=color:#e6db74>{</span>keyword<span style=color:#e6db74>}</span><span style=color:#e6db74>】，编号</span><span style=color:#e6db74>{</span>kid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                    reply <span style=color:#f92672>=</span> qwen_chat(text) <span style=color:#66d9ef>if</span> text<span style=color:#f92672>.</span>strip() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;抱歉，我没有听清楚。&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;回复:&#34;</span>, reply)
</span></span><span style=display:flex><span>                tts_synthesis(reply, output_device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>KeyboardInterrupt</span>:
</span></span><span style=display:flex><span>        killed <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>程序已终止&#34;</span>)
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>exit(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><h1 id=tts线程优化>TTS线程优化<a hidden class=anchor aria-hidden=true href=#tts线程优化>#</a></h1><p>调用都复用同一个流来播放</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#!/usr/bin/env python3</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -*- coding: utf-8 -*-</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sherpa_onnx
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sounddevice <span style=color:#66d9ef>as</span> sd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> threading
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> queue
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> time
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> defaultdict
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> soundfile <span style=color:#66d9ef>as</span> sf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> typing <span style=color:#f92672>import</span> Dict, List, Optional, Tuple
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pypinyin <span style=color:#f92672>import</span> lazy_pinyin
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> fuzzywuzzy <span style=color:#f92672>import</span> fuzz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> resampy  <span style=color:#75715e># 用于重采样</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ========== 新增：SpeechBrain 分离相关 ==========</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> speechbrain.inference <span style=color:#f92672>import</span> SepformerSeparation <span style=color:#66d9ef>as</span> separator
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchaudio.transforms <span style=color:#66d9ef>as</span> T
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 路径工具 ------------------</span>
</span></span><span style=display:flex><span>current_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>abspath(__file__))
</span></span><span style=display:flex><span>parent_dir <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>dirname(current_dir)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_relative_path</span>(relative_path: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(parent_dir, relative_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 加载关键词配置 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_keywords</span>() <span style=color:#f92672>-&gt;</span> Dict[str, int]:
</span></span><span style=display:flex><span>    keywords_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/keywords.json&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(keywords_file):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>FileNotFoundError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;未找到关键词文件: </span><span style=color:#e6db74>{</span>keywords_file<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(keywords_file, <span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> json<span style=color:#f92672>.</span>load(f)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>KEYWORD2ID <span style=color:#f92672>=</span> load_keywords()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fuzzy_match</span>(text: str) <span style=color:#f92672>-&gt;</span> Tuple[Optional[str], int]:
</span></span><span style=display:flex><span>    txt_py <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(text))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> kw, kid <span style=color:#f92672>in</span> KEYWORD2ID<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> fuzz<span style=color:#f92672>.</span>partial_ratio(<span style=color:#e6db74>&#34;&#34;</span><span style=color:#f92672>.</span>join(lazy_pinyin(kw)), txt_py) <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>70</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> kw, kid
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 加载 system prompt ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_system_prompt</span>() <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    prompt_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/system_prompt.txt&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(prompt_file):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>FileNotFoundError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;未找到 system prompt 文件: </span><span style=color:#e6db74>{</span>prompt_file<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(prompt_file, <span style=color:#e6db74>&#34;r&#34;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf-8&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> f<span style=color:#f92672>.</span>read()<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>SYSTEM_PROMPT <span style=color:#f92672>=</span> load_system_prompt()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 大模型初始化 ------------------</span>
</span></span><span style=display:flex><span>QWEN_PATH <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/models/qwen2.5-0.5b&#34;</span>)
</span></span><span style=display:flex><span>tok <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(QWEN_PATH, trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chat_model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Qwen/Qwen2.5-0.5B&#34;</span>,
</span></span><span style=display:flex><span>    cache_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/root/room/models&#34;</span>,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>qwen_chat</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    msgs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: SYSTEM_PROMPT},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tok(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> chat_model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>            <span style=color:#f92672>**</span>inputs,
</span></span><span style=display:flex><span>            max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>            do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>,
</span></span><span style=display:flex><span>            pad_token_id<span style=color:#f92672>=</span>tok<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    raw_output <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>decode(out[<span style=color:#ae81ff>0</span>][len(inputs<span style=color:#f92672>.</span>input_ids[<span style=color:#ae81ff>0</span>]):], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;&lt;/s&gt;&#34;</span>, <span style=color:#e6db74>&#34;&lt;/user&gt;&#34;</span>, <span style=color:#e6db74>&#34;.user&#34;</span>, <span style=color:#e6db74>&#34;🤨&#34;</span>, <span style=color:#e6db74>&#34;## 128000&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\x0c</span><span style=color:#e6db74>&#34;</span>]:
</span></span><span style=display:flex><span>        raw_output <span style=color:#f92672>=</span> raw_output<span style=color:#f92672>.</span>replace(token, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>    reply <span style=color:#f92672>=</span> raw_output<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> reply
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 声纹相关 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_speaker_embedding_model</span>(model_path: str):
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingExtractorConfig(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model_path,
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> config<span style=color:#f92672>.</span>validate():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Invalid config. </span><span style=color:#e6db74>{</span>config<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingExtractor(config)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_speaker_file</span>(speaker_file_path: str) <span style=color:#f92672>-&gt;</span> Dict[str, List[str]]:
</span></span><span style=display:flex><span>    ans <span style=color:#f92672>=</span> defaultdict(list)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(speaker_file_path) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> f:
</span></span><span style=display:flex><span>            line <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> line:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            fields <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(fields) <span style=color:#f92672>!=</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Invalid line: </span><span style=color:#e6db74>{</span>line<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            speaker_name, filename <span style=color:#f92672>=</span> fields
</span></span><span style=display:flex><span>            ans[speaker_name]<span style=color:#f92672>.</span>append(filename)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ans
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_speaker_embedding</span>(filenames: List[str], extractor) <span style=color:#f92672>-&gt;</span> np<span style=color:#f92672>.</span>ndarray:
</span></span><span style=display:flex><span>    ans <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> filename <span style=color:#f92672>in</span> filenames:
</span></span><span style=display:flex><span>        samples, sr <span style=color:#f92672>=</span> sf<span style=color:#f92672>.</span>read(filename, always_2d<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>        samples <span style=color:#f92672>=</span> samples[:, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        samples <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ascontiguousarray(samples)
</span></span><span style=display:flex><span>        stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>        stream<span style=color:#f92672>.</span>accept_waveform(sr, samples)
</span></span><span style=display:flex><span>        stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>        embedding <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(stream))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ans <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            ans <span style=color:#f92672>=</span> embedding
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            ans <span style=color:#f92672>+=</span> embedding
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> ans <span style=color:#f92672>/</span> len(filenames)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_speaker_identification</span>():
</span></span><span style=display:flex><span>    speaker_model <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/wespeaker_zh_cnceleb_resnet34.onnx&#34;</span>)
</span></span><span style=display:flex><span>    speaker_file <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/speaker.txt&#34;</span>)
</span></span><span style=display:flex><span>    extractor <span style=color:#f92672>=</span> load_speaker_embedding_model(speaker_model)
</span></span><span style=display:flex><span>    speaker_data <span style=color:#f92672>=</span> load_speaker_file(speaker_file)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    manager <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>SpeakerEmbeddingManager(extractor<span style=color:#f92672>.</span>dim)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> name, filelist <span style=color:#f92672>in</span> speaker_data<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>        emb <span style=color:#f92672>=</span> compute_speaker_embedding(filelist, extractor)
</span></span><span style=display:flex><span>        status <span style=color:#f92672>=</span> manager<span style=color:#f92672>.</span>add(name, emb)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> status:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>RuntimeError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Failed to register speaker </span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> extractor, manager
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wake_word_detection_with_speaker</span>(input_device: int, input_sr: int, extractor, speaker_manager) <span style=color:#f92672>-&gt;</span> Optional[str]:
</span></span><span style=display:flex><span>    kws_dir <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01&#34;</span>)
</span></span><span style=display:flex><span>    keyword_spotter <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>KeywordSpotter(
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        encoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        decoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        joiner<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        max_active_paths<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        keywords_file<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;keywords.txt&#34;</span>),
</span></span><span style=display:flex><span>        keywords_score<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>        keywords_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.25</span>,
</span></span><span style=display:flex><span>        num_trailing_blanks<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;等待唤醒词（需声纹匹配）...&#34;</span>)
</span></span><span style=display:flex><span>    stream <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    buffer_seconds <span style=color:#f92672>=</span> <span style=color:#ae81ff>2.0</span>  <span style=color:#75715e># 缓存唤醒词语音用于声纹识别</span>
</span></span><span style=display:flex><span>    max_buffer_len <span style=color:#f92672>=</span> int(buffer_seconds <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>    audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>0</span>,), dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            stream<span style=color:#f92672>.</span>accept_waveform(input_sr, samples)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 语音缓存</span>
</span></span><span style=display:flex><span>            audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate((audio_buffer, samples))
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(audio_buffer) <span style=color:#f92672>&gt;</span> max_buffer_len:
</span></span><span style=display:flex><span>                audio_buffer <span style=color:#f92672>=</span> audio_buffer[<span style=color:#f92672>-</span>max_buffer_len:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> keyword_spotter<span style=color:#f92672>.</span>is_ready(stream):
</span></span><span style=display:flex><span>                keyword_spotter<span style=color:#f92672>.</span>decode_stream(stream)
</span></span><span style=display:flex><span>                result <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>get_result(stream)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> result:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;检测到唤醒词: </span><span style=color:#e6db74>{</span>result<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                    keyword_spotter<span style=color:#f92672>.</span>reset_stream(stream)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#75715e># 进行声纹识别</span>
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;正在进行声纹识别验证...&#34;</span>)
</span></span><span style=display:flex><span>                    audio <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ascontiguousarray(audio_buffer)
</span></span><span style=display:flex><span>                    emb_stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                    emb_stream<span style=color:#f92672>.</span>accept_waveform(input_sr, audio)
</span></span><span style=display:flex><span>                    emb_stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                    emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(emb_stream))
</span></span><span style=display:flex><span>                    speaker <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> speaker:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;声纹识别成功，说话人: </span><span style=color:#e6db74>{</span>speaker<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>return</span> speaker
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>&#34;声纹识别失败，忽略本次唤醒&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 唤醒词（不带声纹，仅示例备用） ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>wake_word_detection</span>(input_device: int, input_sr: int) <span style=color:#f92672>-&gt;</span> bool:
</span></span><span style=display:flex><span>    kws_dir <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01&#34;</span>)
</span></span><span style=display:flex><span>    keyword_spotter <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>KeywordSpotter(
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        encoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;encoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        decoder<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;decoder-epoch-12-avg-2-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        joiner<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;joiner-epoch-99-avg-1-chunk-16-left-64.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        max_active_paths<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        keywords_file<span style=color:#f92672>=</span>os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(kws_dir, <span style=color:#e6db74>&#34;keywords.txt&#34;</span>),
</span></span><span style=display:flex><span>        keywords_score<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>        keywords_threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.25</span>,
</span></span><span style=display:flex><span>        num_trailing_blanks<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;等待唤醒词...&#34;</span>)
</span></span><span style=display:flex><span>    stream <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            stream<span style=color:#f92672>.</span>accept_waveform(input_sr, samples)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> keyword_spotter<span style=color:#f92672>.</span>is_ready(stream):
</span></span><span style=display:flex><span>                keyword_spotter<span style=color:#f92672>.</span>decode_stream(stream)
</span></span><span style=display:flex><span>                result <span style=color:#f92672>=</span> keyword_spotter<span style=color:#f92672>.</span>get_result(stream)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> result:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;检测到唤醒词: </span><span style=color:#e6db74>{</span>result<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                    keyword_spotter<span style=color:#f92672>.</span>reset_stream(stream)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 语音识别（保留原有函数可按需使用） ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>speech_recognition</span>(extractor, speaker_manager, input_device: int, input_sr: int) <span style=color:#f92672>-&gt;</span> Tuple[str, str]:
</span></span><span style=display:flex><span>    target_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>16000</span>  <span style=color:#75715e># VAD/ASR 采样率</span>
</span></span><span style=display:flex><span>    recognizer <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineRecognizer<span style=color:#f92672>.</span>from_paraformer(
</span></span><span style=display:flex><span>        paraformer<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>        tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>        num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        sample_rate<span style=color:#f92672>=</span>target_sr,
</span></span><span style=display:flex><span>        feature_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>80</span>,
</span></span><span style=display:flex><span>        decoding_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;greedy_search&#34;</span>,
</span></span><span style=display:flex><span>        debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    vad_config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>VadModelConfig()
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> get_relative_path(<span style=color:#e6db74>&#34;/root/room/silero_vad.onnx&#34;</span>)
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>min_silence_duration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>min_speech_duration <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    vad_config<span style=color:#f92672>.</span>sample_rate <span style=color:#f92672>=</span> target_sr
</span></span><span style=display:flex><span>    vad <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>VoiceActivityDetector(vad_config, buffer_size_in_seconds<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    window_size <span style=color:#f92672>=</span> vad_config<span style=color:#f92672>.</span>silero_vad<span style=color:#f92672>.</span>window_size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;请说出您的指令...&#34;</span>)
</span></span><span style=display:flex><span>    chunk <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr)
</span></span><span style=display:flex><span>    audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array([], dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(chunk)
</span></span><span style=display:flex><span>            samples <span style=color:#f92672>=</span> samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(samples, input_sr, target_sr)
</span></span><span style=display:flex><span>            audio_buffer <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate([audio_buffer, resampled])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> len(audio_buffer) <span style=color:#f92672>&gt;</span> window_size:
</span></span><span style=display:flex><span>                vad<span style=color:#f92672>.</span>accept_waveform(audio_buffer[:window_size])
</span></span><span style=display:flex><span>                audio_buffer <span style=color:#f92672>=</span> audio_buffer[window_size:]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> vad<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> len(vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> target_sr:
</span></span><span style=display:flex><span>                    vad<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                stream<span style=color:#f92672>.</span>accept_waveform(target_sr, vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples)
</span></span><span style=display:flex><span>                stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(stream))
</span></span><span style=display:flex><span>                speaker <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>) <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>                asr_stream <span style=color:#f92672>=</span> recognizer<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                asr_stream<span style=color:#f92672>.</span>accept_waveform(target_sr, vad<span style=color:#f92672>.</span>front<span style=color:#f92672>.</span>samples)
</span></span><span style=display:flex><span>                recognizer<span style=color:#f92672>.</span>decode_stream(asr_stream)
</span></span><span style=display:flex><span>                text <span style=color:#f92672>=</span> asr_stream<span style=color:#f92672>.</span>result<span style=color:#f92672>.</span>text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                vad<span style=color:#f92672>.</span>pop()
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span> text, speaker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ TTS ------------------</span>
</span></span><span style=display:flex><span>buffer <span style=color:#f92672>=</span> queue<span style=color:#f92672>.</span>Queue()
</span></span><span style=display:flex><span>started <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>killed <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>event <span style=color:#f92672>=</span> threading<span style=color:#f92672>.</span>Event()
</span></span><span style=display:flex><span>tts_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>24000</span>
</span></span><span style=display:flex><span>output_sr <span style=color:#f92672>=</span> <span style=color:#ae81ff>48000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generated_audio_callback</span>(samples: np<span style=color:#f92672>.</span>ndarray, progress: float) <span style=color:#f92672>-&gt;</span> int:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> started
</span></span><span style=display:flex><span>    resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(samples, tts_sr, output_sr)
</span></span><span style=display:flex><span>    buffer<span style=color:#f92672>.</span>put(resampled)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> started:
</span></span><span style=display:flex><span>        started <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> killed <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>play_audio_callback</span>(outdata: np<span style=color:#f92672>.</span>ndarray, frames: int, time, status):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> killed <span style=color:#f92672>or</span> (started <span style=color:#f92672>and</span> buffer<span style=color:#f92672>.</span>empty() <span style=color:#f92672>and</span> stopped):
</span></span><span style=display:flex><span>        event<span style=color:#f92672>.</span>set()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> buffer<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>        outdata<span style=color:#f92672>.</span>fill(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> n <span style=color:#f92672>&lt;</span> frames <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> buffer<span style=color:#f92672>.</span>empty():
</span></span><span style=display:flex><span>        remaining <span style=color:#f92672>=</span> frames <span style=color:#f92672>-</span> n
</span></span><span style=display:flex><span>        k <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> remaining <span style=color:#f92672>&lt;=</span> k:
</span></span><span style=display:flex><span>            outdata[n:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>][:remaining]
</span></span><span style=display:flex><span>            buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>][remaining:]
</span></span><span style=display:flex><span>            n <span style=color:#f92672>=</span> frames
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> buffer<span style=color:#f92672>.</span>queue[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                buffer<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>        outdata[n:n <span style=color:#f92672>+</span> k, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> buffer<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>        n <span style=color:#f92672>+=</span> k
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> n <span style=color:#f92672>&lt;</span> frames:
</span></span><span style=display:flex><span>        outdata[n:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ SpeechBrain 分离：初始化与工具函数 ------------------</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;初始化 SpeechBrain 分离模型（Sepformer）... 这可能需要下载模型到 pretrained_models/sepformer-wsj02mix&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    sep_model <span style=color:#f92672>=</span> separator<span style=color:#f92672>.</span>from_hparams(
</span></span><span style=display:flex><span>        source<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;speechbrain/sepformer-wsj02mix&#34;</span>,
</span></span><span style=display:flex><span>        savedir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pretrained_models/sepformer-wsj02mix&#34;</span>,
</span></span><span style=display:flex><span>        run_opts<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;device&#34;</span>: <span style=color:#e6db74>&#34;cpu&#34;</span>},
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    SEP_SR <span style=color:#f92672>=</span> getattr(sep_model<span style=color:#f92672>.</span>hparams, <span style=color:#e6db74>&#34;sample_rate&#34;</span>, <span style=color:#ae81ff>8000</span>)  <span style=color:#75715e># 通常 sepformer 用 8k</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Sepformer 采样率: </span><span style=color:#e6db74>{</span>SEP_SR<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;加载 Sepformer 模型失败:&#34;</span>, e)
</span></span><span style=display:flex><span>    sep_model <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    SEP_SR <span style=color:#f92672>=</span> <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 我们的 ASR / 声纹 统一用 16k（与 原逻辑一致）</span>
</span></span><span style=display:flex><span>ASR_SR <span style=color:#f92672>=</span> <span style=color:#ae81ff>16000</span>
</span></span><span style=display:flex><span>MIN_AUDIO_SECONDS <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.3</span>  <span style=color:#75715e># 跳过太短片段（秒）</span>
</span></span><span style=display:flex><span>MIN_AUDIO_SAMPLES <span style=color:#f92672>=</span> int(MIN_AUDIO_SECONDS <span style=color:#f92672>*</span> ASR_SR)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_extract_sources_from_sep_output</span>(est_np: np<span style=color:#f92672>.</span>ndarray, max_sources<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>) <span style=color:#f92672>-&gt;</span> List[np<span style=color:#f92672>.</span>ndarray]:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    将 sep_model 输出 ndarray 转为 [n_src, time] 的 list（numpy 1D float32）。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    尽量自动识别哪个轴是源、哪个轴是时间。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> est_np <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> est_np<span style=color:#f92672>.</span>ndim <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 先尝试找到较小的轴（&gt;1 且 &lt;= max_sources）作为 source 轴</span>
</span></span><span style=display:flex><span>    src_axis <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    time_axis <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, s <span style=color:#f92672>in</span> enumerate(est_np<span style=color:#f92672>.</span>shape):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>&lt;</span> s <span style=color:#f92672>&lt;=</span> max_sources:
</span></span><span style=display:flex><span>            src_axis <span style=color:#f92672>=</span> i
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1000</span>:
</span></span><span style=display:flex><span>            time_axis <span style=color:#f92672>=</span> i
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> src_axis <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 退化策略：2D 情况下，较小维度当作 src</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> est_np<span style=color:#f92672>.</span>ndim <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> est_np<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>&lt;</span> est_np<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:
</span></span><span style=display:flex><span>                src_axis, time_axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                src_axis, time_axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> est_np<span style=color:#f92672>.</span>ndim <span style=color:#f92672>==</span> <span style=color:#ae81ff>3</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 常见： (1, n_src, time) 或 (channels, time, n_src)</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 优先选 size 小的轴作为 src</span>
</span></span><span style=display:flex><span>            sizes <span style=color:#f92672>=</span> list(est_np<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>            min_idx <span style=color:#f92672>=</span> int(np<span style=color:#f92672>.</span>argmin(sizes))
</span></span><span style=display:flex><span>            <span style=color:#75715e># 保证不是 time axis</span>
</span></span><span style=display:flex><span>            src_axis <span style=color:#f92672>=</span> min_idx
</span></span><span style=display:flex><span>            <span style=color:#75715e># time axis 取最大轴</span>
</span></span><span style=display:flex><span>            time_axis <span style=color:#f92672>=</span> int(np<span style=color:#f92672>.</span>argmax(sizes))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 最保守策略</span>
</span></span><span style=display:flex><span>            src_axis <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>            time_axis <span style=color:#f92672>=</span> est_np<span style=color:#f92672>.</span>ndim <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> time_axis <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        time_axis <span style=color:#f92672>=</span> max(range(est_np<span style=color:#f92672>.</span>ndim), key<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> i: est_np<span style=color:#f92672>.</span>shape[i] <span style=color:#66d9ef>if</span> i <span style=color:#f92672>!=</span> src_axis <span style=color:#66d9ef>else</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 将 array 重新排列为 (src, time, ...)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 我们把 time 移到 axis=1，src 移到 axis=0</span>
</span></span><span style=display:flex><span>    arr <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>moveaxis(est_np, (src_axis, time_axis), (<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#75715e># 现在 arr.shape[0] = n_src, arr.shape[1] = time</span>
</span></span><span style=display:flex><span>    n_src <span style=color:#f92672>=</span> arr<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    sources <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_src):
</span></span><span style=display:flex><span>        s <span style=color:#f92672>=</span> arr[i]
</span></span><span style=display:flex><span>        <span style=color:#75715e># 若还有额外轴（如 channels），把它们 flatten 或取第一通道</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s<span style=color:#f92672>.</span>ndim <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 把多通道求均值为单通道</span>
</span></span><span style=display:flex><span>            s <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(s, axis<span style=color:#f92672>=</span>tuple(range(<span style=color:#ae81ff>1</span>, s<span style=color:#f92672>.</span>ndim)))
</span></span><span style=display:flex><span>        sources<span style=color:#f92672>.</span>append(s<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sources
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>separate_speakers</span>(mixed_audio: np<span style=color:#f92672>.</span>ndarray, input_sr: int) <span style=color:#f92672>-&gt;</span> List[np<span style=color:#f92672>.</span>ndarray]:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    返回 list，每项为以 SEP_SR 为采样率的 numpy 1D float32 信号（单通道）。
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    （后续处理会把它重采样为 ASR_SR）
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> sep_model <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Sepformer 未初始化，跳过分离，直接返回原始音频&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> [mixed_audio]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 准备模型输入：sep_model 要求的形状可能是 [batch, time] 或 [batch, 1, time] 等</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 我们统一提供 [1, time]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> input_sr <span style=color:#f92672>!=</span> SEP_SR:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 用 resampy 将输入混音重采样到 sep 模型采样率</span>
</span></span><span style=display:flex><span>        mixed_audio_sep <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(mixed_audio, input_sr, SEP_SR)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        mixed_audio_sep <span style=color:#f92672>=</span> mixed_audio<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    audio_tensor <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(mixed_audio_sep)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)  <span style=color:#75715e># [1, time]</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        est <span style=color:#f92672>=</span> sep_model<span style=color:#f92672>.</span>separate_batch(audio_tensor)  <span style=color:#75715e># 返回 tensor，shape 可能多样</span>
</span></span><span style=display:flex><span>    est_np <span style=color:#f92672>=</span> est<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>    sources <span style=color:#f92672>=</span> _extract_sources_from_sep_output(est_np, max_sources<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 过滤极短或静音的源（避免后续造成 extractor 报错）</span>
</span></span><span style=display:flex><span>    filtered <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, s <span style=color:#f92672>in</span> enumerate(sources):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span> <span style=color:#f92672>or</span> s<span style=color:#f92672>.</span>size <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        power <span style=color:#f92672>=</span> float(np<span style=color:#f92672>.</span>mean(np<span style=color:#f92672>.</span>abs(s)))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>10</span>:  <span style=color:#75715e># 非常短</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 过滤非常静的（阈值可根据实际调整）</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> power <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1e-5</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        filtered<span style=color:#f92672>.</span>append(s)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> filtered
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 主流程相关（设备选择 / 录音） ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>select_devices</span>():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;=== 可用输入输出设备列表 ===&#34;</span>)
</span></span><span style=display:flex><span>    print(sd<span style=color:#f92672>.</span>query_devices())
</span></span><span style=display:flex><span>    input_device <span style=color:#f92672>=</span> int(input(<span style=color:#e6db74>&#34;请输入输入设备ID: &#34;</span>))
</span></span><span style=display:flex><span>    output_device <span style=color:#f92672>=</span> int(input(<span style=color:#e6db74>&#34;请输入输出设备ID: &#34;</span>))
</span></span><span style=display:flex><span>    input_info <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>query_devices(input_device)
</span></span><span style=display:flex><span>    output_info <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>query_devices(output_device)
</span></span><span style=display:flex><span>    input_sr <span style=color:#f92672>=</span> int(input_info[<span style=color:#e6db74>&#39;default_samplerate&#39;</span>])
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;选择输入采样率: </span><span style=color:#e6db74>{</span>input_sr<span style=color:#e6db74>}</span><span style=color:#e6db74> Hz&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> input_device, output_device, input_sr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>record_mixed_audio</span>(input_device: int, input_sr: int, duration: float <span style=color:#f92672>=</span> <span style=color:#ae81ff>5.0</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;从麦克风录制一段混合音频（单声道 float32）&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;开始录音 </span><span style=color:#e6db74>{</span>duration<span style=color:#e6db74>}</span><span style=color:#e6db74> 秒...&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> sd<span style=color:#f92672>.</span>InputStream(device<span style=color:#f92672>=</span>input_device, channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>, samplerate<span style=color:#f92672>=</span>input_sr) <span style=color:#66d9ef>as</span> s:
</span></span><span style=display:flex><span>        frames <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        total_chunks <span style=color:#f92672>=</span> int(duration <span style=color:#f92672>/</span> <span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(total_chunks):
</span></span><span style=display:flex><span>            samples, _ <span style=color:#f92672>=</span> s<span style=color:#f92672>.</span>read(int(<span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> input_sr))
</span></span><span style=display:flex><span>            frames<span style=color:#f92672>.</span>append(samples<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>        mixed_audio <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate(frames)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;录音完成&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mixed_audio
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>output_stream <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_output_stream</span>(output_device: int):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> output_stream
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> output_stream <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        output_stream <span style=color:#f92672>=</span> sd<span style=color:#f92672>.</span>OutputStream(
</span></span><span style=display:flex><span>            device<span style=color:#f92672>=</span>output_device,
</span></span><span style=display:flex><span>            channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            callback<span style=color:#f92672>=</span>play_audio_callback,
</span></span><span style=display:flex><span>            dtype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;float32&#34;</span>,
</span></span><span style=display:flex><span>            samplerate<span style=color:#f92672>=</span>output_sr,
</span></span><span style=display:flex><span>            blocksize<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        output_stream<span style=color:#f92672>.</span>start()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tts_synthesis</span>(text: str, output_device: int):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> tts_sr, started, stopped
</span></span><span style=display:flex><span>    init_output_stream(output_device)  <span style=color:#75715e># 复用全局输出流</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    tts_config <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsConfig(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsModelConfig(
</span></span><span style=display:flex><span>            vits<span style=color:#f92672>=</span>sherpa_onnx<span style=color:#f92672>.</span>OfflineTtsVitsModelConfig(
</span></span><span style=display:flex><span>                model<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/zh_CN-huayan-medium.onnx&#34;</span>),
</span></span><span style=display:flex><span>                tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>                data_dir<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/vits-piper-zh_CN-huayan-medium/espeak-ng-data&#34;</span>),
</span></span><span style=display:flex><span>            ),
</span></span><span style=display:flex><span>            provider<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cpu&#34;</span>,
</span></span><span style=display:flex><span>            debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>        max_num_sentences<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    tts <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineTts(tts_config)
</span></span><span style=display:flex><span>    tts_sr <span style=color:#f92672>=</span> tts<span style=color:#f92672>.</span>sample_rate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    event<span style=color:#f92672>.</span>clear()
</span></span><span style=display:flex><span>    started <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    tts<span style=color:#f92672>.</span>generate(text, sid<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, speed<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, callback<span style=color:#f92672>=</span>generated_audio_callback)
</span></span><span style=display:flex><span>    stopped <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    event<span style=color:#f92672>.</span>wait()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ------------------ 主流程 ------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>global</span> killed
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;初始化声纹识别...&#34;</span>)
</span></span><span style=display:flex><span>        extractor, speaker_manager <span style=color:#f92672>=</span> init_speaker_identification()
</span></span><span style=display:flex><span>        input_device, output_device, input_sr <span style=color:#f92672>=</span> select_devices()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 预创建 ASR recognizer（避免在每个说话人上重复创建）</span>
</span></span><span style=display:flex><span>        recognizer <span style=color:#f92672>=</span> sherpa_onnx<span style=color:#f92672>.</span>OfflineRecognizer<span style=color:#f92672>.</span>from_paraformer(
</span></span><span style=display:flex><span>            paraformer<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/model.int8.onnx&#34;</span>),
</span></span><span style=display:flex><span>            tokens<span style=color:#f92672>=</span>get_relative_path(<span style=color:#e6db74>&#34;/root/room/sherpa-onnx-paraformer-zh-2023-03-28/tokens.txt&#34;</span>),
</span></span><span style=display:flex><span>            num_threads<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>            sample_rate<span style=color:#f92672>=</span>ASR_SR,
</span></span><span style=display:flex><span>            feature_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>80</span>,
</span></span><span style=display:flex><span>            decoding_method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;greedy_search&#34;</span>,
</span></span><span style=display:flex><span>            debug<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        speaker <span style=color:#f92672>=</span> wake_word_detection_with_speaker(input_device, input_sr, extractor, speaker_manager)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> speaker:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 1) 录制混合音频</span>
</span></span><span style=display:flex><span>            mixed_audio <span style=color:#f92672>=</span> record_mixed_audio(input_device, input_sr, duration<span style=color:#f92672>=</span><span style=color:#ae81ff>5.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 2) 分离</span>
</span></span><span style=display:flex><span>            separated <span style=color:#f92672>=</span> separate_speakers(mixed_audio, input_sr)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;分离后有效说话人数量: </span><span style=color:#e6db74>{</span>len(separated)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(separated) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;未检测到有效分离结果，回退为直接识别整段语音&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#75715e># 回退：把混合音频重采样到 ASR_SR 并识别</span>
</span></span><span style=display:flex><span>                mixed_resampled <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(mixed_audio, input_sr, ASR_SR)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>                <span style=color:#75715e># 声纹与识别（与单路流程相同）</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> mixed_resampled<span style=color:#f92672>.</span>size <span style=color:#f92672>&gt;=</span> MIN_AUDIO_SAMPLES:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                        emb_stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                        emb_stream<span style=color:#f92672>.</span>accept_waveform(ASR_SR, mixed_resampled)
</span></span><span style=display:flex><span>                        emb_stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                        emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(emb_stream))
</span></span><span style=display:flex><span>                        spk_name <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>) <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>&#34;声纹识别失败:&#34;</span>, e)
</span></span><span style=display:flex><span>                        spk_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                        asr_stream <span style=color:#f92672>=</span> recognizer<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                        asr_stream<span style=color:#f92672>.</span>accept_waveform(ASR_SR, mixed_resampled)
</span></span><span style=display:flex><span>                        recognizer<span style=color:#f92672>.</span>decode_stream(asr_stream)
</span></span><span style=display:flex><span>                        text <span style=color:#f92672>=</span> asr_stream<span style=color:#f92672>.</span>result<span style=color:#f92672>.</span>text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>&#34;ASR 失败:&#34;</span>, e)
</span></span><span style=display:flex><span>                        text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;识别结果: </span><span style=color:#e6db74>{</span>text<span style=color:#e6db74>}</span><span style=color:#e6db74> (说话人: </span><span style=color:#e6db74>{</span>spk_name<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>                    keyword, kid <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> keyword:
</span></span><span style=display:flex><span>                        reply <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;已为您操作【</span><span style=color:#e6db74>{</span>keyword<span style=color:#e6db74>}</span><span style=color:#e6db74>】，编号</span><span style=color:#e6db74>{</span>kid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                        reply <span style=color:#f92672>=</span> qwen_chat(text)
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;回复:&#34;</span>, reply)
</span></span><span style=display:flex><span>                    tts_synthesis(reply, output_device)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;录音过短，无法识别。&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 3) 对每一路分别做 声纹 -&gt; ASR -&gt; reply -&gt; TTS</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i, sp_audio_sep <span style=color:#f92672>in</span> enumerate(separated):
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>=== 处理第 </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> 个说话人 ===&#34;</span>)
</span></span><span style=display:flex><span>                <span style=color:#75715e># 先把 sep 模型的采样率转换到 ASR_SR（如果 sep 模型采样率不同）</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> SEP_SR <span style=color:#f92672>!=</span> ASR_SR:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                        sp_audio <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(sp_audio_sep, SEP_SR, ASR_SR)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                        print(<span style=color:#e6db74>&#34;重采样出错，跳过该说话人:&#34;</span>, e)
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                    sp_audio <span style=color:#f92672>=</span> sp_audio_sep<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># 过滤太短或静音</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> sp_audio<span style=color:#f92672>.</span>size <span style=color:#f92672>&lt;</span> MIN_AUDIO_SAMPLES:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;第 </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> 路音频过短（</span><span style=color:#e6db74>{</span>sp_audio<span style=color:#f92672>.</span>size<span style=color:#e6db74>}</span><span style=color:#e6db74> samples），跳过&#34;</span>)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> float(np<span style=color:#f92672>.</span>mean(np<span style=color:#f92672>.</span>abs(sp_audio))) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>1e-5</span>:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;第 </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> 路音频过静，跳过&#34;</span>)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># 声纹识别（保护性 try）</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                    emb_stream <span style=color:#f92672>=</span> extractor<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                    emb_stream<span style=color:#f92672>.</span>accept_waveform(ASR_SR, sp_audio)
</span></span><span style=display:flex><span>                    emb_stream<span style=color:#f92672>.</span>input_finished()
</span></span><span style=display:flex><span>                    emb <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(extractor<span style=color:#f92672>.</span>compute(emb_stream))
</span></span><span style=display:flex><span>                    spk_name <span style=color:#f92672>=</span> speaker_manager<span style=color:#f92672>.</span>search(emb, threshold<span style=color:#f92672>=</span><span style=color:#ae81ff>0.6</span>) <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;声纹识别出错（跳过声纹或标为 unknown）:&#34;</span>, e)
</span></span><span style=display:flex><span>                    spk_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;unknown&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># ASR</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                    asr_stream <span style=color:#f92672>=</span> recognizer<span style=color:#f92672>.</span>create_stream()
</span></span><span style=display:flex><span>                    asr_stream<span style=color:#f92672>.</span>accept_waveform(ASR_SR, sp_audio)
</span></span><span style=display:flex><span>                    recognizer<span style=color:#f92672>.</span>decode_stream(asr_stream)
</span></span><span style=display:flex><span>                    text <span style=color:#f92672>=</span> asr_stream<span style=color:#f92672>.</span>result<span style=color:#f92672>.</span>text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>                    print(<span style=color:#e6db74>&#34;ASR 失败:&#34;</span>, e)
</span></span><span style=display:flex><span>                    text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;识别结果: </span><span style=color:#e6db74>{</span>text<span style=color:#e6db74>}</span><span style=color:#e6db74> (说话人: </span><span style=color:#e6db74>{</span>spk_name<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e># 生成回复并 TTS</span>
</span></span><span style=display:flex><span>                keyword, kid <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> keyword:
</span></span><span style=display:flex><span>                    reply <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;已为您操作【</span><span style=color:#e6db74>{</span>keyword<span style=color:#e6db74>}</span><span style=color:#e6db74>】，编号</span><span style=color:#e6db74>{</span>kid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                    reply <span style=color:#f92672>=</span> qwen_chat(text) <span style=color:#66d9ef>if</span> text<span style=color:#f92672>.</span>strip() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#34;抱歉，我没有听清楚。&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>&#34;回复:&#34;</span>, reply)
</span></span><span style=display:flex><span>                tts_synthesis(reply, output_device)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>KeyboardInterrupt</span>:
</span></span><span style=display:flex><span>        killed <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>程序已终止&#34;</span>)
</span></span><span style=display:flex><span>        sys<span style=color:#f92672>.</span>exit(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><h1 id=大模型过滤符号等>大模型过滤符号等<a hidden class=anchor aria-hidden=true href=#大模型过滤符号等>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>qwen_chat</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        msgs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>            {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>SYSTEM_PROMPT<span style=color:#e6db74>}</span><span style=color:#e6db74>。请用纯中文回答，不要包含任何英文、代码、特殊符号，语句要自然通顺。&#34;</span>},
</span></span><span style=display:flex><span>            {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>        text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        inputs <span style=color:#f92672>=</span> tok(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>            out <span style=color:#f92672>=</span> chat_model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>                <span style=color:#f92672>**</span>inputs,
</span></span><span style=display:flex><span>                max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>                do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>,
</span></span><span style=display:flex><span>                pad_token_id<span style=color:#f92672>=</span>tok<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>        raw_output <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>decode(out[<span style=color:#ae81ff>0</span>][len(inputs<span style=color:#f92672>.</span>input_ids[<span style=color:#ae81ff>0</span>]):], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 加强过滤：移除所有非中文字符（保留标点）</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span>        <span style=color:#75715e># 只保留中文、中文标点、数字</span>
</span></span><span style=display:flex><span>        filtered_output <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;[^\u4e00-\u9fa5，。？！,.:;？！]&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>, raw_output)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 如果过滤后为空，返回默认回复</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> filtered_output<span style=color:#f92672>.</span>strip():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;抱歉，我没理解 意思，请再说一遍。&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> filtered_output
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>Exception</span> <span style=color:#66d9ef>as</span> e:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;大模型生成出错: </span><span style=color:#e6db74>{</span>e<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;抱歉，处理 请求时出现错误。&#34;</span>
</span></span></code></pre></div><h1 id=最小音频功率>最小音频功率<a hidden class=anchor aria-hidden=true href=#最小音频功率>#</a></h1><h2 id=1-提高分离后音频的过滤阈值过滤短片段和噪音>1. 提高分离后音频的过滤阈值（过滤短片段和噪音）<a hidden class=anchor aria-hidden=true href=#1-提高分离后音频的过滤阈值过滤短片段和噪音>#</a></h2><p>修改音频过滤参数，只保留足够长、音量足够大的片段：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 修改全局参数（原参数值调大）</span>
</span></span><span style=display:flex><span>MIN_AUDIO_SECONDS <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.8</span>  <span style=color:#75715e># 最小音频长度从0.2秒提高到0.8秒（过滤过短片段）</span>
</span></span><span style=display:flex><span>MIN_AUDIO_SAMPLES <span style=color:#f92672>=</span> int(MIN_AUDIO_SECONDS <span style=color:#f92672>*</span> ASR_SR)
</span></span><span style=display:flex><span>MIN_AUDIO_POWER <span style=color:#f92672>=</span> <span style=color:#ae81ff>1e-4</span>  <span style=color:#75715e># 新增：最小音频功率（过滤静音/噪音）</span>
</span></span></code></pre></div><h2 id=2-在分离后过滤时应用更严格的条件>2. 在分离后过滤时应用更严格的条件<a hidden class=anchor aria-hidden=true href=#2-在分离后过滤时应用更严格的条件>#</a></h2><p>修改 separate_speakers 函数中的过滤逻辑：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>separate_speakers</span>(mixed_audio: np<span style=color:#f92672>.</span>ndarray, input_sr: int) <span style=color:#f92672>-&gt;</span> List[np<span style=color:#f92672>.</span>ndarray]:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> sep_model <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;Sepformer 未初始化，跳过分离，直接返回原始音频&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> [mixed_audio]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># （原代码不变：采样率转换、模型分离）</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> input_sr <span style=color:#f92672>!=</span> SEP_SR:
</span></span><span style=display:flex><span>        mixed_audio_sep <span style=color:#f92672>=</span> resampy<span style=color:#f92672>.</span>resample(mixed_audio, input_sr, SEP_SR)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        mixed_audio_sep <span style=color:#f92672>=</span> mixed_audio<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>    audio_tensor <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>from_numpy(mixed_audio_sep)<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        est <span style=color:#f92672>=</span> sep_model<span style=color:#f92672>.</span>separate_batch(audio_tensor)
</span></span><span style=display:flex><span>    est_np <span style=color:#f92672>=</span> est<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>    sources <span style=color:#f92672>=</span> _extract_sources_from_sep_output(est_np, max_sources<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 更严格的过滤：只保留长音频、高音量的片段</span>
</span></span><span style=display:flex><span>    filtered <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, s <span style=color:#f92672>in</span> enumerate(sources):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span> <span style=color:#f92672>or</span> s<span style=color:#f92672>.</span>size <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 计算音频功率（音量）</span>
</span></span><span style=display:flex><span>        power <span style=color:#f92672>=</span> float(np<span style=color:#f92672>.</span>mean(np<span style=color:#f92672>.</span>square(s)))  <span style=color:#75715e># 用平方均值更能反映音量</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 长度过滤（至少0.8秒）+ 音量过滤（功率足够大）</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> s<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>&lt;</span> MIN_AUDIO_SAMPLES <span style=color:#f92672>*</span> SEP_SR <span style=color:#f92672>/</span> ASR_SR:  <span style=color:#75715e># 按SEP_SR换算长度</span>
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;第 </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> 路音频过短（</span><span style=color:#e6db74>{</span>s<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>/</span>SEP_SR<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>秒），跳过&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> power <span style=color:#f92672>&lt;</span> MIN_AUDIO_POWER:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;第 </span><span style=color:#e6db74>{</span>i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74> 路音频音量过低（功率</span><span style=color:#e6db74>{</span>power<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>），跳过&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        filtered<span style=color:#f92672>.</span>append(s)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 限制最大分离数量（如果是单人场景，强制只保留1个最可能的说话人）</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> len(filtered) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 按音量排序，保留最大音量的那个（最可能是有效说话人）</span>
</span></span><span style=display:flex><span>        filtered<span style=color:#f92672>.</span>sort(key<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> x: np<span style=color:#f92672>.</span>mean(np<span style=color:#f92672>.</span>square(x)), reverse<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        filtered <span style=color:#f92672>=</span> filtered[:<span style=color:#ae81ff>1</span>]  <span style=color:#75715e># 只保留1个</span>
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;检测到多个分离结果，保留最可能的1个说话人&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> filtered
</span></span></code></pre></div><p>关键修改：</p><p>提高最小音频长度（0.8 秒），过滤因噪音产生的短片段；</p><p>增加音量过滤（功率阈值），排除静音或低音量的无效片段；</p><p>限制最大分离数量为 1（单人场景下），避免同一人语音被拆分。</p><h1 id=使用gtp-oos-20b---需-128g内存>使用gtp-oos-20b 需 128g内存<a hidden class=anchor aria-hidden=true href=#使用gtp-oos-20b---需-128g内存>#</a></h1><p>下载模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install huggingface-cli
</span></span><span style=display:flex><span>huggingface-cli download openai/gpt-oss-20b --include <span style=color:#e6db74>&#34;original/*&#34;</span> --local-dir gpt-oss-20b/
</span></span></code></pre></div><p>下载tokenizer 文件</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>huggingface-cli download openai/gpt-oss-20b --include <span style=color:#e6db74>&#34;tokenizer*&#34;</span> --local-dir /root/room/gpt-oss-20b/original
</span></span></code></pre></div><h2 id=python-version--313>python version 3.13<a hidden class=anchor aria-hidden=true href=#python-version--313>#</a></h2><p>使用 pyenv 管理版本
pyenv 可以方便地安装和切换多个 Python 版本：</p><p>安装 pyenv</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 克隆 pyenv 仓库</span>
</span></span><span style=display:flex><span>git clone https://github.com/pyenv/pyenv.git ~/.pyenv
</span></span><span style=display:flex><span><span style=color:#75715e># 配置环境变量（根据shell类型选择，如bash/zsh）</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#39;export PYENV_ROOT=&#34;$HOME/.pyenv&#34;&#39;</span> &gt;&gt; ~/.bashrc
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#39;export PATH=&#34;$PYENV_ROOT/bin:$PATH&#34;&#39;</span> &gt;&gt; ~/.bashrc
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#39;eval &#34;$(pyenv init -)&#34;&#39;</span> &gt;&gt; ~/.bashrc
</span></span><span style=display:flex><span>source ~/.bashrc  <span style=color:#75715e># 刷新配置</span>
</span></span></code></pre></div><h2 id=安装-python-313>安装 Python 3.13<a hidden class=anchor aria-hidden=true href=#安装-python-313>#</a></h2><p>如果 3.13 已在 pyenv 支持列表中：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pyenv install 3.13.0  <span style=color:#75715e># 替换为实际版本号</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 全局启用 3.13</span>
</span></span><span style=display:flex><span>pyenv global 3.13.0
</span></span><span style=display:flex><span><span style=color:#75715e># 验证</span>
</span></span><span style=display:flex><span>python --version
</span></span></code></pre></div><p>为了确保 Python 3.13 编译时相关模块（ctypes、lzma、ssl、bz2 等）都能正常编译，推荐一次装齐依赖：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>apt-get update
</span></span><span style=display:flex><span>apt-get install -y <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  build-essential <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libffi-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libssl-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  zlib1g-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libbz2-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libreadline-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libsqlite3-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libncurses5-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  libncursesw5-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  xz-utils <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  tk-dev <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  liblzma-dev
</span></span></code></pre></div><p>安装依赖后重新编译 Python
安装完依赖包后，重新用 pyenv 编译：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pyenv uninstall 3.13.0
</span></span><span style=display:flex><span>pyenv install 3.13.0
</span></span><span style=display:flex><span><span style=color:#75715e># 然后激活环境：</span>
</span></span><span style=display:flex><span>pyenv global 3.13.0
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>source myenv/bin/activate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pip install sounddevice numpy resampy  transformers torch fuzzywuzzy pypinyin soundfile sounddevice sherpa_onnx numpy  scipy
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 大模型gpt-oss-20b</span>
</span></span><span style=display:flex><span>GPT_OSS_PATH <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/root/room/gpt-oss-20b/original&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tok <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(GPT_OSS_PATH, trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>chat_model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    GPT_OSS_PATH,
</span></span><span style=display:flex><span>    torch_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float16 <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> torch<span style=color:#f92672>.</span>float32,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>gptoss_chat</span>(prompt: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    msgs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: SYSTEM_PROMPT},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: prompt}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>apply_chat_template(msgs, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, add_generation_prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tok(text, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> inputs<span style=color:#f92672>.</span>to(chat_model<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> chat_model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>            <span style=color:#f92672>**</span>inputs,
</span></span><span style=display:flex><span>            max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>            do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>,
</span></span><span style=display:flex><span>            pad_token_id<span style=color:#f92672>=</span>tok<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    raw <span style=color:#f92672>=</span> tok<span style=color:#f92672>.</span>decode(out[<span style=color:#ae81ff>0</span>][len(inputs<span style=color:#f92672>.</span>input_ids[<span style=color:#ae81ff>0</span>]):], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;&lt;/s&gt;&#34;</span>, <span style=color:#e6db74>&#34;&lt;/user&gt;&#34;</span>, <span style=color:#e6db74>&#34;.user&#34;</span>, <span style=color:#e6db74>&#34;## 128000&#34;</span>, <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\x0c</span><span style=color:#e6db74>&#34;</span>]:
</span></span><span style=display:flex><span>        raw <span style=color:#f92672>=</span> raw<span style=color:#f92672>.</span>replace(token, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> raw<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span><span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  speaker <span style=color:#f92672>=</span> wake_word_detection_with_speaker(input_device, input_sr, extractor, speaker_manager)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> speaker:
</span></span><span style=display:flex><span>            text, detected_speaker <span style=color:#f92672>=</span> speech_recognition(extractor, speaker_manager, input_device, input_sr)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;识别结果: </span><span style=color:#e6db74>{</span>text<span style=color:#e6db74>}</span><span style=color:#e6db74> (说话人: </span><span style=color:#e6db74>{</span>detected_speaker<span style=color:#e6db74>}</span><span style=color:#e6db74>)&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            keyword, kid <span style=color:#f92672>=</span> fuzzy_match(text)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> keyword:
</span></span><span style=display:flex><span>                reply <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;已为您操作【</span><span style=color:#e6db74>{</span>keyword<span style=color:#e6db74>}</span><span style=color:#e6db74>】，编号</span><span style=color:#e6db74>{</span>kid<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                reply <span style=color:#f92672>=</span> gptoss_chat(text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;回复:&#34;</span>, reply)
</span></span><span style=display:flex><span>            tts_synthesis(reply, output_device)
</span></span><span style=display:flex><span>            
</span></span></code></pre></div><p>升级到 transformers 的 最新开发版，因为 gpt-oss 是非常新的模型类型：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install --upgrade pip setuptools
</span></span><span style=display:flex><span>pip uninstall transformers -y
</span></span><span style=display:flex><span>pip install git+https://github.com/huggingface/transformers.git
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python asrtts.py
</span></span></code></pre></div><h1 id=模型本地运行参考>模型本地运行参考<a hidden class=anchor aria-hidden=true href=#模型本地运行参考>#</a></h1><h2 id=qwen>Qwen<a hidden class=anchor aria-hidden=true href=#qwen>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Qwen2.5-0.5B：至少 4GB 内存，可使用 CPU 部署，若有 NVIDIA GeForce GT <span style=color:#ae81ff>1030</span> 等显卡则更好。CPU 方面没有特别高要求，普通家用电脑 CPU 即可。
</span></span><span style=display:flex><span>Qwen2.5-1.5B：8GB 内存起步，建议使用 Intel i5 或 AMD Ryzen <span style=color:#ae81ff>5</span> 及以上 CPU，若有 NVIDIA GeForce GTX <span style=color:#ae81ff>1660</span> 或同等性能显卡（显存≥6GB）可加速运算。
</span></span><span style=display:flex><span>Qwen2.5-3B：建议 12GB 及以上内存，推荐使用 NVIDIA GeForce RTX <span style=color:#ae81ff>2060</span> 及以上显卡，CPU 建议为 Intel i7 或 AMD Ryzen <span style=color:#ae81ff>7</span> 及以上型号，以保证处理更复杂语义理解、多轮对话等任务时的效率。
</span></span><span style=display:flex><span>Qwen2.5-7B：至少 16GB 内存，推荐使用 NVIDIA GeForce RTX <span style=color:#ae81ff>3060</span> 及以上显卡。CPU 至少 <span style=color:#ae81ff>8</span> 核心的高性能处理器，如 Intel Core i7 或 AMD Ryzen <span style=color:#ae81ff>7</span> 系列。
</span></span><span style=display:flex><span>Qwen2.5-14B：32GB 内存起步，可使用 NVIDIA GeForce RTX <span style=color:#ae81ff>3060</span> 及以上显卡。建议配备至少 <span style=color:#ae81ff>16</span> 核 CPU，如 AMD EPYC 或 Intel Xeon 系列处理器，以应对复杂长文本处理等任务。
</span></span><span style=display:flex><span>Qwen2.5-32B：64GB 及以上内存，需搭配 NVIDIA GeForce RTX <span style=color:#ae81ff>40</span> 系列及以上高端显卡。CPU 建议为 <span style=color:#ae81ff>16</span> 核以上的高性能处理器，如 AMD EPYC 7xxx 或 Intel Xeon Scalable 系列。
</span></span><span style=display:flex><span>Qwen2.5-72B：128GB 及以上内存，推荐使用 NVIDIA GeForce RTX <span style=color:#ae81ff>40</span> 系列及以上显卡，且多卡并行更佳。CPU 建议至少 <span style=color:#ae81ff>32</span> 核，如 AMD Ryzen Threadripper 或 Intel Core i9 系列，若能使用 <span style=color:#ae81ff>64</span> 核以上的 AMD EPYC 或 Intel Xeon 系列处理器则更好
</span></span></code></pre></div><h2 id=deepseek-r1>DeepSeek-R1<a hidden class=anchor aria-hidden=true href=#deepseek-r1>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>DeepSeek-R1-1.5B：CPU 最低 <span style=color:#ae81ff>4</span> 核（推荐 Intel/AMD 多核处理器），内存 8GB+。
</span></span><span style=display:flex><span>DeepSeek-R1-7B/8B：建议 CPU 为 <span style=color:#ae81ff>8</span> 核以上（推荐现代多核 CPU），内存 16GB+。
</span></span><span style=display:flex><span>DeepSeek-R1-14B：建议配备 <span style=color:#ae81ff>12</span> 核以上 CPU，内存 32GB+。
</span></span><span style=display:flex><span>DeepSeek-R1-32B：需要 <span style=color:#ae81ff>16</span> 核以上 CPU（如 AMD Ryzen <span style=color:#ae81ff>9</span> 或 Intel i9），内存 64GB+。
</span></span><span style=display:flex><span>DeepSeek-R1-70B：建议使用 <span style=color:#ae81ff>32</span> 核以上服务器级 CPU，内存 128GB+。
</span></span><span style=display:flex><span>DeepSeek-R1-671B：需 <span style=color:#ae81ff>64</span> 核以上的服务器集群，内存 512GB+。
</span></span></code></pre></div><hr></div><footer class=post-footer><ul class=post-tags><li><a href=https://qfsyso.github.io/tags/api/>API</a></li><li><a href=https://qfsyso.github.io/tags/webapi/>WebAPI</a></li><li><a href=https://qfsyso.github.io/tags/ai/>AI</a></li><li><a href=https://qfsyso.github.io/tags/vosk/>Vosk</a></li><li><a href=https://qfsyso.github.io/tags/python/>Python</a></li></ul><nav class=paginav><a class=prev href=https://qfsyso.github.io/posts/immich/><span class=title>« Prev</span><br><span>Immich</span>
</a><a class=next href=https://qfsyso.github.io/posts/api-signature/><span class=title>Next »</span><br><span>API Signature</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Vosk STT on x" href="https://x.com/intent/tweet/?text=Vosk%20STT&amp;url=https%3a%2f%2fqfsyso.github.io%2fposts%2fvosk-stt%2f&amp;hashtags=API%2cWebAPI%2cAI%2cVosk%2cPython"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vosk STT on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fqfsyso.github.io%2fposts%2fvosk-stt%2f&amp;title=Vosk%20STT&amp;summary=Vosk%20STT&amp;source=https%3a%2f%2fqfsyso.github.io%2fposts%2fvosk-stt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vosk STT on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fqfsyso.github.io%2fposts%2fvosk-stt%2f&title=Vosk%20STT"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vosk STT on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fqfsyso.github.io%2fposts%2fvosk-stt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vosk STT on whatsapp" href="https://api.whatsapp.com/send?text=Vosk%20STT%20-%20https%3a%2f%2fqfsyso.github.io%2fposts%2fvosk-stt%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vosk STT on telegram" href="https://telegram.me/share/url?text=Vosk%20STT&amp;url=https%3a%2f%2fqfsyso.github.io%2fposts%2fvosk-stt%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Vosk STT on ycombinator" href="https://news.ycombinator.com/submitlink?t=Vosk%20STT&u=https%3a%2f%2fqfsyso.github.io%2fposts%2fvosk-stt%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><div class=wtime><div class=wtime2>CST<span id=beijingTime></span> GMT <span id=londonTime></span> EST <span id=newYorkTime></span> PST<span id=losAngelesTime></span></div><div class=wtime2 style=display:none>东京<span id=tokyoTime></span></div><div class=wtime2 style=display:none>欧洲-巴黎<span id=parisTime></span></div><div class=wtime2 style=display:none>UTC<span id=utcTime></span></div></div><span>&copy; 2025 <a href=https://qfsyso.github.io/>MLOG</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script type=text/javascript>const timeZoneOffsets={beijing:8,tokyo:9,paris:[1,2],london:[0,1],newYork:[-5,-4],losAngeles:[-8,-7]};function padZero(e){return e<10?"0"+e:e}function formatTime(e){const t=e.getFullYear(),n=padZero(e.getMonth()+1),s=padZero(e.getDate()),o=padZero(e.getHours()),i=padZero(e.getMinutes()),a=padZero(e.getSeconds());return`${t}-${n}-${s} ${o}:${i}:${a}`}function isDaylightSavingTime(e,t){const o=e.getMonth(),n=new Date(e.getFullYear(),3,8,2,0,0),s=new Date(e.getFullYear(),10,1,2,0,0);return(t==="paris"||t==="london"||t==="newYork"||t==="losAngeles")&&e>=n&&e<s}function updateTime(){const e=new Date,t=new Date(e.getUTCFullYear(),e.getUTCMonth(),e.getUTCDate(),e.getUTCHours(),e.getUTCMinutes(),e.getUTCSeconds()),n=formatTime(t);document.getElementById("utcTime").innerHTML=n;for(const n in timeZoneOffsets){const e=timeZoneOffsets[n];let s;Array.isArray(e)?s=isDaylightSavingTime(t,n)?e[1]:e[0]:s=e;const o=new Date(t.getTime()+s*60*60*1e3);document.getElementById(n+"Time").innerHTML=formatTime(o)}setTimeout(updateTime,1e3)}window.onload=function(){updateTime()};function notfound(e){var n=e.src,t=n.split("/"),s=t[t.length-1];e.src="https://47.115.223.75:8080/group1/v2/"+s+"?timestamp="+Date.now(),e.onerror=null}</script></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>