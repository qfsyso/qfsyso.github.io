<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>deepseek local deploy | MLOG</title>
<meta name=keywords content="DeepSeek"><meta name=description content="DeepSeek DeepSeek是杭州深度求索人工智能技术有限公司推出的人工智能项目，涵盖公司品牌、人工智能模型及相关 AI 工具等多层含义，以下是具体介绍： 公司层面DeepSeek 是一家 2023 年 5 月成立的中国人工智能公司，由对冲基金公司 High-Flyer 出资成立。其创始人梁文峰带领团队专注于人工智能基础技术研究与开发，在人工智能领域尤其是大语言模型方向投入了大量研究资源并取得成果。 模型层面 DeepSeek Coder：2023 年 11 月 2 日发布，是完全开源且可免费用于商业用途的模型，为开发者提供了基础的代码生成等能力支持。 DeepSeek LLM：2023 年 11 月 29 日推出，具有 670 亿参数，性能与当时的 GPT-4 接近，同时推出了聊天版本 DeepSeek Chat。 DeepSeek V2：2024 年 5 月发布，以性价比高而闻名，每百万输出 tokens 仅需 2 元人民币，在 LLM 排名中位列第七。 DeepSeek R1-lite-preview：2024 年 11 月发布，擅长逻辑推理、数学推理和实时问题解决，在一些基准测试中声称超越了 OpenAI O1，但也有测试显示存在速度等方面的挑战。 DeepSeek V3：2024 年 12 月发布，拥有 6710 亿参数，训练仅花费 558 万美元，约 55 天时间，在基准测试中性能优于 Llama 3.1 和 Qwen 2.5，与 GPT-4 O 和 Claude 3."><meta name=author content="dwd"><link rel=canonical href=https://qfsyso.github.io/posts/deepseek-local-deploy/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://qfsyso.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://qfsyso.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://qfsyso.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://qfsyso.github.io/apple-touch-icon.png><link rel=mask-icon href=https://qfsyso.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://qfsyso.github.io/posts/deepseek-local-deploy/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://qfsyso.github.io/posts/deepseek-local-deploy/"><meta property="og:site_name" content="MLOG"><meta property="og:title" content="deepseek local deploy"><meta property="og:description" content="DeepSeek DeepSeek是杭州深度求索人工智能技术有限公司推出的人工智能项目，涵盖公司品牌、人工智能模型及相关 AI 工具等多层含义，以下是具体介绍： 公司层面DeepSeek 是一家 2023 年 5 月成立的中国人工智能公司，由对冲基金公司 High-Flyer 出资成立。其创始人梁文峰带领团队专注于人工智能基础技术研究与开发，在人工智能领域尤其是大语言模型方向投入了大量研究资源并取得成果。 模型层面 DeepSeek Coder：2023 年 11 月 2 日发布，是完全开源且可免费用于商业用途的模型，为开发者提供了基础的代码生成等能力支持。 DeepSeek LLM：2023 年 11 月 29 日推出，具有 670 亿参数，性能与当时的 GPT-4 接近，同时推出了聊天版本 DeepSeek Chat。 DeepSeek V2：2024 年 5 月发布，以性价比高而闻名，每百万输出 tokens 仅需 2 元人民币，在 LLM 排名中位列第七。 DeepSeek R1-lite-preview：2024 年 11 月发布，擅长逻辑推理、数学推理和实时问题解决，在一些基准测试中声称超越了 OpenAI O1，但也有测试显示存在速度等方面的挑战。 DeepSeek V3：2024 年 12 月发布，拥有 6710 亿参数，训练仅花费 558 万美元，约 55 天时间，在基准测试中性能优于 Llama 3.1 和 Qwen 2.5，与 GPT-4 O 和 Claude 3."><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-06T04:21:55+00:00"><meta property="article:modified_time" content="2025-02-06T04:21:55+00:00"><meta property="article:tag" content="DeepSeek"><meta name=twitter:card content="summary"><meta name=twitter:title content="deepseek local deploy"><meta name=twitter:description content="DeepSeek DeepSeek是杭州深度求索人工智能技术有限公司推出的人工智能项目，涵盖公司品牌、人工智能模型及相关 AI 工具等多层含义，以下是具体介绍： 公司层面DeepSeek 是一家 2023 年 5 月成立的中国人工智能公司，由对冲基金公司 High-Flyer 出资成立。其创始人梁文峰带领团队专注于人工智能基础技术研究与开发，在人工智能领域尤其是大语言模型方向投入了大量研究资源并取得成果。 模型层面 DeepSeek Coder：2023 年 11 月 2 日发布，是完全开源且可免费用于商业用途的模型，为开发者提供了基础的代码生成等能力支持。 DeepSeek LLM：2023 年 11 月 29 日推出，具有 670 亿参数，性能与当时的 GPT-4 接近，同时推出了聊天版本 DeepSeek Chat。 DeepSeek V2：2024 年 5 月发布，以性价比高而闻名，每百万输出 tokens 仅需 2 元人民币，在 LLM 排名中位列第七。 DeepSeek R1-lite-preview：2024 年 11 月发布，擅长逻辑推理、数学推理和实时问题解决，在一些基准测试中声称超越了 OpenAI O1，但也有测试显示存在速度等方面的挑战。 DeepSeek V3：2024 年 12 月发布，拥有 6710 亿参数，训练仅花费 558 万美元，约 55 天时间，在基准测试中性能优于 Llama 3.1 和 Qwen 2.5，与 GPT-4 O 和 Claude 3."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://qfsyso.github.io/posts/"},{"@type":"ListItem","position":2,"name":"deepseek local deploy","item":"https://qfsyso.github.io/posts/deepseek-local-deploy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"deepseek local deploy","name":"deepseek local deploy","description":"DeepSeek DeepSeek是杭州深度求索人工智能技术有限公司推出的人工智能项目，涵盖公司品牌、人工智能模型及相关 AI 工具等多层含义，以下是具体介绍： 公司层面DeepSeek 是一家 2023 年 5 月成立的中国人工智能公司，由对冲基金公司 High-Flyer 出资成立。其创始人梁文峰带领团队专注于人工智能基础技术研究与开发，在人工智能领域尤其是大语言模型方向投入了大量研究资源并取得成果。 模型层面 DeepSeek Coder：2023 年 11 月 2 日发布，是完全开源且可免费用于商业用途的模型，为开发者提供了基础的代码生成等能力支持。 DeepSeek LLM：2023 年 11 月 29 日推出，具有 670 亿参数，性能与当时的 GPT-4 接近，同时推出了聊天版本 DeepSeek Chat。 DeepSeek V2：2024 年 5 月发布，以性价比高而闻名，每百万输出 tokens 仅需 2 元人民币，在 LLM 排名中位列第七。 DeepSeek R1-lite-preview：2024 年 11 月发布，擅长逻辑推理、数学推理和实时问题解决，在一些基准测试中声称超越了 OpenAI O1，但也有测试显示存在速度等方面的挑战。 DeepSeek V3：2024 年 12 月发布，拥有 6710 亿参数，训练仅花费 558 万美元，约 55 天时间，在基准测试中性能优于 Llama 3.1 和 Qwen 2.5，与 GPT-4 O 和 Claude 3.","keywords":["DeepSeek"],"articleBody":"DeepSeek DeepSeek是杭州深度求索人工智能技术有限公司推出的人工智能项目，涵盖公司品牌、人工智能模型及相关 AI 工具等多层含义，以下是具体介绍： 公司层面DeepSeek 是一家 2023 年 5 月成立的中国人工智能公司，由对冲基金公司 High-Flyer 出资成立。其创始人梁文峰带领团队专注于人工智能基础技术研究与开发，在人工智能领域尤其是大语言模型方向投入了大量研究资源并取得成果。 模型层面 DeepSeek Coder：2023 年 11 月 2 日发布，是完全开源且可免费用于商业用途的模型，为开发者提供了基础的代码生成等能力支持。 DeepSeek LLM：2023 年 11 月 29 日推出，具有 670 亿参数，性能与当时的 GPT-4 接近，同时推出了聊天版本 DeepSeek Chat。 DeepSeek V2：2024 年 5 月发布，以性价比高而闻名，每百万输出 tokens 仅需 2 元人民币，在 LLM 排名中位列第七。 DeepSeek R1-lite-preview：2024 年 11 月发布，擅长逻辑推理、数学推理和实时问题解决，在一些基准测试中声称超越了 OpenAI O1，但也有测试显示存在速度等方面的挑战。 DeepSeek V3：2024 年 12 月发布，拥有 6710 亿参数，训练仅花费 558 万美元，约 55 天时间，在基准测试中性能优于 Llama 3.1 和 Qwen 2.5，与 GPT-4 O 和 Claude 3.5 Sonnet 相当。 DeepSeek R1 和 DeepSeek R1-zero：2025 年 1 月发布，R1 模型是开源的，可让任何开发者使用，在一些关键的数学和编程测试中表现出色，能与 OpenAI 的先进模型相媲美或超越。 工具层面DeepSeek 作为 AI 工具，具有强大的自然语言处理能力，可进行知识问答、文本生成、数据分析、代码编写等多种任务，还支持多模态输入和联网搜索，能实时获取最新信息。广泛应用于教育、编程、内容创作、智能客服等领域，为用户提供高效、智能的解决方案。\nDeepSeek的官方网站地址 https://www.deepseek.com/ APP https://download.deepseek.com/app/ WebUI https://chat.deepseek.com/ API文档 https://api-docs.deepseek.com/zh-cn/\n服务状态 https://status.deepseek.com/\n本地运行 ollama run deepseek-r1 or\nollama run deepseek-r1:1.5b curl http://localhost:11434/api/generate -d '{ \"model\": \"deepseek-r1:1.5b\", \"prompt\": \"Why is the sky blue?\" }' https://github.com/ollama/ollama/blob/main/docs/api.md\npython python # Please install OpenAI SDK first: `pip3 install openai` from openai import OpenAI client = OpenAI(api_key=\"\", base_url=\"https://api.deepseek.com\") response = client.chat.completions.create( model=\"deepseek-chat\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}, {\"role\": \"user\", \"content\": \"Hello\"}, ], stream=False ) print(response.choices[0].message.content) 注意model=‘deepseek-chat’调用的是DeepSeek-V3模型，model=‘deepseek-reasoner’才是DeepSeek-R1大模型。\ncsharp 要使用C#开发一个与Ollama API进行对话的程序，首先需要确保已经安装了Ollama并且它正在运行。Ollama通常会在本地提供一个REST API，可以通过HTTP请求与它进行交互。\n以下是一个简单的C#示例，展示如何使用HttpClient与Ollama API进行交互，并获取模型标签（如/api/tags）。\n1. 创建C#控制台应用程序 首先，创建一个新的C#控制台应用程序项目。\ndotnet new console -n OllamaChat cd OllamaChat 2. 添加必要的NuGet包 需要添加System.Net.Http包来处理HTTP请求。如果使用的是.NET Core 3.1或更高版本，System.Net.Http已经包含在SDK中，无需额外安装。\n3. 编写代码 在Program.cs中编写以下代码：\nusing System; using System.Net.Http; using System.Text; using System.Threading.Tasks; class Program { static async Task Main(string[] args) { // Ollama API的URL string ollamaApiUrl = \"http://localhost:11434/api/tags\"; // 创建HttpClient实例 using (HttpClient client = new HttpClient()) { try { // 发送GET请求获取模型标签 HttpResponseMessage response = await client.GetAsync(ollamaApiUrl); // 确保请求成功 response.EnsureSuccessStatusCode(); // 读取响应内容 string responseBody = await response.Content.ReadAsStringAsync(); // 输出响应内容 Console.WriteLine(\"Response from Ollama API:\"); Console.WriteLine(responseBody); } catch (HttpRequestException e) { // 处理请求异常 Console.WriteLine(\"\\nException Caught!\"); Console.WriteLine(\"Message :{0} \", e.Message); } } } } 4. 运行程序 确保Ollama正在运行，并且API端点http://localhost:11434/api/tags是可访问的。然后运行的C#程序：\ndotnet run 5. 解释代码 HttpClient: 用于发送HTTP请求和接收HTTP响应。\nGetAsync: 发送一个GET请求到指定的URL。\nEnsureSuccessStatusCode: 确保HTTP响应是成功的（状态码200-299）。\nReadAsStringAsync: 读取响应内容为字符串。\nResponse from Ollama API: {“models”:[{“name”:“deepseek-r1:7b”,“model”:“deepseek-r1:7b”,“modified_at”:“2025-02-04T62xxxxxxxx}}]}\nC# 调用 Ollama API 进行对话 包括获取模型列表和进行对话。 使用 HttpClient 发送 HTTP 请求，并解析返回的 JSON 数据。\nusing System; using System.Net.Http; using System.Text; using System.Text.Json; using System.Threading.Tasks; class Program { private static readonly string BaseUrl = \"http://1.1.1.1:11434\"; static async Task Main() { Console.WriteLine(\"获取可用模型列表...\"); var models = await GetModelList(); Console.WriteLine($\"模型列表: {string.Join(\", \", models)}\"); if (models.Length == 0) { Console.WriteLine(\"没有可用的模型！\"); return; } string model = models[0]; // 选择第一个模型 Console.WriteLine($\"使用模型: {model}\"); while (true) { Console.Write(\": \"); string userInput = Console.ReadLine(); if (string.IsNullOrWhiteSpace(userInput)) break; string response = await ChatWithOllama(model, userInput); Console.WriteLine($\"Ollama: {response}\"); } } // 获取可用模型列表 static async Task\u003cstring[]\u003e GetModelList() { using HttpClient client = new(); HttpResponseMessage response = await client.GetAsync($\"{BaseUrl}/api/tags\"); if (!response.IsSuccessStatusCode) { Console.WriteLine(\"获取模型列表失败\"); return Array.Empty\u003cstring\u003e(); } string json = await response.Content.ReadAsStringAsync(); using JsonDocument doc = JsonDocument.Parse(json); var models = doc.RootElement.GetProperty(\"models\"); string[] modelNames = new string[models.GetArrayLength()]; for (int i = 0; i \u003c models.GetArrayLength(); i++) { modelNames[i] = models[i].GetProperty(\"name\").GetString(); } return modelNames; } // 发送对话请求 static async Task\u003cstring\u003e ChatWithOllama(string model, string prompt) { using HttpClient client = new(); var requestBody = new { model = model, prompt = prompt, stream = false }; string jsonRequest = JsonSerializer.Serialize(requestBody); var content = new StringContent(jsonRequest, Encoding.UTF8, \"application/json\"); HttpResponseMessage response = await client.PostAsync($\"{BaseUrl}/api/generate\", content); if (!response.IsSuccessStatusCode) { return \"对话请求失败\"; } string jsonResponse = await response.Content.ReadAsStringAsync(); using JsonDocument doc = JsonDocument.Parse(jsonResponse); return doc.RootElement.GetProperty(\"response\").GetString(); } } 获取模型列表 (GetModelList):\n发送 GET 请求到 http://1.1.1.1:11434/api/tags 获取模型列表。 解析 JSON 响应，提取模型名称。 进行对话 (ChatWithOllama):\n发送 POST 请求到 http://1.1.1.1:11434/api/generate。 请求体包含： model: 选择的模型名称。 prompt: 用户输入的问题。 stream: false: 禁用流式响应，直接返回完整的结果。 交互方式:\n先获取模型列表，默认选择第一个模型。 进入对话循环，用户输入文本后，程序调用 API 获取 AI 回复并打印。\n运行示例： 获取可用模型列表… 模型列表: llama3, gemma, mistral 使用模型: llama3 好！ Ollama: 好！很高兴见到。 是谁？ Ollama: 我是一个 AI 语言模型，可以帮助回答问题。 nodejs vue3 Node.js（Express + Vue 3）实现的对话网页，支持选择模型并与 Ollama 进行对话。\n后端（Node.js + Express）：提供 API 获取模型列表，并与 Ollama 进行对话。 前端（Vue 3 + Element Plus）：创建界面，允许用户选择模型并进行对话。\n后端（Node.js + Express） 安装依赖：\nnpm init -y npm install express axios cors 创建 server.js：\nconst express = require(\"express\"); const axios = require(\"axios\"); const cors = require(\"cors\"); const app = express(); const PORT = 3000; const OLLAMA_URL = \"http://1.1.1.1:11434\"; app.use(express.json()); app.use(cors()); // 获取可用模型列表 app.get(\"/api/models\", async (req, res) =\u003e { try { const response = await axios.get(`${OLLAMA_URL}/api/tags`); res.json(response.data.models); } catch (error) { res.status(500).json({ error: \"获取模型列表失败\" }); } }); // 发送对话请求 app.post(\"/api/chat\", async (req, res) =\u003e { const { model, message } = req.body; if (!model || !message) { return res.status(400).json({ error: \"缺少模型或消息参数\" }); } try { const response = await axios.post(`${OLLAMA_URL}/api/generate`, { model, prompt: message }); res.json(response.data); } catch (error) { res.status(500).json({ error: \"对话请求失败\" }); } }); app.listen(PORT, () =\u003e { console.log(`Server running at http://localhost:${PORT}`); }); 前端（Vue 3 + Element Plus） 安装 Vue 3 项目：\nnpm create vue@latest ollama-chat cd ollama-chat npm install axios element-plus npm run dev 修改 src/App.vue：\n\u003cscript setup\u003e import { ref, onMounted } from \"vue\"; import axios from \"axios\"; import { ElMessage } from \"element-plus\"; const models = ref([]); const selectedModel = ref(\"\"); const userInput = ref(\"\"); const chatHistory = ref([]); const fetchModels = async () =\u003e { try { const response = await axios.get(\"http://localhost:3000/api/models\"); models.value = response.data; if (models.value.length \u003e 0) { selectedModel.value = models.value[0].name; // 默认选第一个 } } catch (error) { ElMessage.error(\"无法获取模型列表\"); } }; const sendMessage = async () =\u003e { if (!userInput.value.trim()) { ElMessage.warning(\"请输入内容\"); return; } if (!selectedModel.value) { ElMessage.warning(\"请选择模型\"); return; } const userMessage = { role: \"user\", content: userInput.value }; chatHistory.value.push(userMessage); try { const response = await axios.post(\"http://localhost:3000/api/chat\", { model: selectedModel.value, message: userInput.value, }); chatHistory.value.push({ role: \"assistant\", content: response.data.response }); } catch (error) { ElMessage.error(\"请求失败\"); } userInput.value = \"\"; }; onMounted(fetchModels); \u003c/script\u003e \u003ctemplate\u003e \u003cdiv style=\"max-width: 600px; margin: auto; padding: 20px\"\u003e \u003cel-select v-model=\"selectedModel\" placeholder=\"选择模型\" style=\"width: 100%; margin-bottom: 10px\"\u003e \u003cel-option v-for=\"model in models\" :key=\"model.name\" :label=\"model.name\" :value=\"model.name\" /\u003e \u003c/el-select\u003e \u003cdiv v-for=\"(msg, index) in chatHistory\" :key=\"index\" :style=\"{ textAlign: msg.role === 'user' ? 'right' : 'left' }\"\u003e \u003cel-card shadow=\"never\" style=\"margin: 5px 0\"\u003e \u003cstrong\u003e{{ msg.role === \"user\" ? \"：\" : \"Ollama：\" }}\u003c/strong\u003e {{ msg.content }} \u003c/el-card\u003e \u003c/div\u003e \u003cel-input v-model=\"userInput\" placeholder=\"输入消息...\" @keyup.enter=\"sendMessage\" /\u003e \u003cel-button type=\"primary\" @click=\"sendMessage\" style=\"width: 100%; margin-top: 10px\"\u003e发送\u003c/el-button\u003e \u003c/div\u003e \u003c/template\u003e \u003cstyle\u003e .el-card { padding: 10px; border-radius: 8px; } \u003c/style\u003e 运行项目 运行后端： node server.js 运行前端： npm run dev\n打开浏览器访问 Vue 开发服务器（一般是 http://localhost:5173）。 功能说明 ✅ 模型选择：前端请求 http://1.1.1.1:11434/api/tags 获取可用的 Ollama 模型列表。 ✅ 对话功能：输入消息后，前端发送到后端，后端请求 Ollama 进行生成，并返回回复。 ✅ 历史记录：显示用户和 AI 的对话内容。 ✅ 简单 UI：使用 Vue 3 和 Element Plus 构建界面，支持快捷键 Enter 发送消息。\n这个示例可以直接用于和 Ollama 交互，并支持不同的 LLM 模型选择 🚀\n解析 Ollama 流式响应，显示对话内容。 前端调整\n\u003ctemplate\u003e \u003cdiv\u003e \u003ch1\u003eOllama Chat\u003c/h1\u003e \u003cdiv\u003e \u003clabel for=\"model-select\"\u003e选择模型:\u003c/label\u003e \u003cselect id=\"model-select\" v-model=\"selectedModel\"\u003e \u003coption v-for=\"model in models\" :key=\"model.name\" :value=\"model.name\"\u003e {{ model.name }} \u003c/option\u003e \u003c/select\u003e \u003c/div\u003e \u003cdiv\u003e \u003ctextarea v-model=\"userInput\" placeholder=\"输入的消息...\"\u003e\u003c/textarea\u003e \u003cbutton @click=\"sendMessage\"\u003e发送\u003c/button\u003e \u003c/div\u003e \u003cdiv\u003e \u003ch2\u003e对话内容:\u003c/h2\u003e \u003cpre\u003e{{ conversation }}\u003c/pre\u003e \u003c/div\u003e \u003c/div\u003e \u003c/template\u003e \u003cscript\u003e import axios from 'axios'; export default { data() { return { models: [], // 模型列表 selectedModel: '', // 当前选中的模型 userInput: '', // 用户输入的消息 conversation: '', // 对话内容 }; }, async created() { // 页面加载时获取模型列表 await this.fetchModels(); }, methods: { // 获取模型列表 async fetchModels() { try { const response = await axios.get('https://eog9l2jl-cig0n4bq-ferbg20o9l88.vcc4.mcprev.cn/api/models'); this.models = response.data; if (this.models.length \u003e 0) { this.selectedModel = this.models[0].name; // 默认选择第一个模型 } } catch (error) { console.error('获取模型列表失败:', error); } }, // 发送消息 async sendMessage() { if (!this.userInput.trim()) return; // 如果输入为空，直接返回 const message = this.userInput; this.conversation += `用户: ${message}\\n`; // 将用户输入添加到对话内容 this.userInput = ''; // 清空输入框 try { const response = await axios.post('https://eog9l2jl-cig0n4bq-ferbg20o9l88.vcc4.mcprev.cn/api/chat', { model: this.selectedModel, message: message, }); // 解析流式返回的数据 const rawData = response.data.split('\\n'); // 按行拆分 JSON let aiResponse = ''; rawData.forEach(line =\u003e { if (line.trim()) { try { const parsed = JSON.parse(line); if (parsed.response) { aiResponse += parsed.response; // 拼接 AI 回复 } } catch (error) { console.error('JSON 解析失败:', error); } } }); // 更新对话内容 this.conversation += `AI: ${aiResponse}\\n`; } catch (error) { console.error('发送消息失败:', error); this.conversation += `AI: 请求失败，请稍后重试。\\n`; } }, }, }; \u003c/script\u003e \u003cstyle\u003e /* 可以在这里添加一些样式 */ textarea { width: 300px; height: 100px; margin-bottom: 10px; } button { padding: 5px 10px; cursor: pointer; } pre { white-space: pre-wrap; /* 保留换行符 */ background: #f4f4f4; padding: 10px; border-radius: 5px; } \u003c/style\u003e 可滚动框 我们可以使用 CSS 的 overflow 属性。\n对话内容: {{ conversation }} 我们添加了一个新的 元素，并设置了它的 style 属性。max-height: 200px 限制了这个框的最大高度，而 overflow-y: auto 则允许在内容超出这个高度时出现滚动条。\n智商测试 strawberry有多少个r？ 10.11和10.2谁大？ 24点游戏解题:3 4 7 13 树上10只鸟，开枪打死1只，请问还有多少只鸟？\n","wordCount":"1259","inLanguage":"en","datePublished":"2025-02-06T04:21:55Z","dateModified":"2025-02-06T04:21:55Z","author":{"@type":"Person","name":"dwd"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qfsyso.github.io/posts/deepseek-local-deploy/"},"publisher":{"@type":"Organization","name":"MLOG","logo":{"@type":"ImageObject","url":"https://qfsyso.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://qfsyso.github.io/ accesskey=h title="MLOG (Alt + H)">MLOG</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://qfsyso.github.io/posts/ title=Archive><span>Archive</span></a></li><li><a href=https://qfsyso.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://qfsyso.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">deepseek local deploy</h1><div class=post-meta><span title='2025-02-06 04:21:55 +0000 UTC'>February 6, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;dwd</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#deepseek aria-label=DeepSeek>DeepSeek</a></li><li><a href=#%e6%9c%ac%e5%9c%b0%e8%bf%90%e8%a1%8c aria-label=本地运行>本地运行</a></li><li><a href=#python aria-label=python>python</a></li><li><a href=#csharp aria-label=csharp>csharp</a><ul><li><a href=#1-%e5%88%9b%e5%bb%bac%e6%8e%a7%e5%88%b6%e5%8f%b0%e5%ba%94%e7%94%a8%e7%a8%8b%e5%ba%8f aria-label="1. 创建C#控制台应用程序">1. 创建C#控制台应用程序</a></li><li><a href=#2-%e6%b7%bb%e5%8a%a0%e5%bf%85%e8%a6%81%e7%9a%84nuget%e5%8c%85 aria-label="2. 添加必要的NuGet包">2. 添加必要的NuGet包</a></li><li><a href=#3-%e7%bc%96%e5%86%99%e4%bb%a3%e7%a0%81 aria-label="3. 编写代码">3. 编写代码</a></li><li><a href=#4-%e8%bf%90%e8%a1%8c%e7%a8%8b%e5%ba%8f aria-label="4. 运行程序">4. 运行程序</a></li><li><a href=#5-%e8%a7%a3%e9%87%8a%e4%bb%a3%e7%a0%81 aria-label="5. 解释代码">5. 解释代码</a></li><li><a href=#c-%e8%b0%83%e7%94%a8-ollama-api-%e8%bf%9b%e8%a1%8c%e5%af%b9%e8%af%9d aria-label="C# 调用 Ollama API 进行对话">C# 调用 Ollama API 进行对话</a></li><li><a href=#%e8%bf%90%e8%a1%8c%e7%a4%ba%e4%be%8b aria-label=运行示例：>运行示例：</a></li></ul></li><li><a href=#nodejs--vue3 aria-label="nodejs  vue3">nodejs vue3</a><ul><li><a href=#%e5%90%8e%e7%ab%afnodejs--express aria-label="后端（Node.js + Express）">后端（Node.js + Express）</a></li><li><a href=#%e5%89%8d%e7%ab%afvue-3--element-plus aria-label="前端（Vue 3 + Element Plus）">前端（Vue 3 + Element Plus）</a></li><li><a href=#%e8%bf%90%e8%a1%8c%e9%a1%b9%e7%9b%ae aria-label=运行项目>运行项目</a></li></ul></li><li><a href=#%e8%a7%a3%e6%9e%90-ollama-%e6%b5%81%e5%bc%8f%e5%93%8d%e5%ba%94%e6%98%be%e7%a4%ba%e5%af%b9%e8%af%9d%e5%86%85%e5%ae%b9 aria-label="解析 Ollama 流式响应，显示对话内容。">解析 Ollama 流式响应，显示对话内容。</a></li><li><a href=#%e5%8f%af%e6%bb%9a%e5%8a%a8%e6%a1%86 aria-label=可滚动框>可滚动框</a></li><li><a href=#%e6%99%ba%e5%95%86%e6%b5%8b%e8%af%95 aria-label=智商测试>智商测试</a></li></ul></div></details></div><div class=post-content><h1 id=deepseek>DeepSeek<a hidden class=anchor aria-hidden=true href=#deepseek>#</a></h1><p><strong>DeepSeek</strong>是杭州深度求索人工智能技术有限公司推出的人工智能项目，涵盖公司品牌、人工智能模型及相关 AI 工具等多层含义，以下是具体介绍：
<strong>公司层面</strong>DeepSeek 是一家 2023 年 5 月成立的中国人工智能公司，由对冲基金公司 High-Flyer 出资成立。其创始人梁文峰带领团队专注于人工智能基础技术研究与开发，在人工智能领域尤其是大语言模型方向投入了大量研究资源并取得成果。
<strong>模型层面</strong>
DeepSeek Coder：2023 年 11 月 2 日发布，是完全开源且可免费用于商业用途的模型，为开发者提供了基础的代码生成等能力支持。
DeepSeek LLM：2023 年 11 月 29 日推出，具有 670 亿参数，性能与当时的 GPT-4 接近，同时推出了聊天版本 DeepSeek Chat。
DeepSeek V2：2024 年 5 月发布，以性价比高而闻名，每百万输出 tokens 仅需 2 元人民币，在 LLM 排名中位列第七。
DeepSeek R1-lite-preview：2024 年 11 月发布，擅长逻辑推理、数学推理和实时问题解决，在一些基准测试中声称超越了 OpenAI O1，但也有测试显示存在速度等方面的挑战。
DeepSeek V3：2024 年 12 月发布，拥有 6710 亿参数，训练仅花费 558 万美元，约 55 天时间，在基准测试中性能优于 Llama 3.1 和 Qwen 2.5，与 GPT-4 O 和 Claude 3.5 Sonnet 相当。
DeepSeek R1 和 DeepSeek R1-zero：2025 年 1 月发布，R1 模型是开源的，可让任何开发者使用，在一些关键的数学和编程测试中表现出色，能与 OpenAI 的先进模型相媲美或超越。
<strong>工具层面</strong>DeepSeek 作为 AI 工具，具有强大的自然语言处理能力，可进行知识问答、文本生成、数据分析、代码编写等多种任务，还支持多模态输入和联网搜索，能实时获取最新信息。广泛应用于教育、编程、内容创作、智能客服等领域，为用户提供高效、智能的解决方案。</p><p>DeepSeek的官方网站地址 <a href=https://www.deepseek.com/>https://www.deepseek.com/</a>
APP <a href=https://download.deepseek.com/app/>https://download.deepseek.com/app/</a>
WebUI <a href=https://chat.deepseek.com/>https://chat.deepseek.com/</a>
API文档 <a href=https://api-docs.deepseek.com/zh-cn/>https://api-docs.deepseek.com/zh-cn/</a></p><p>服务状态 <a href=https://status.deepseek.com/>https://status.deepseek.com/</a></p><h1 id=本地运行>本地运行<a hidden class=anchor aria-hidden=true href=#本地运行>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ollama run deepseek-r1
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ollama run deepseek-r1:1.5b
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl http://localhost:11434/api/generate -d <span style=color:#e6db74>&#39;{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;model&#34;: &#34;deepseek-r1:1.5b&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;prompt&#34;: &#34;Why is the sky blue?&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}&#39;</span>
</span></span></code></pre></div><p><a href=https://github.com/ollama/ollama/blob/main/docs/api.md>https://github.com/ollama/ollama/blob/main/docs/api.md</a></p><h1 id=python>python<a hidden class=anchor aria-hidden=true href=#python>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span>python
</span></span><span style=display:flex><span><span style=color:#75715e># Please install OpenAI SDK first: `pip3 install openai`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> openai <span style=color:#f92672>import</span> OpenAI
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> OpenAI(api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;DeepSeek API Key&gt;&#34;</span>, base_url<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;https://api.deepseek.com&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>chat<span style=color:#f92672>.</span>completions<span style=color:#f92672>.</span>create(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;deepseek-chat&#34;</span>,
</span></span><span style=display:flex><span>    messages<span style=color:#f92672>=</span>[
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;You are a helpful assistant&#34;</span>},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;Hello&#34;</span>},
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    stream<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(response<span style=color:#f92672>.</span>choices[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>message<span style=color:#f92672>.</span>content)
</span></span></code></pre></div><p>注意model=&lsquo;deepseek-chat&rsquo;调用的是DeepSeek-V3模型，model=&lsquo;deepseek-reasoner&rsquo;才是DeepSeek-R1大模型。</p><h1 id=csharp>csharp<a hidden class=anchor aria-hidden=true href=#csharp>#</a></h1><p>要使用C#开发一个与Ollama API进行对话的程序，首先需要确保已经安装了Ollama并且它正在运行。Ollama通常会在本地提供一个REST API，可以通过HTTP请求与它进行交互。</p><p>以下是一个简单的C#示例，展示如何使用HttpClient与Ollama API进行交互，并获取模型标签（如/api/tags）。</p><h2 id=1-创建c控制台应用程序>1. 创建C#控制台应用程序<a hidden class=anchor aria-hidden=true href=#1-创建c控制台应用程序>#</a></h2><p>首先，创建一个新的C#控制台应用程序项目。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>dotnet new console -n OllamaChat
</span></span><span style=display:flex><span>cd OllamaChat
</span></span></code></pre></div><h2 id=2-添加必要的nuget包>2. 添加必要的NuGet包<a hidden class=anchor aria-hidden=true href=#2-添加必要的nuget包>#</a></h2><p>需要添加System.Net.Http包来处理HTTP请求。如果使用的是.NET Core 3.1或更高版本，System.Net.Http已经包含在SDK中，无需额外安装。</p><h2 id=3-编写代码>3. 编写代码<a hidden class=anchor aria-hidden=true href=#3-编写代码>#</a></h2><p>在Program.cs中编写以下代码：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-csharp data-lang=csharp><span style=display:flex><span><span style=color:#66d9ef>using</span> System;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> System.Net.Http;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> System.Text;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> System.Threading.Tasks;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Program</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>async</span> Task Main(<span style=color:#66d9ef>string</span>[] args)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#75715e>// Ollama API的URL</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>string</span> ollamaApiUrl = <span style=color:#e6db74>&#34;http://localhost:11434/api/tags&#34;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>// 创建HttpClient实例</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>using</span> (HttpClient client = <span style=color:#66d9ef>new</span> HttpClient())
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>try</span>
</span></span><span style=display:flex><span>            {
</span></span><span style=display:flex><span>                <span style=color:#75715e>// 发送GET请求获取模型标签</span>
</span></span><span style=display:flex><span>                HttpResponseMessage response = <span style=color:#66d9ef>await</span> client.GetAsync(ollamaApiUrl);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e>// 确保请求成功</span>
</span></span><span style=display:flex><span>                response.EnsureSuccessStatusCode();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e>// 读取响应内容</span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>string</span> responseBody = <span style=color:#66d9ef>await</span> response.Content.ReadAsStringAsync();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#75715e>// 输出响应内容</span>
</span></span><span style=display:flex><span>                Console.WriteLine(<span style=color:#e6db74>&#34;Response from Ollama API:&#34;</span>);
</span></span><span style=display:flex><span>                Console.WriteLine(responseBody);
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>catch</span> (HttpRequestException e)
</span></span><span style=display:flex><span>            {
</span></span><span style=display:flex><span>                <span style=color:#75715e>// 处理请求异常</span>
</span></span><span style=display:flex><span>                Console.WriteLine(<span style=color:#e6db74>&#34;\nException Caught!&#34;</span>);
</span></span><span style=display:flex><span>                Console.WriteLine(<span style=color:#e6db74>&#34;Message :{0} &#34;</span>, e.Message);
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=4-运行程序>4. 运行程序<a hidden class=anchor aria-hidden=true href=#4-运行程序>#</a></h2><p>确保Ollama正在运行，并且API端点http://localhost:11434/api/tags是可访问的。然后运行的C#程序：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>dotnet run
</span></span></code></pre></div><h2 id=5-解释代码>5. 解释代码<a hidden class=anchor aria-hidden=true href=#5-解释代码>#</a></h2><p>HttpClient: 用于发送HTTP请求和接收HTTP响应。</p><p>GetAsync: 发送一个GET请求到指定的URL。</p><p>EnsureSuccessStatusCode: 确保HTTP响应是成功的（状态码200-299）。</p><p>ReadAsStringAsync: 读取响应内容为字符串。</p><p>Response from Ollama API:
{&ldquo;models&rdquo;:[{&ldquo;name&rdquo;:&ldquo;deepseek-r1:7b&rdquo;,&ldquo;model&rdquo;:&ldquo;deepseek-r1:7b&rdquo;,&ldquo;modified_at&rdquo;:&ldquo;2025-02-04T62xxxxxxxx}}]}</p><h2 id=c-调用-ollama-api-进行对话>C# 调用 Ollama API 进行对话<a hidden class=anchor aria-hidden=true href=#c-调用-ollama-api-进行对话>#</a></h2><p>包括获取模型列表和进行对话。
使用 HttpClient 发送 HTTP 请求，并解析返回的 JSON 数据。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-csharp data-lang=csharp><span style=display:flex><span><span style=color:#66d9ef>using</span> System;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> System.Net.Http;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> System.Text;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> System.Text.Json;
</span></span><span style=display:flex><span><span style=color:#66d9ef>using</span> System.Threading.Tasks;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Program</span>
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>private</span> <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>readonly</span> <span style=color:#66d9ef>string</span> BaseUrl = <span style=color:#e6db74>&#34;http://1.1.1.1:11434&#34;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>async</span> Task Main()
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        Console.WriteLine(<span style=color:#e6db74>&#34;获取可用模型列表...&#34;</span>);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>var</span> models = <span style=color:#66d9ef>await</span> GetModelList();
</span></span><span style=display:flex><span>        Console.WriteLine(<span style=color:#e6db74>$&#34;模型列表: {string.Join(&#34;</span>, <span style=color:#e6db74>&#34;, models)}&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (models.Length == <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            Console.WriteLine(<span style=color:#e6db74>&#34;没有可用的模型！&#34;</span>);
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>string</span> model = models[<span style=color:#ae81ff>0</span>];  <span style=color:#75715e>// 选择第一个模型</span>
</span></span><span style=display:flex><span>        Console.WriteLine(<span style=color:#e6db74>$&#34;使用模型: {model}&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>while</span> (<span style=color:#66d9ef>true</span>)
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            Console.Write(<span style=color:#e6db74>&#34;: &#34;</span>);
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>string</span> userInput = Console.ReadLine();
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> (<span style=color:#66d9ef>string</span>.IsNullOrWhiteSpace(userInput)) <span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>string</span> response = <span style=color:#66d9ef>await</span> ChatWithOllama(model, userInput);
</span></span><span style=display:flex><span>            Console.WriteLine(<span style=color:#e6db74>$&#34;Ollama: {response}&#34;</span>);
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 获取可用模型列表</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>async</span> Task&lt;<span style=color:#66d9ef>string</span>[]&gt; GetModelList()
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>using</span> HttpClient client = <span style=color:#66d9ef>new</span>();
</span></span><span style=display:flex><span>        HttpResponseMessage response = <span style=color:#66d9ef>await</span> client.GetAsync(<span style=color:#e6db74>$&#34;{BaseUrl}/api/tags&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (!response.IsSuccessStatusCode)
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            Console.WriteLine(<span style=color:#e6db74>&#34;获取模型列表失败&#34;</span>);
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> Array.Empty&lt;<span style=color:#66d9ef>string</span>&gt;();
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>string</span> json = <span style=color:#66d9ef>await</span> response.Content.ReadAsStringAsync();
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>using</span> JsonDocument doc = JsonDocument.Parse(json);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>var</span> models = doc.RootElement.GetProperty(<span style=color:#e6db74>&#34;models&#34;</span>);
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>string</span>[] modelNames = <span style=color:#66d9ef>new</span> <span style=color:#66d9ef>string</span>[models.GetArrayLength()];
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> i = <span style=color:#ae81ff>0</span>; i &lt; models.GetArrayLength(); i++)
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            modelNames[i] = models[i].GetProperty(<span style=color:#e6db74>&#34;name&#34;</span>).GetString();
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> modelNames;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// 发送对话请求</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>static</span> <span style=color:#66d9ef>async</span> Task&lt;<span style=color:#66d9ef>string</span>&gt; ChatWithOllama(<span style=color:#66d9ef>string</span> model, <span style=color:#66d9ef>string</span> prompt)
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>using</span> HttpClient client = <span style=color:#66d9ef>new</span>();
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>var</span> requestBody = <span style=color:#66d9ef>new</span>
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            model = model,
</span></span><span style=display:flex><span>            prompt = prompt,
</span></span><span style=display:flex><span>            stream = <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>        };
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>string</span> jsonRequest = JsonSerializer.Serialize(requestBody);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>var</span> content = <span style=color:#66d9ef>new</span> StringContent(jsonRequest, Encoding.UTF8, <span style=color:#e6db74>&#34;application/json&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        HttpResponseMessage response = <span style=color:#66d9ef>await</span> client.PostAsync(<span style=color:#e6db74>$&#34;{BaseUrl}/api/generate&#34;</span>, content);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (!response.IsSuccessStatusCode)
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;对话请求失败&#34;</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>string</span> jsonResponse = <span style=color:#66d9ef>await</span> response.Content.ReadAsStringAsync();
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>using</span> JsonDocument doc = JsonDocument.Parse(jsonResponse);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> doc.RootElement.GetProperty(<span style=color:#e6db74>&#34;response&#34;</span>).GetString();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>获取模型列表 (GetModelList):</p><p>发送 GET 请求到 http://1.1.1.1:11434/api/tags 获取模型列表。
解析 JSON 响应，提取模型名称。
进行对话 (ChatWithOllama):</p><p>发送 POST 请求到 http://1.1.1.1:11434/api/generate。
请求体包含：
model: 选择的模型名称。
prompt: 用户输入的问题。
stream: false: 禁用流式响应，直接返回完整的结果。
<strong>交互方式:</strong></p><p>先获取模型列表，默认选择第一个模型。
进入对话循环，用户输入文本后，程序调用 API 获取 AI 回复并打印。</p><h2 id=运行示例>运行示例：<a hidden class=anchor aria-hidden=true href=#运行示例>#</a></h2><dl><dt>获取可用模型列表&mldr;</dt><dt>模型列表: llama3, gemma, mistral</dt><dt>使用模型: llama3</dt><dd>好！
Ollama: 好！很高兴见到。</dd><dd>是谁？
Ollama: 我是一个 AI 语言模型，可以帮助回答问题。</dd></dl><h1 id=nodejs--vue3>nodejs vue3<a hidden class=anchor aria-hidden=true href=#nodejs--vue3>#</a></h1><p>Node.js（Express + Vue 3）实现的对话网页，支持选择模型并与 Ollama 进行对话。</p><p>后端（Node.js + Express）：提供 API 获取模型列表，并与 Ollama 进行对话。
前端（Vue 3 + Element Plus）：创建界面，允许用户选择模型并进行对话。</p><h2 id=后端nodejs--express>后端（Node.js + Express）<a hidden class=anchor aria-hidden=true href=#后端nodejs--express>#</a></h2><p>安装依赖：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>npm init -y
</span></span><span style=display:flex><span>npm install express axios cors
</span></span></code></pre></div><p>创建 server.js：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>express</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#34;express&#34;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>axios</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#34;axios&#34;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>cors</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>require</span>(<span style=color:#e6db74>&#34;cors&#34;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>app</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>express</span>();
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>PORT</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>3000</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>OLLAMA_URL</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;http://1.1.1.1:11434&#34;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>app</span>.<span style=color:#a6e22e>use</span>(<span style=color:#a6e22e>express</span>.<span style=color:#a6e22e>json</span>());
</span></span><span style=display:flex><span><span style=color:#a6e22e>app</span>.<span style=color:#a6e22e>use</span>(<span style=color:#a6e22e>cors</span>());
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 获取可用模型列表
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#a6e22e>app</span>.<span style=color:#a6e22e>get</span>(<span style=color:#e6db74>&#34;/api/models&#34;</span>, <span style=color:#66d9ef>async</span> (<span style=color:#a6e22e>req</span>, <span style=color:#a6e22e>res</span>) =&gt; {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>axios</span>.<span style=color:#a6e22e>get</span>(<span style=color:#e6db74>`</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>OLLAMA_URL</span><span style=color:#e6db74>}</span><span style=color:#e6db74>/api/tags`</span>);
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>res</span>.<span style=color:#a6e22e>json</span>(<span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>models</span>);
</span></span><span style=display:flex><span>    } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>error</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>res</span>.<span style=color:#a6e22e>status</span>(<span style=color:#ae81ff>500</span>).<span style=color:#a6e22e>json</span>({ <span style=color:#a6e22e>error</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;获取模型列表失败&#34;</span> });
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 发送对话请求
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#a6e22e>app</span>.<span style=color:#a6e22e>post</span>(<span style=color:#e6db74>&#34;/api/chat&#34;</span>, <span style=color:#66d9ef>async</span> (<span style=color:#a6e22e>req</span>, <span style=color:#a6e22e>res</span>) =&gt; {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> { <span style=color:#a6e22e>model</span>, <span style=color:#a6e22e>message</span> } <span style=color:#f92672>=</span> <span style=color:#a6e22e>req</span>.<span style=color:#a6e22e>body</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>model</span> <span style=color:#f92672>||</span> <span style=color:#f92672>!</span><span style=color:#a6e22e>message</span>) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#a6e22e>res</span>.<span style=color:#a6e22e>status</span>(<span style=color:#ae81ff>400</span>).<span style=color:#a6e22e>json</span>({ <span style=color:#a6e22e>error</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;缺少模型或消息参数&#34;</span> });
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>axios</span>.<span style=color:#a6e22e>post</span>(<span style=color:#e6db74>`</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>OLLAMA_URL</span><span style=color:#e6db74>}</span><span style=color:#e6db74>/api/generate`</span>, {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>model</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>prompt</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>message</span>
</span></span><span style=display:flex><span>        });
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>res</span>.<span style=color:#a6e22e>json</span>(<span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>data</span>);
</span></span><span style=display:flex><span>    } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>error</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>res</span>.<span style=color:#a6e22e>status</span>(<span style=color:#ae81ff>500</span>).<span style=color:#a6e22e>json</span>({ <span style=color:#a6e22e>error</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;对话请求失败&#34;</span> });
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>app</span>.<span style=color:#a6e22e>listen</span>(<span style=color:#a6e22e>PORT</span>, () =&gt; {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>`Server running at http://localhost:</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>PORT</span><span style=color:#e6db74>}</span><span style=color:#e6db74>`</span>);
</span></span><span style=display:flex><span>});
</span></span></code></pre></div><h2 id=前端vue-3--element-plus>前端（Vue 3 + Element Plus）<a hidden class=anchor aria-hidden=true href=#前端vue-3--element-plus>#</a></h2><p>安装 Vue 3 项目：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>npm create vue@latest ollama-chat
</span></span><span style=display:flex><span>cd ollama-chat
</span></span><span style=display:flex><span>npm install axios element-plus
</span></span><span style=display:flex><span>npm run dev
</span></span></code></pre></div><p>修改 src/App.vue：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-html data-lang=html><span style=display:flex><span>&lt;<span style=color:#f92672>script</span> <span style=color:#a6e22e>setup</span>&gt;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>ref</span>, <span style=color:#a6e22e>onMounted</span> } <span style=color:#a6e22e>from</span> <span style=color:#e6db74>&#34;vue&#34;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>axios</span> <span style=color:#a6e22e>from</span> <span style=color:#e6db74>&#34;axios&#34;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>ElMessage</span> } <span style=color:#a6e22e>from</span> <span style=color:#e6db74>&#34;element-plus&#34;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>models</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>ref</span>([]);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>selectedModel</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>ref</span>(<span style=color:#e6db74>&#34;&#34;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>userInput</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>ref</span>(<span style=color:#e6db74>&#34;&#34;</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>chatHistory</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>ref</span>([]);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>fetchModels</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>async</span> () =&gt; {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>axios</span>.<span style=color:#a6e22e>get</span>(<span style=color:#e6db74>&#34;http://localhost:3000/api/models&#34;</span>);
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>models</span>.<span style=color:#a6e22e>value</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>data</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>models</span>.<span style=color:#a6e22e>value</span>.<span style=color:#a6e22e>length</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>) {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>selectedModel</span>.<span style=color:#a6e22e>value</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>models</span>.<span style=color:#a6e22e>value</span>[<span style=color:#ae81ff>0</span>].<span style=color:#a6e22e>name</span>; <span style=color:#75715e>// 默认选第一个
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        }
</span></span><span style=display:flex><span>    } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>error</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>ElMessage</span>.<span style=color:#a6e22e>error</span>(<span style=color:#e6db74>&#34;无法获取模型列表&#34;</span>);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>sendMessage</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>async</span> () =&gt; {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>userInput</span>.<span style=color:#a6e22e>value</span>.<span style=color:#a6e22e>trim</span>()) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>ElMessage</span>.<span style=color:#a6e22e>warning</span>(<span style=color:#e6db74>&#34;请输入内容&#34;</span>);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>selectedModel</span>.<span style=color:#a6e22e>value</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>ElMessage</span>.<span style=color:#a6e22e>warning</span>(<span style=color:#e6db74>&#34;请选择模型&#34;</span>);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>userMessage</span> <span style=color:#f92672>=</span> { <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>userInput</span>.<span style=color:#a6e22e>value</span> };
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>chatHistory</span>.<span style=color:#a6e22e>value</span>.<span style=color:#a6e22e>push</span>(<span style=color:#a6e22e>userMessage</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>axios</span>.<span style=color:#a6e22e>post</span>(<span style=color:#e6db74>&#34;http://localhost:3000/api/chat&#34;</span>, {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>model</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>selectedModel</span>.<span style=color:#a6e22e>value</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>message</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>userInput</span>.<span style=color:#a6e22e>value</span>,
</span></span><span style=display:flex><span>        });
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>chatHistory</span>.<span style=color:#a6e22e>value</span>.<span style=color:#a6e22e>push</span>({ <span style=color:#a6e22e>role</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#34;assistant&#34;</span>, <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>response</span> });
</span></span><span style=display:flex><span>    } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>error</span>) {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>ElMessage</span>.<span style=color:#a6e22e>error</span>(<span style=color:#e6db74>&#34;请求失败&#34;</span>);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>userInput</span>.<span style=color:#a6e22e>value</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>;
</span></span><span style=display:flex><span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>onMounted</span>(<span style=color:#a6e22e>fetchModels</span>);
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>script</span>&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>template</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#f92672>div</span> <span style=color:#a6e22e>style</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;max-width: 600px; margin: auto; padding: 20px&#34;</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>el-select</span> <span style=color:#a6e22e>v-model</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;selectedModel&#34;</span> <span style=color:#a6e22e>placeholder</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;选择模型&#34;</span> <span style=color:#a6e22e>style</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;width: 100%; margin-bottom: 10px&#34;</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#f92672>el-option</span> <span style=color:#a6e22e>v-for</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model in models&#34;</span> <span style=color:#a6e22e>:key</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model.name&#34;</span> <span style=color:#a6e22e>:label</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model.name&#34;</span> <span style=color:#a6e22e>:value</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model.name&#34;</span> /&gt;
</span></span><span style=display:flex><span>        &lt;/<span style=color:#f92672>el-select</span>&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>div</span> <span style=color:#a6e22e>v-for</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;(msg, index) in chatHistory&#34;</span> <span style=color:#a6e22e>:key</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;index&#34;</span> <span style=color:#a6e22e>:style</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;{ textAlign: msg.role === &#39;user&#39; ? &#39;right&#39; : &#39;left&#39; }&#34;</span>&gt;
</span></span><span style=display:flex><span>            &lt;<span style=color:#f92672>el-card</span> <span style=color:#a6e22e>shadow</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;never&#34;</span> <span style=color:#a6e22e>style</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;margin: 5px 0&#34;</span>&gt;
</span></span><span style=display:flex><span>                &lt;<span style=color:#f92672>strong</span>&gt;{{ msg.role === &#34;user&#34; ? &#34;：&#34; : &#34;Ollama：&#34; }}&lt;/<span style=color:#f92672>strong</span>&gt; {{ msg.content }}
</span></span><span style=display:flex><span>            &lt;/<span style=color:#f92672>el-card</span>&gt;
</span></span><span style=display:flex><span>        &lt;/<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>el-input</span> <span style=color:#a6e22e>v-model</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;userInput&#34;</span> <span style=color:#a6e22e>placeholder</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;输入消息...&#34;</span> <span style=color:#960050;background-color:#1e0010>@</span><span style=color:#a6e22e>keyup</span><span style=color:#960050;background-color:#1e0010>.</span><span style=color:#a6e22e>enter</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sendMessage&#34;</span> /&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>el-button</span> <span style=color:#a6e22e>type</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;primary&#34;</span> <span style=color:#960050;background-color:#1e0010>@</span><span style=color:#a6e22e>click</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sendMessage&#34;</span> <span style=color:#a6e22e>style</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;width: 100%; margin-top: 10px&#34;</span>&gt;发送&lt;/<span style=color:#f92672>el-button</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>template</span>&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&lt;<span style=color:#f92672>style</span>&gt;
</span></span><span style=display:flex><span>.<span style=color:#a6e22e>el-card</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>padding</span>: <span style=color:#ae81ff>10</span><span style=color:#66d9ef>px</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>border-radius</span>: <span style=color:#ae81ff>8</span><span style=color:#66d9ef>px</span>;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>&lt;/<span style=color:#f92672>style</span>&gt;
</span></span></code></pre></div><h2 id=运行项目>运行项目<a hidden class=anchor aria-hidden=true href=#运行项目>#</a></h2><p>运行后端：
node server.js
运行前端：
npm run dev</p><p>打开浏览器访问 Vue 开发服务器（一般是 http://localhost:5173）。
<strong>功能说明</strong>
✅ 模型选择：前端请求 http://1.1.1.1:11434/api/tags 获取可用的 Ollama 模型列表。
✅ 对话功能：输入消息后，前端发送到后端，后端请求 Ollama 进行生成，并返回回复。
✅ 历史记录：显示用户和 AI 的对话内容。
✅ 简单 UI：使用 Vue 3 和 Element Plus 构建界面，支持快捷键 Enter 发送消息。</p><p>这个示例可以直接用于和 Ollama 交互，并支持不同的 LLM 模型选择 🚀</p><h1 id=解析-ollama-流式响应显示对话内容>解析 Ollama 流式响应，显示对话内容。<a hidden class=anchor aria-hidden=true href=#解析-ollama-流式响应显示对话内容>#</a></h1><p>前端调整</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-html data-lang=html><span style=display:flex><span>&lt;<span style=color:#f92672>template</span>&gt;
</span></span><span style=display:flex><span>    &lt;<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#f92672>h1</span>&gt;Ollama Chat&lt;/<span style=color:#f92672>h1</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>label</span> <span style=color:#a6e22e>for</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model-select&#34;</span>&gt;选择模型:&lt;/<span style=color:#f92672>label</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>select</span> <span style=color:#a6e22e>id</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model-select&#34;</span> <span style=color:#a6e22e>v-model</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;selectedModel&#34;</span>&gt;
</span></span><span style=display:flex><span>          &lt;<span style=color:#f92672>option</span> <span style=color:#a6e22e>v-for</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model in models&#34;</span> <span style=color:#a6e22e>:key</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model.name&#34;</span> <span style=color:#a6e22e>:value</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model.name&#34;</span>&gt;
</span></span><span style=display:flex><span>            {{ model.name }}
</span></span><span style=display:flex><span>          &lt;/<span style=color:#f92672>option</span>&gt;
</span></span><span style=display:flex><span>        &lt;/<span style=color:#f92672>select</span>&gt;
</span></span><span style=display:flex><span>      &lt;/<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>textarea</span> <span style=color:#a6e22e>v-model</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;userInput&#34;</span> <span style=color:#a6e22e>placeholder</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;输入的消息...&#34;</span>&gt;&lt;/<span style=color:#f92672>textarea</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>button</span> <span style=color:#960050;background-color:#1e0010>@</span><span style=color:#a6e22e>click</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sendMessage&#34;</span>&gt;发送&lt;/<span style=color:#f92672>button</span>&gt;
</span></span><span style=display:flex><span>      &lt;/<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>      &lt;<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>h2</span>&gt;对话内容:&lt;/<span style=color:#f92672>h2</span>&gt;
</span></span><span style=display:flex><span>        &lt;<span style=color:#f92672>pre</span>&gt;{{ conversation }}&lt;/<span style=color:#f92672>pre</span>&gt;
</span></span><span style=display:flex><span>      &lt;/<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>    &lt;/<span style=color:#f92672>div</span>&gt;
</span></span><span style=display:flex><span>  &lt;/<span style=color:#f92672>template</span>&gt;
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  &lt;<span style=color:#f92672>script</span>&gt;
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>import</span> <span style=color:#a6e22e>axios</span> <span style=color:#a6e22e>from</span> <span style=color:#e6db74>&#39;axios&#39;</span>;
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>export</span> <span style=color:#66d9ef>default</span> {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>data</span>() {
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>return</span> {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>models</span><span style=color:#f92672>:</span> [], <span style=color:#75715e>// 模型列表
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#a6e22e>selectedModel</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#75715e>// 当前选中的模型
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#a6e22e>userInput</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#75715e>// 用户输入的消息
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#a6e22e>conversation</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#75715e>// 对话内容
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      };
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#a6e22e>created</span>() {
</span></span><span style=display:flex><span>      <span style=color:#75715e>// 页面加载时获取模型列表
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      <span style=color:#66d9ef>await</span> <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>fetchModels</span>();
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>methods</span><span style=color:#f92672>:</span> {
</span></span><span style=display:flex><span>      <span style=color:#75715e>// 获取模型列表
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      <span style=color:#66d9ef>async</span> <span style=color:#a6e22e>fetchModels</span>() {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>axios</span>.<span style=color:#a6e22e>get</span>(<span style=color:#e6db74>&#39;https://eog9l2jl-cig0n4bq-ferbg20o9l88.vcc4.mcprev.cn/api/models&#39;</span>);
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>models</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>data</span>;
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>if</span> (<span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>models</span>.<span style=color:#a6e22e>length</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>) {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>selectedModel</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>models</span>[<span style=color:#ae81ff>0</span>].<span style=color:#a6e22e>name</span>; <span style=color:#75715e>// 默认选择第一个模型
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>          }
</span></span><span style=display:flex><span>        } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>error</span>) {
</span></span><span style=display:flex><span>          <span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>error</span>(<span style=color:#e6db74>&#39;获取模型列表失败:&#39;</span>, <span style=color:#a6e22e>error</span>);
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:#75715e>// 发送消息
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>      <span style=color:#66d9ef>async</span> <span style=color:#a6e22e>sendMessage</span>() {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>userInput</span>.<span style=color:#a6e22e>trim</span>()) <span style=color:#66d9ef>return</span>; <span style=color:#75715e>// 如果输入为空，直接返回
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>message</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>userInput</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>conversation</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`用户: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>message</span><span style=color:#e6db74>}</span><span style=color:#e6db74>\n`</span>; <span style=color:#75715e>// 将用户输入添加到对话内容
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>userInput</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span>; <span style=color:#75715e>// 清空输入框
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>  
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>response</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> <span style=color:#a6e22e>axios</span>.<span style=color:#a6e22e>post</span>(<span style=color:#e6db74>&#39;https://eog9l2jl-cig0n4bq-ferbg20o9l88.vcc4.mcprev.cn/api/chat&#39;</span>, {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>model</span><span style=color:#f92672>:</span> <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>selectedModel</span>,
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>message</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>message</span>,
</span></span><span style=display:flex><span>          });
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>          <span style=color:#75715e>// 解析流式返回的数据
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>          <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>rawData</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>response</span>.<span style=color:#a6e22e>data</span>.<span style=color:#a6e22e>split</span>(<span style=color:#e6db74>&#39;\n&#39;</span>); <span style=color:#75715e>// 按行拆分 JSON
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>          <span style=color:#66d9ef>let</span> <span style=color:#a6e22e>aiResponse</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span>;
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>          <span style=color:#a6e22e>rawData</span>.<span style=color:#a6e22e>forEach</span>(<span style=color:#a6e22e>line</span> =&gt; {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>line</span>.<span style=color:#a6e22e>trim</span>()) {
</span></span><span style=display:flex><span>              <span style=color:#66d9ef>try</span> {
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>parsed</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>parse</span>(<span style=color:#a6e22e>line</span>);
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> (<span style=color:#a6e22e>parsed</span>.<span style=color:#a6e22e>response</span>) {
</span></span><span style=display:flex><span>                  <span style=color:#a6e22e>aiResponse</span> <span style=color:#f92672>+=</span> <span style=color:#a6e22e>parsed</span>.<span style=color:#a6e22e>response</span>; <span style=color:#75715e>// 拼接 AI 回复
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>                }
</span></span><span style=display:flex><span>              } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>error</span>) {
</span></span><span style=display:flex><span>                <span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>error</span>(<span style=color:#e6db74>&#39;JSON 解析失败:&#39;</span>, <span style=color:#a6e22e>error</span>);
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>          });
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>          <span style=color:#75715e>// 更新对话内容
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>          <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>conversation</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`AI: </span><span style=color:#e6db74>${</span><span style=color:#a6e22e>aiResponse</span><span style=color:#e6db74>}</span><span style=color:#e6db74>\n`</span>;
</span></span><span style=display:flex><span>        } <span style=color:#66d9ef>catch</span> (<span style=color:#a6e22e>error</span>) {
</span></span><span style=display:flex><span>          <span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>error</span>(<span style=color:#e6db74>&#39;发送消息失败:&#39;</span>, <span style=color:#a6e22e>error</span>);
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>this</span>.<span style=color:#a6e22e>conversation</span> <span style=color:#f92672>+=</span> <span style=color:#e6db74>`AI: 请求失败，请稍后重试。\n`</span>;
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>  };
</span></span><span style=display:flex><span>  &lt;/<span style=color:#f92672>script</span>&gt;
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  &lt;<span style=color:#f92672>style</span>&gt;
</span></span><span style=display:flex><span>  <span style=color:#75715e>/* 可以在这里添加一些样式 */</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>textarea</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>width</span>: <span style=color:#ae81ff>300</span><span style=color:#66d9ef>px</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>height</span>: <span style=color:#ae81ff>100</span><span style=color:#66d9ef>px</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>margin-bottom</span>: <span style=color:#ae81ff>10</span><span style=color:#66d9ef>px</span>;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:#f92672>button</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>padding</span>: <span style=color:#ae81ff>5</span><span style=color:#66d9ef>px</span> <span style=color:#ae81ff>10</span><span style=color:#66d9ef>px</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>cursor</span>: <span style=color:#66d9ef>pointer</span>;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:#f92672>pre</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>white-space</span>: <span style=color:#66d9ef>pre-wrap</span>; <span style=color:#75715e>/* 保留换行符 */</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>background</span>: <span style=color:#ae81ff>#f4f4f4</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>padding</span>: <span style=color:#ae81ff>10</span><span style=color:#66d9ef>px</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>border-radius</span>: <span style=color:#ae81ff>5</span><span style=color:#66d9ef>px</span>;
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  &lt;/<span style=color:#f92672>style</span>&gt;
</span></span><span style=display:flex><span>  
</span></span></code></pre></div><h1 id=可滚动框>可滚动框<a hidden class=anchor aria-hidden=true href=#可滚动框>#</a></h1><p>我们可以使用 CSS 的 overflow 属性。</p><pre><code>  &lt;div&gt;
    &lt;h2&gt;对话内容:&lt;/h2&gt;
    &lt;div style=&quot;max-height: 200px; overflow-y: auto;&quot;&gt;
      &lt;pre&gt;{{ conversation }}&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/div&gt;
</code></pre><p>我们添加了一个新的<div>元素，并设置了它的 style 属性。max-height: 200px 限制了这个框的最大高度，而 overflow-y: auto 则允许在内容超出这个高度时出现滚动条。</p><h1 id=智商测试>智商测试<a hidden class=anchor aria-hidden=true href=#智商测试>#</a></h1><p>strawberry有多少个r？
10.11和10.2谁大？
24点游戏解题:3 4 7 13
树上10只鸟，开枪打死1只，请问还有多少只鸟？</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://qfsyso.github.io/tags/deepseek/>DeepSeek</a></li></ul><nav class=paginav><a class=prev href=https://qfsyso.github.io/posts/free-https-ssl/><span class=title>« Prev</span><br><span>free https ssl</span>
</a><a class=next href=https://qfsyso.github.io/posts/.net9-lock-aspire-uuid-jsonserializeroptions/><span class=title>Next »</span><br><span>.NET9 lock Aspire UUID JsonSerializerOptions</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share deepseek local deploy on x" href="https://x.com/intent/tweet/?text=deepseek%20local%20deploy&amp;url=https%3a%2f%2fqfsyso.github.io%2fposts%2fdeepseek-local-deploy%2f&amp;hashtags=DeepSeek"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share deepseek local deploy on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fqfsyso.github.io%2fposts%2fdeepseek-local-deploy%2f&amp;title=deepseek%20local%20deploy&amp;summary=deepseek%20local%20deploy&amp;source=https%3a%2f%2fqfsyso.github.io%2fposts%2fdeepseek-local-deploy%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share deepseek local deploy on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fqfsyso.github.io%2fposts%2fdeepseek-local-deploy%2f&title=deepseek%20local%20deploy"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share deepseek local deploy on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fqfsyso.github.io%2fposts%2fdeepseek-local-deploy%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share deepseek local deploy on whatsapp" href="https://api.whatsapp.com/send?text=deepseek%20local%20deploy%20-%20https%3a%2f%2fqfsyso.github.io%2fposts%2fdeepseek-local-deploy%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share deepseek local deploy on telegram" href="https://telegram.me/share/url?text=deepseek%20local%20deploy&amp;url=https%3a%2f%2fqfsyso.github.io%2fposts%2fdeepseek-local-deploy%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share deepseek local deploy on ycombinator" href="https://news.ycombinator.com/submitlink?t=deepseek%20local%20deploy&u=https%3a%2f%2fqfsyso.github.io%2fposts%2fdeepseek-local-deploy%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><div class=wtime><div class=wtime2>CST<span id=beijingTime></span> GMT <span id=londonTime></span> EST <span id=newYorkTime></span> PST<span id=losAngelesTime></span></div><div class=wtime2 style=display:none>东京<span id=tokyoTime></span></div><div class=wtime2 style=display:none>欧洲-巴黎<span id=parisTime></span></div><div class=wtime2 style=display:none>UTC<span id=utcTime></span></div></div><span>&copy; 2025 <a href=https://qfsyso.github.io/>MLOG</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span><script type=text/javascript>const timeZoneOffsets={beijing:8,tokyo:9,paris:[1,2],london:[0,1],newYork:[-5,-4],losAngeles:[-8,-7]};function padZero(e){return e<10?"0"+e:e}function formatTime(e){const t=e.getFullYear(),n=padZero(e.getMonth()+1),s=padZero(e.getDate()),o=padZero(e.getHours()),i=padZero(e.getMinutes()),a=padZero(e.getSeconds());return`${t}-${n}-${s} ${o}:${i}:${a}`}function isDaylightSavingTime(e,t){const o=e.getMonth(),n=new Date(e.getFullYear(),3,8,2,0,0),s=new Date(e.getFullYear(),10,1,2,0,0);return(t==="paris"||t==="london"||t==="newYork"||t==="losAngeles")&&e>=n&&e<s}function updateTime(){const e=new Date,t=new Date(e.getUTCFullYear(),e.getUTCMonth(),e.getUTCDate(),e.getUTCHours(),e.getUTCMinutes(),e.getUTCSeconds()),n=formatTime(t);document.getElementById("utcTime").innerHTML=n;for(const n in timeZoneOffsets){const e=timeZoneOffsets[n];let s;Array.isArray(e)?s=isDaylightSavingTime(t,n)?e[1]:e[0]:s=e;const o=new Date(t.getTime()+s*60*60*1e3);document.getElementById(n+"Time").innerHTML=formatTime(o)}setTimeout(updateTime,1e3)}window.onload=function(){updateTime()};function notfound(e){var n=e.src,t=n.split("/"),s=t[t.length-1];e.src="https://47.115.223.75:8080/group1/v2/"+s+"?timestamp="+Date.now(),e.onerror=null}</script></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>